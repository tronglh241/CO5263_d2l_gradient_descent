{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dbf639-4c63-4693-8946-139fbc148da6",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω gi·ªõi thi·ªáu c√°c kh√°i ni·ªám c∆° b·∫£n li√™n quan ƒë·∫øn h·∫° gradient. M·∫∑c d√π ph∆∞∆°ng ph√°p n√†y hi·∫øm khi ƒë∆∞·ª£c s·ª≠ d·ª•ng tr·ª±c ti·∫øp trong h·ªçc s√¢u, vi·ªác hi·ªÉu v·ªÅ h·∫° gradient l√† ch√¨a kh√≥a ƒë·ªÉ hi·ªÉu c√°c thu·∫≠t to√°n h·∫° gradient ng·∫´u nhi√™n. V√≠ d·ª•, v·∫•n ƒë·ªÅ t·ªëi ∆∞u h√≥a c√≥ th·ªÉ b·ªã ph√¢n k·ª≥ do t·ªëc ƒë·ªô h·ªçc qu√° l·ªõn. Hi·ªán t∆∞·ª£ng n√†y ƒë√£ c√≥ th·ªÉ th·∫•y trong h·∫° gradient. T∆∞∆°ng t·ª±, vi·ªác ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a l√† m·ªôt k·ªπ thu·∫≠t ph·ªï bi·∫øn trong h·∫° gradient v√† ƒë∆∞·ª£c √°p d·ª•ng trong c√°c thu·∫≠t to√°n n√¢ng cao h∆°n. H√£y b·∫Øt ƒë·∫ßu v·ªõi m·ªôt tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát ƒë∆°n gi·∫£n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf1d29-f160-4a3a-854b-3a8748e13307",
   "metadata": {},
   "source": [
    "## One-Dimensional Gradient Descent\n",
    "H·∫° gradient trong m·ªôt chi·ªÅu l√† m·ªôt v√≠ d·ª• tuy·ªát v·ªùi ƒë·ªÉ gi·∫£i th√≠ch t·∫°i sao thu·∫≠t to√°n h·∫° gradient c√≥ th·ªÉ gi·∫£m gi√° tr·ªã c·ªßa h√†m m·ª•c ti√™u. X√©t m·ªôt h√†m th·ª±c kh·∫£ vi li√™n t·ª•c $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. S·ª≠ d·ª•ng khai tri·ªÉn Taylor, ta ƒë∆∞·ª£c:\n",
    "$$f(x+\\epsilon)=f(x)+\\epsilon f^{\\prime}(x)+O\\left(\\epsilon^2\\right).$$\n",
    "T·ª©c l√†, trong x·∫•p x·ªâ b·∫≠c m·ªôt, $f(x+\\epsilon)$ ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi gi√° tr·ªã h√†m $f(x)$ v√† ƒë·∫°o h√†m b·∫≠c m·ªôt $f^{\\prime}(x)$ t·∫°i $x$. Kh√¥ng v√¥ l√Ω khi gi·∫£ ƒë·ªãnh r·∫±ng v·ªõi $\\epsilon$ nh·ªè, vi·ªác di chuy·ªÉn theo h∆∞·ªõng gradient √¢m s·∫Ω gi·∫£m $f$. ƒê·ªÉ ƒë∆°n gi·∫£n, ta ch·ªçn m·ªôt k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh $\\eta>0$ v√† ch·ªçn $\\epsilon=-\\eta f^{\\prime}(x)$. Thay v√†o khai tri·ªÉn Taylor ·ªü tr√™n, ta ƒë∆∞·ª£c:\n",
    "$$f\\left(x-\\eta f^{\\prime}(x)\\right)=f(x)-\\eta f^{\\prime 2}(x)+O\\left(\\eta^2 f^{\\prime 2}(x)\\right).$$\n",
    "N·∫øu ƒë·∫°o h√†m $f^{\\prime}(x) \\neq 0$ kh√¥ng bi·∫øn m·∫•t, ta ƒë·∫°t ƒë∆∞·ª£c ti·∫øn b·ªô v√¨ $\\eta f^{\\prime 2}(x)>0$. H∆°n n·ªØa, ta lu√¥n c√≥ th·ªÉ ch·ªçn $\\eta$ ƒë·ªß nh·ªè ƒë·ªÉ c√°c h·∫°ng b·∫≠c cao tr·ªü n√™n kh√¥ng ƒë√°ng k·ªÉ. Do ƒë√≥, ta c√≥:\n",
    "$$f\\left(x-\\eta f^{\\prime}(x)\\right) \\leq f(x).$$\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†, n·∫øu ta s·ª≠ d·ª•ng:\n",
    "$$x \\leftarrow x-\\eta f^{\\prime}(x)$$\n",
    "ƒë·ªÉ l·∫∑p $x$, gi√° tr·ªã c·ªßa h√†m $f(x)$ c√≥ th·ªÉ gi·∫£m. Do ƒë√≥, trong h·∫° gradient, ta ƒë·∫ßu ti√™n ch·ªçn m·ªôt gi√° tr·ªã ban ƒë·∫ßu $x$ v√† m·ªôt h·∫±ng s·ªë $\\eta>0$, sau ƒë√≥ s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ l·∫∑p $x$ li√™n t·ª•c cho ƒë·∫øn khi ƒë·∫°t ƒëi·ªÅu ki·ªán d·ª´ng, v√≠ d·ª•, khi ƒë·ªô l·ªõn c·ªßa gradient $\\left|f^{\\prime}(x)\\right|$ ƒë·ªß nh·ªè ho·∫∑c s·ªë l·∫ßn l·∫∑p ƒë·∫°t m·ªôt gi√° tr·ªã nh·∫•t ƒë·ªãnh.\n",
    "ƒê·ªÉ ƒë∆°n gi·∫£n, ta ch·ªçn h√†m m·ª•c ti√™u $f(x)=x^2$ ƒë·ªÉ minh h·ªça c√°ch th·ª±c hi·ªán h·∫° gradient. M·∫∑c d√π ta bi·∫øt r·∫±ng $x=0$ l√† nghi·ªám t·ªëi ∆∞u c·ªßa $f(x)$, ta v·∫´n s·ª≠ d·ª•ng h√†m ƒë∆°n gi·∫£n n√†y ƒë·ªÉ quan s√°t c√°ch $x$ thay ƒë·ªïi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5239bff-4d53-470c-98d1-8115d76c38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from d21 import torch as d21\n",
    "\n",
    "def f(x):  # objective function\n",
    "    return x ** 2\n",
    "\n",
    "def f_grad(x):  # Gradient (derivative) of the objective function\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f83e0",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ta s·ª≠ d·ª•ng $x=10$ l√†m gi√° tr·ªã ban ƒë·∫ßu v√† gi·∫£ s·ª≠ $\\eta=0.2$. S·ª≠ d·ª•ng h·∫° gradient ƒë·ªÉ l·∫∑p $x$ 10 l·∫ßn, ta c√≥ th·ªÉ th·∫•y r·∫±ng cu·ªëi c√πng, gi√° tr·ªã c·ªßa $x$ ti·∫øn g·∫ßn ƒë·∫øn nghi·ªám t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(eta, f_grad):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(float(x))\n",
    "    print(f'epoch 10, x: {x:f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.2, f_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d45c0",
   "metadata": {},
   "source": [
    "epoch 10, x: 0.060466"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73410907",
   "metadata": {},
   "source": [
    "Ti·∫øn tr√¨nh t·ªëi ∆∞u h√≥a $x$ c√≥ th·ªÉ ƒë∆∞·ª£c v·∫Ω nh∆∞ sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = torch.arange(-n, n, 0.01)\n",
    "    d21.set_figsize()\n",
    "    d21.plot([f_line, results], [[f(x) for x in f_line], [f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "    show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fea95",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "T·ªëc ƒë·ªô h·ªçc (learning rate) ùúÇ c√≥ th·ªÉ ƒë∆∞·ª£c thi·∫øt l·∫≠p b·ªüi ng∆∞·ªùi thi·∫øt k·∫ø thu·∫≠t to√°n. N·∫øu ch√∫ng ta s·ª≠ d·ª•ng m·ªôt t·ªëc ƒë·ªô h·ªçc qu√° nh·ªè, n√≥ s·∫Ω khi·∫øn `ùë•` c·∫≠p nh·∫≠t r·∫•t ch·∫≠m, ƒë√≤i h·ªèi nhi·ªÅu v√≤ng l·∫∑p h∆°n ƒë·ªÉ thu ƒë∆∞·ª£c nghi·ªám t·ªët h∆°n. ƒê·ªÉ minh h·ªça ƒëi·ªÅu x·∫£y ra trong tr∆∞·ªùng h·ª£p nh∆∞ v·∫≠y, h√£y xem x√©t ti·∫øn tr√¨nh trong c√πng b√†i to√°n t·ªëi ∆∞u v·ªõi ùúÇ = 0.05. Nh∆∞ ta c√≥ th·ªÉ th·∫•y, ngay c·∫£ sau 10 b∆∞·ªõc l·∫∑p, ch√∫ng ta v·∫´n c√≤n c√°ch r·∫•t xa nghi·ªám t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(0.05, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14efca73",
   "metadata": {},
   "source": [
    "epoch 10, x: 3.486784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d3615",
   "metadata": {},
   "source": [
    "Ng∆∞·ª£c l·∫°i, n·∫øu ch√∫ng ta s·ª≠ d·ª•ng t·ªëc ƒë·ªô h·ªçc qu√° l·ªõn, gi√° tr·ªã `|ùúÇ ùëì'(ùë•)|` c√≥ th·ªÉ tr·ªü n√™n qu√° l·ªõn ƒë·ªëi v·ªõi c√¥ng th·ª©c khai tri·ªÉn Taylor b·∫≠c nh·∫•t. Nghƒ©a l√†, s·ªë h·∫°ng `O (ùúÇ¬≤ ùëì'¬≤(ùë•))` trong c√¥ng th·ª©c (12.3.2) c√≥ th·ªÉ tr·ªü n√™n ƒë√°ng k·ªÉ. Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta kh√¥ng th·ªÉ ƒë·∫£m b·∫£o r·∫±ng qu√° tr√¨nh c·∫≠p nh·∫≠t l·∫∑p c·ªßa `ùë•` s·∫Ω l√†m gi·∫£m gi√° tr·ªã c·ªßa h√†m `ùëì(ùë•)`. V√≠ d·ª•, khi ch√∫ng ta ƒë·∫∑t t·ªëc ƒë·ªô h·ªçc `ùúÇ = 1.1`, `ùë•` v∆∞·ª£t qu√° (overshoots) nghi·ªám t·ªëi ∆∞u `ùë• = 0` v√† d·∫ßn d·∫ßn ph√¢n k·ª≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(1.1, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e647c",
   "metadata": {},
   "source": [
    "epoch 10, x: 61.917364"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c64e8b",
   "metadata": {},
   "source": [
    "ƒê·ªÉ minh h·ªça ƒëi·ªÅu g√¨ x·∫£y ra v·ªõi c√°c h√†m kh√¥ng l·ªìi, h√£y xem x√©t tr∆∞·ªùng h·ª£p $f(x)=x \\cdot \\cos(cx)$ v·ªõi m·ªôt h·∫±ng s·ªë $c$. H√†m n√†y c√≥ v√¥ s·ªë c·ª±c ti·ªÉu c·ª•c b·ªô. T√πy thu·ªôc v√†o l·ª±a ch·ªçn t·ªëc ƒë·ªô h·ªçc v√† m·ª©c ƒë·ªô ƒëi·ªÅu ki·ªán c·ªßa b√†i to√°n, ta c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ªôt trong nhi·ªÅu nghi·ªám. V√≠ d·ª• d∆∞·ªõi ƒë√¢y minh h·ªça c√°ch m·ªôt t·ªëc ƒë·ªô h·ªçc cao (kh√¥ng th·ª±c t·∫ø) s·∫Ω d·∫´n ƒë·∫øn m·ªôt c·ª±c ti·ªÉu c·ª•c b·ªô k√©m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "show_trace(gd(2, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15686ef4",
   "metadata": {},
   "source": [
    "epoch 10, x: -1.528166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c54212-5549-40c3-a71a-0f979d29431c",
   "metadata": {},
   "source": [
    "## Multivariate Gradient Descent\n",
    "B√¢y gi·ªù khi ƒë√£ c√≥ tr·ª±c gi√°c t·ªët h∆°n v·ªÅ tr∆∞·ªùng h·ª£p m·ªôt bi·∫øn, h√£y xem x√©t t√¨nh hu·ªëng m√† $\\mathbf{x}=\\left[x_1, x_2, \\ldots, x_d\\right]^{\\top}$. T·ª©c l√†, h√†m m·ª•c ti√™u $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ √°nh x·∫° c√°c vector th√†nh s·ªë th·ª±c. Gradient c·ªßa n√≥ c≈©ng l√† ƒëa bi·∫øn, l√† m·ªôt vector g·ªìm $d$ ƒë·∫°o h√†m ri√™ng:\n",
    "$$\\nabla f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\\right]^{\\top}.$$\n",
    "M·ªói ph·∫ßn t·ª≠ ƒë·∫°o h√†m ri√™ng $\\partial f(\\mathbf{x}) / \\partial x_i$ trong gradient bi·ªÉu th·ªã t·ªëc ƒë·ªô thay ƒë·ªïi c·ªßa $f$ t·∫°i $\\mathbf{x}$ theo ƒë·∫ßu v√†o $x_i$. Nh∆∞ trong tr∆∞·ªùng h·ª£p m·ªôt bi·∫øn, ta c√≥ th·ªÉ s·ª≠ d·ª•ng x·∫•p x·ªâ Taylor ƒëa bi·∫øn ƒë·ªÉ c√≥ √Ω t∆∞·ªüng v·ªÅ vi·ªác n√™n l√†m g√¨. C·ª• th·ªÉ, ta c√≥:\n",
    "$$f(\\mathbf{x}+\\boldsymbol{\\epsilon})=f(\\mathbf{x})+\\boldsymbol{\\epsilon}^{\\top} \\nabla f(\\mathbf{x})+O\\left(|\\boldsymbol{\\epsilon}|^2\\right).$$\n",
    "N√≥i c√°ch kh√°c, ƒë·∫øn c√°c h·∫°ng b·∫≠c hai trong $\\epsilon$, h∆∞·ªõng gi·∫£m nhanh nh·∫•t ƒë∆∞·ª£c cho b·ªüi gradient √¢m $-\\nabla f(\\mathbf{x})$. Ch·ªçn m·ªôt t·ªëc ƒë·ªô h·ªçc ph√π h·ª£p $\\eta>0$ cho ra thu·∫≠t to√°n h·∫° gradient nguy√™n m·∫´u:\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathbf{x}).$$\n",
    "ƒê·ªÉ th·∫•y thu·∫≠t to√°n ho·∫°t ƒë·ªông th·∫ø n√†o trong th·ª±c t·∫ø, h√£y x√¢y d·ª±ng m·ªôt h√†m m·ª•c ti√™u $f(\\mathbf{x})=x_1^2+2x_2^2$ v·ªõi vector hai chi·ªÅu $\\mathbf{x}=\\left[x_1, x_2\\right]^{\\top}$ l√†m ƒë·∫ßu v√†o v√† m·ªôt s·ªë th·ª±c l√†m ƒë·∫ßu ra. Gradient ƒë∆∞·ª£c cho b·ªüi $\\nabla f(\\mathbf{x})=\\left[2x_1, 4x_2\\right]^{\\top}$. Ta s·∫Ω quan s√°t qu·ªπ ƒë·∫°o c·ªßa $\\mathbf{x}$ b·∫±ng h·∫° gradient t·ª´ v·ªã tr√≠ ban ƒë·∫ßu $[-5, -2]$.\n",
    "ƒê·ªÉ b·∫Øt ƒë·∫ßu, ta c·∫ßn hai h√†m h·ªó tr·ª£. H√†m ƒë·∫ßu ti√™n s·ª≠ d·ª•ng m·ªôt h√†m c·∫≠p nh·∫≠t v√† √°p d·ª•ng n√≥ 20 l·∫ßn cho gi√° tr·ªã ban ƒë·∫ßu. H√†m h·ªó tr·ª£ th·ª© hai tr·ª±c quan h√≥a qu·ªπ ƒë·∫°o c·ªßa $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f95578-6a5b-4d14-839c-c2b3618aba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2d(trainer, steps=20, f_grad=None):  # save\n",
    "    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n",
    "    # s1 and s2 are internal state variables that will be used in Momentum,\n",
    "    # Adagrad, RMSProp\n",
    "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
    "    results = [(x1, x2)]\n",
    "    for i in range(steps):\n",
    "        if f_grad:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
    "        else:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
    "        results.append((x1, x2))\n",
    "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
    "    return results\n",
    "\n",
    "def show_trace_2d(f, results):  # save\n",
    "    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n",
    "    d21.set_figsize()\n",
    "    d21.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
    "    x1, x2 = torch.meshgrid(torch.arange(-5.5, 1.0, 0.1),\n",
    "                            torch.arange(-3.0, 1.0, 0.1), indexing='ij')\n",
    "    d21.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
    "    d21.plt.xlabel('x1')\n",
    "    d21.plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ce2ad",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ta quan s√°t qu·ªπ ƒë·∫°o c·ªßa bi·∫øn t·ªëi ∆∞u $\\mathbf{x}$ v·ªõi t·ªëc ƒë·ªô h·ªçc $\\eta=0.1$. Ta th·∫•y r·∫±ng sau 20 b∆∞·ªõc, gi√° tr·ªã c·ªßa $\\mathbf{x}$ ti·∫øn g·∫ßn ƒë·∫øn c·ª±c ti·ªÉu t·∫°i $[0, 0]$. Ti·∫øn tr√¨nh kh√° ·ªïn ƒë·ªãnh m·∫∑c d√π kh√° ch·∫≠m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2d(x1, x2):  # Objective function\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_2d_grad(x1, x2):  # Gradient of the objective function\n",
    "    return (2 * x1, 4 * x2)\n",
    "\n",
    "def gd_2d(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
    "\n",
    "eta = 0.1\n",
    "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a06c086",
   "metadata": {},
   "source": [
    "epoch 20, x1: -0.057646, x2: -0.000073"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b8123-eec4-40cf-921c-a9e71351ced9",
   "metadata": {},
   "source": [
    "## Adaptive Methods\n",
    "Nh∆∞ ƒë√£ th·∫•y trong Ph·∫ßn 12.3.1, vi·ªác ch·ªçn t·ªëc ƒë·ªô h·ªçc $\\eta$ \"v·ª´a ƒë√∫ng\" l√† m·ªôt vi·ªác kh√≥ khƒÉn. N·∫øu ch·ªçn qu√° nh·ªè, ta ti·∫øn b·ªô r·∫•t √≠t. N·∫øu ch·ªçn qu√° l·ªõn, nghi·ªám s·∫Ω dao ƒë·ªông v√† trong tr∆∞·ªùng h·ª£p x·∫•u nh·∫•t, c√≥ th·ªÉ ph√¢n k·ª≥. ƒêi·ªÅu g√¨ s·∫Ω x·∫£y ra n·∫øu ta c√≥ th·ªÉ t·ª± ƒë·ªông x√°c ƒë·ªãnh $\\eta$ ho·∫∑c lo·∫°i b·ªè ho√†n to√†n vi·ªác ch·ªçn t·ªëc ƒë·ªô h·ªçc? C√°c ph∆∞∆°ng ph√°p b·∫≠c hai, kh√¥ng ch·ªâ xem x√©t gi√° tr·ªã v√† gradient c·ªßa h√†m m·ª•c ti√™u m√† c√≤n xem x√©t ƒë·ªô cong c·ªßa n√≥, c√≥ th·ªÉ gi√∫p √≠ch trong tr∆∞·ªùng h·ª£p n√†y. M·∫∑c d√π c√°c ph∆∞∆°ng ph√°p n√†y kh√¥ng th·ªÉ √°p d·ª•ng tr·ª±c ti·∫øp cho h·ªçc s√¢u do chi ph√≠ t√≠nh to√°n, ch√∫ng cung c·∫•p tr·ª±c gi√°c h·ªØu √≠ch ƒë·ªÉ thi·∫øt k·∫ø c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a n√¢ng cao, m√¥ ph·ªèng nhi·ªÅu ƒë·∫∑c t√≠nh mong mu·ªën c·ªßa c√°c thu·∫≠t to√°n ƒë∆∞·ª£c tr√¨nh b√†y d∆∞·ªõi ƒë√¢y.\n",
    "### Ph∆∞∆°ng Ph√°p Newton\n",
    "Xem l·∫°i khai tri·ªÉn Taylor c·ªßa h√†m $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$, kh√¥ng c·∫ßn d·ª´ng l·∫°i sau h·∫°ng ƒë·∫ßu ti√™n. Th·ª±c t·∫ø, ta c√≥ th·ªÉ vi·∫øt:\n",
    "$$f(\\mathbf{x}+\\boldsymbol{\\epsilon})=f(\\mathbf{x})+\\boldsymbol{\\epsilon}^{\\top} \\nabla f(\\mathbf{x})+\\frac{1}{2} \\boldsymbol{\\epsilon}^{\\top} \\nabla^2 f(\\mathbf{x}) \\boldsymbol{\\epsilon}+O\\left(|\\boldsymbol{\\epsilon}|^3\\right).$$\n",
    "ƒê·ªÉ tr√°nh k√Ω hi·ªáu ph·ª©c t·∫°p, ta ƒë·ªãnh nghƒ©a $\\mathbf{H} \\stackrel{\\text{def}}{=} \\nabla^2 f(\\mathbf{x})$ l√† ma tr·∫≠n Hessian c·ªßa $f$, m·ªôt ma tr·∫≠n $d \\times d$. V·ªõi $d$ nh·ªè v√† b√†i to√°n ƒë∆°n gi·∫£n, $\\mathbf{H}$ d·ªÖ t√≠nh to√°n. Tuy nhi√™n, v·ªõi m·∫°ng n∆°-ron s√¢u, $\\mathbf{H}$ c√≥ th·ªÉ qu√° l·ªõn ƒë·ªÉ l∆∞u tr·ªØ, do chi ph√≠ l∆∞u tr·ªØ $O\\left(d^2\\right)$ ph·∫ßn t·ª≠. H∆°n n·ªØa, vi·ªác t√≠nh to√°n qua lan truy·ªÅn ng∆∞·ª£c c√≥ th·ªÉ qu√° t·ªën k√©m. Hi·ªán t·∫°i, h√£y b·ªè qua c√°c c√¢n nh·∫Øc n√†y v√† xem x√©t thu·∫≠t to√°n ta s·∫Ω nh·∫≠n ƒë∆∞·ª£c.\n",
    "Sau c√πng, c·ª±c ti·ªÉu c·ªßa $f$ th·ªèa m√£n $\\nabla f=0$. Theo c√°c quy t·∫Øc vi t√≠ch ph√¢n trong Ph·∫ßn 2.4.3, b·∫±ng c√°ch l·∫•y ƒë·∫°o h√†m c·ªßa (12.3.8) theo $\\epsilon$ v√† b·ªè qua c√°c h·∫°ng b·∫≠c cao, ta ƒë∆∞·ª£c:\n",
    "$$\\nabla f(\\mathbf{x})+\\mathbf{H} \\boldsymbol{\\epsilon}=0 \\text{ v√† do ƒë√≥ } \\boldsymbol{\\epsilon}=-\\mathbf{H}^{-1} \\nabla f(\\mathbf{x}).$$\n",
    "T·ª©c l√†, ta c·∫ßn ngh·ªãch ƒë·∫£o ma tr·∫≠n Hessian $\\mathbf{H}$ nh∆∞ m·ªôt ph·∫ßn c·ªßa b√†i to√°n t·ªëi ∆∞u h√≥a.\n",
    "V√≠ d·ª• ƒë∆°n gi·∫£n, v·ªõi $f(x)=\\frac{1}{2}x^2$, ta c√≥ $\\nabla f(x)=x$ v√† $\\mathbf{H}=1$. Do ƒë√≥, v·ªõi m·ªçi $x$, ta ƒë∆∞·ª£c $\\epsilon=-x$. N√≥i c√°ch kh√°c, ch·ªâ m·ªôt b∆∞·ªõc l√† ƒë·ªß ƒë·ªÉ h·ªôi t·ª• ho√†n h·∫£o m√† kh√¥ng c·∫ßn b·∫•t k·ª≥ ƒëi·ªÅu ch·ªânh n√†o! ·ªû ƒë√¢y, ta h∆°i may m·∫Øn: khai tri·ªÉn Taylor ch√≠nh x√°c v√¨ $f(x+\\epsilon)=\\frac{1}{2}x^2+\\epsilon x+\\frac{1}{2}\\epsilon^2$.\n",
    "H√£y xem ƒëi·ªÅu g√¨ x·∫£y ra trong c√°c b√†i to√°n kh√°c. V·ªõi h√†m hyperbolic cosine l·ªìi $f(x)=\\cosh(cx)$ v·ªõi m·ªôt h·∫±ng s·ªë $c$, ta th·∫•y c·ª±c ti·ªÉu to√†n c·ª•c t·∫°i $x=0$ ƒë∆∞·ª£c ƒë·∫°t sau v√†i l·∫ßn l·∫∑p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25253aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.5)\n",
    "\n",
    "def f(x):  # objective function\n",
    "    return torch.cosh(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return c * torch.sinh(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return c ** 2 * torch.cosh(c * x)\n",
    "\n",
    "def newton(eta=1):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x) / f_hess(x)\n",
    "        results.append(float(x))\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a91933",
   "metadata": {},
   "source": [
    "epoch 10, x: tensor(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9babe",
   "metadata": {},
   "source": [
    "B√¢y gi·ªù, h√£y xem x√©t m·ªôt h√†m kh√¥ng l·ªìi, ch·∫≥ng h·∫°n $f(x)=x \\cos(cx)$ v·ªõi m·ªôt h·∫±ng s·ªë $c$. L∆∞u √Ω r·∫±ng trong ph∆∞∆°ng ph√°p Newton, ta chia cho Hessian. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu ƒë·∫°o h√†m b·∫≠c hai √¢m, ta c√≥ th·ªÉ ƒëi theo h∆∞·ªõng l√†m tƒÉng gi√° tr·ªã c·ªßa $f$. ƒê√≥ l√† m·ªôt l·ªó h·ªïng nghi√™m tr·ªçng c·ªßa thu·∫≠t to√°n. H√£y xem ƒëi·ªÅu g√¨ x·∫£y ra trong th·ª±c t·∫ø."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6d17d-6adb-4733-8fba-3da795c64a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return -2 * c * torch.sin(c * x) - c ** 2 * x * torch.cos(c * x)\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daad94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch 10, x: tensor(26.8341)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bf0ca",
   "metadata": {},
   "source": [
    "ƒêi·ªÅu n√†y ƒë√£ sai l·∫ßm nghi√™m tr·ªçng. L√†m th·∫ø n√†o ƒë·ªÉ s·ª≠a n√≥? M·ªôt c√°ch l√† \"s·ª≠a\" Hessian b·∫±ng c√°ch l·∫•y gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa n√≥. M·ªôt chi·∫øn l∆∞·ª£c kh√°c l√† ƒë∆∞a l·∫°i t·ªëc ƒë·ªô h·ªçc.\n",
    "V·ªõi t·ªëc ƒë·ªô h·ªçc nh·ªè h∆°n m·ªôt ch√∫t, ch·∫≥ng h·∫°n $\\eta=0.5$, ta th·∫•y thu·∫≠t to√°n ho·∫°t ƒë·ªông hi·ªáu qu·∫£ h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd52349",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(newton(0.5), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ccee3",
   "metadata": {},
   "source": [
    "epoch 10, x: tensor(7.2699)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31d0ef",
   "metadata": {},
   "source": [
    "### Ph√¢n T√≠ch H·ªôi T·ª•\n",
    "Ta ch·ªâ ph√¢n t√≠ch t·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa ph∆∞∆°ng ph√°p Newton cho m·ªôt h√†m m·ª•c ti√™u l·ªìi v√† kh·∫£ vi ba l·∫ßn, trong ƒë√≥ ƒë·∫°o h√†m b·∫≠c hai kh√°c kh√¥ng, t·ª©c l√† $f^{\\prime\\prime}>0$. B·∫±ng ch·ª©ng ƒëa bi·∫øn l√† ph·∫ßn m·ªü r·ªông tr·ª±c ti·∫øp c·ªßa l·∫≠p lu·∫≠n m·ªôt chi·ªÅu d∆∞·ªõi ƒë√¢y v√† ƒë∆∞·ª£c b·ªè qua v√¨ n√≥ kh√¥ng gi√∫p nhi·ªÅu v·ªÅ m·∫∑t tr·ª±c gi√°c.\n",
    "G·ªçi $x^{(k)}$ l√† gi√° tr·ªã c·ªßa $x$ t·∫°i l·∫ßn l·∫∑p th·ª© $k$ v√† ƒë·∫∑t $e^{(k)} \\stackrel{\\text{def}}{=} x^{(k)}-x^*$ l√† kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm t·ªëi ∆∞u t·∫°i l·∫ßn l·∫∑p th·ª© $k$. B·∫±ng khai tri·ªÉn Taylor, ta c√≥ ƒëi·ªÅu ki·ªán $f^{\\prime}\\left(x^{(*)}\\right)=0$ c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt l√†:\n",
    "$$0=f^{\\prime}\\left(x^{(k)}-e^{(k)}\\right)=f^{\\prime}\\left(x^{(k)}\\right)-e^{(k)} f^{\\prime\\prime}\\left(x^{(k)}\\right)+\\frac{1}{2}\\left(e^{(k)}\\right)^2 f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right),$$\n",
    "ƒëi·ªÅu n√†y ƒë√∫ng v·ªõi m·ªôt $\\xi^{(k)} \\in \\left[x^{(k)}-e^{(k)}, x^{(k)}\\right]$. Chia khai tri·ªÉn tr√™n cho $f^{\\prime\\prime}\\left(x^{(k)}\\right)$, ta ƒë∆∞·ª£c:\n",
    "$$e^{(k)}-\\frac{f^{\\prime}\\left(x^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)}=\\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)} .$$\n",
    "Nh·ªõ r·∫±ng ta c√≥ c·∫≠p nh·∫≠t $x^{(k+1)}=x^{(k)}-f^{\\prime}\\left(x^{(k)}\\right) / f^{\\prime\\prime}\\left(x^{(k)}\\right)$. Thay v√†o ph∆∞∆°ng tr√¨nh c·∫≠p nh·∫≠t n√†y v√† l·∫•y gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa c·∫£ hai v·∫ø, ta c√≥:\n",
    "$$\\left|e^{(k+1)}\\right|=\\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{\\left|f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)\\right|}{f^{\\prime\\prime}\\left(x^{(k)}\\right)} .$$\n",
    "Do ƒë√≥, b·∫•t c·ª© khi n√†o ta ·ªü trong m·ªôt v√πng c√≥ $\\left|f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)\\right| /\\left(2 f^{\\prime\\prime}\\left(x^{(k)}\\right)\\right) \\leq c$, ta c√≥ sai s·ªë gi·∫£m b·∫≠c hai:\n",
    "$$\\left|e^{(k+1)}\\right| \\leq c\\left(e^{(k)}\\right)^2 .$$\n",
    "L∆∞u √Ω r·∫±ng c√°c nh√† nghi√™n c·ª©u t·ªëi ∆∞u h√≥a g·ªçi ƒë√¢y l√† h·ªôi t·ª• tuy·∫øn t√≠nh, trong khi m·ªôt ƒëi·ªÅu ki·ªán nh∆∞ $\\left|e^{(k+1)}\\right| \\leq \\alpha\\left|e^{(k)}\\right|$ ƒë∆∞·ª£c g·ªçi l√† t·ªëc ƒë·ªô h·ªôi t·ª• h·∫±ng s·ªë. Ph√¢n t√≠ch n√†y ƒëi k√®m v·ªõi m·ªôt s·ªë l∆∞u √Ω. Th·ª© nh·∫•t, ta kh√¥ng th·ª±c s·ª± c√≥ ƒë·∫£m b·∫£o khi n√†o s·∫Ω ƒë·∫°t ƒë∆∞·ª£c v√πng h·ªôi t·ª• nhanh. Thay v√†o ƒë√≥, ta ch·ªâ bi·∫øt r·∫±ng m·ªôt khi ƒë·∫°t ƒë∆∞·ª£c, h·ªôi t·ª• s·∫Ω r·∫•t nhanh. Th·ª© hai, ph√¢n t√≠ch n√†y y√™u c·∫ßu $f$ c√≥ t√≠nh ch·∫•t t·ªët ƒë·∫øn c√°c ƒë·∫°o h√†m b·∫≠c cao. N√≥ ph·ª• thu·ªôc v√†o vi·ªác ƒë·∫£m b·∫£o r·∫±ng $f$ kh√¥ng c√≥ b·∫•t k·ª≥ ƒë·∫∑c t√≠nh \"b·∫•t ng·ªù\" n√†o v·ªÅ c√°ch n√≥ c√≥ th·ªÉ thay ƒë·ªïi gi√° tr·ªã."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e920ef",
   "metadata": {},
   "source": [
    "### Ti·ªÅn ƒêi·ªÅu Ki·ªán H√≥a\n",
    "Kh√¥ng ng·∫°c nhi√™n khi vi·ªác t√≠nh to√°n v√† l∆∞u tr·ªØ to√†n b·ªô Hessian r·∫•t t·ªën k√©m. Do ƒë√≥, vi·ªác t√¨m c√°c gi·∫£i ph√°p thay th·∫ø l√† mong mu·ªën. M·ªôt c√°ch ƒë·ªÉ c·∫£i thi·ªán l√† ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a. N√≥ tr√°nh t√≠nh to√°n to√†n b·ªô Hessian m√† ch·ªâ t√≠nh c√°c ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn c√°c thu·∫≠t to√°n c·∫≠p nh·∫≠t d·∫°ng:\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\operatorname{diag}(\\mathbf{H})^{-1} \\nabla f(\\mathbf{x}) .$$\n",
    "M·∫∑c d√π ƒëi·ªÅu n√†y kh√¥ng t·ªët b·∫±ng ph∆∞∆°ng ph√°p Newton ƒë·∫ßy ƒë·ªß, n√≥ v·∫´n t·ªët h∆°n nhi·ªÅu so v·ªõi vi·ªác kh√¥ng s·ª≠ d·ª•ng. ƒê·ªÉ th·∫•y t·∫°i sao ƒë√¢y l√† √Ω t∆∞·ªüng t·ªët, h√£y xem x√©t m·ªôt t√¨nh hu·ªëng m√† m·ªôt bi·∫øn bi·ªÉu th·ªã chi·ªÅu cao t√≠nh b·∫±ng milimet v√† m·ªôt bi·∫øn kh√°c bi·ªÉu th·ªã chi·ªÅu cao t√≠nh b·∫±ng kil√¥m√©t. Gi·∫£ s·ª≠ r·∫±ng v·ªõi c·∫£ hai, t·ª∑ l·ªá t·ª± nhi√™n l√† m√©t, ta c√≥ s·ª± kh√¥ng kh·ªõp l·ªõn trong tham s·ªë h√≥a. May m·∫Øn thay, vi·ªác s·ª≠ d·ª•ng ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a s·∫Ω lo·∫°i b·ªè ƒëi·ªÅu n√†y. Hi·ªáu qu·∫£, ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a v·ªõi h·∫° gradient t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác ch·ªçn m·ªôt t·ªëc ƒë·ªô h·ªçc kh√°c nhau cho m·ªói bi·∫øn (t·ªça ƒë·ªô c·ªßa vector $\\mathbf{x}$). Nh∆∞ ta s·∫Ω th·∫•y sau, ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a th√∫c ƒë·∫©y m·ªôt s·ªë c·∫£i ti·∫øn trong c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a h·∫° gradient ng·∫´u nhi√™n.\n",
    "### H·∫° Gradient v·ªõi T√¨m Ki·∫øm Tuy·∫øn\n",
    "M·ªôt trong nh·ªØng v·∫•n ƒë·ªÅ ch√≠nh trong h·∫° gradient l√† ta c√≥ th·ªÉ v∆∞·ª£t qu√° m·ª•c ti√™u ho·∫∑c ti·∫øn b·ªô kh√¥ng ƒë·ªß. M·ªôt c√°ch s·ª≠a ƒë∆°n gi·∫£n l√† s·ª≠ d·ª•ng t√¨m ki·∫øm tuy·∫øn k·∫øt h·ª£p v·ªõi h·∫° gradient. T·ª©c l√†, ta s·ª≠ d·ª•ng h∆∞·ªõng ƒë∆∞·ª£c cho b·ªüi $\\nabla f(\\mathbf{x})$ v√† sau ƒë√≥ th·ª±c hi·ªán t√¨m ki·∫øm nh·ªã ph√¢n ƒë·ªÉ x√°c ƒë·ªãnh t·ªëc ƒë·ªô h·ªçc $\\eta$ n√†o t·ªëi ∆∞u h√≥a $f(\\mathbf{x}-\\eta \\nabla f(\\mathbf{x}))$.\n",
    "Thu·∫≠t to√°n n√†y h·ªôi t·ª• nhanh ch√≥ng (xem ph√¢n t√≠ch v√† ch·ª©ng minh, v√≠ d·ª•, Boyd v√† Vandenberghe (2004)). Tuy nhi√™n, ƒë·ªëi v·ªõi m·ª•c ƒë√≠ch h·ªçc s√¢u, ƒëi·ªÅu n√†y kh√¥ng th·ª±c s·ª± kh·∫£ thi, v√¨ m·ªói b∆∞·ªõc c·ªßa t√¨m ki·∫øm tuy·∫øn s·∫Ω y√™u c·∫ßu ƒë√°nh gi√° h√†m m·ª•c ti√™u tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu. ƒêi·ªÅu n√†y qu√° t·ªën k√©m ƒë·ªÉ th·ª±c hi·ªán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022be34-d4b4-4a4f-9aa2-7564b660f378",
   "metadata": {},
   "source": [
    "## Excercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ce5c4-ad2a-4341-85eb-86181e44d9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
