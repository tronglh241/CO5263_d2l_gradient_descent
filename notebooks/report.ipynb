{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb525dec-a9af-4d99-a38f-a390976cd8ca",
   "metadata": {},
   "source": [
    "# 01. Gi·ªõi thi·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfc60d-09d5-4be8-9f35-92936b14131a",
   "metadata": {},
   "source": [
    "## B√†i to√°n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8be8fc-eb89-490b-9b71-48ca44785602",
   "metadata": {},
   "source": [
    "Nhi·ªÅu b√†i to√°n t·ªëi ∆∞u trong khoa h·ªçc m√°y t√≠nh v√† h·ªçc m√°y li√™n quan ƒë·∫øn vi·ªác t·ªëi thi·ªÉu h√≥a m·ªôt h√†m m·∫•t m√°t (loss function), h√†m n√†y ƒëo l∆∞·ªùng s·ª± kh√°c bi·ªát gi·ªØa d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh v√† gi√° tr·ªã th·ª±c t·∫ø. C√°c b√†i to√°n n√†y th∆∞·ªùng c√≥ d·∫°ng t·ªëi thi·ªÉu t·ªïng c·ªßa nhi·ªÅu h√†m kh·∫£ vi, ch·∫≥ng h·∫°n nh∆∞:\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{w}),\n",
    "$$\n",
    "trong ƒë√≥ m·ªói $f_i$ t∆∞∆°ng ·ª©ng v·ªõi gi√° tr·ªã m·∫•t m√°t c·ªßa ƒëi·ªÉm d·ªØ li·ªáu th·ª© $i$. Vi·ªác t·ªëi thi·ªÉu h√≥a tr·ª±c ti·∫øp h√†m m·∫•t m√°t n√†y c√≥ th·ªÉ r·∫•t t·ªën k√©m v·ªÅ t√≠nh to√°n khi t·∫≠p d·ªØ li·ªáu l·ªõn, do ƒë√≥ c√°c thu·∫≠t to√°n t·ªëi ∆∞u hi·ªáu qu·∫£ l√† r·∫•t c·∫ßn thi·∫øt. Ph∆∞∆°ng ph√°p Gradient Descent v√† c√°c bi·∫øn th·ªÉ c·ªßa n√≥ nh∆∞ Stochastic Gradient Descent (SGD) v√† Mini-batch SGD l√† m·ªôt trong nh·ªØng ph∆∞∆°ng ph√°p t·ªëi ∆∞u ƒë∆∞·ª£c s·ª≠ d·ª•ng kh√° r·ªông r√£i ƒë·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n nh∆∞ v·∫≠y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d46ec90-2b92-4bb3-b3b3-1e579db7f44a",
   "metadata": {},
   "source": [
    "## ·ª®ng d·ª•ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a91e0f-2d95-420b-869b-711909a38ea9",
   "metadata": {},
   "source": [
    "Gradient Descent (GD), Stochastic Gradient Descent (SGD) v√† Mini-batch SGD l√† c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a c∆° b·∫£n v·ªõi nhi·ªÅu ·ª©ng d·ª•ng r·ªông r√£i, ƒë·∫∑c bi·ªát trong lƒ©nh v·ª±c h·ªçc m√°y. Trong ƒë√≥, **deep learning (h·ªçc s√¢u)** l√† lƒ©nh v·ª±c n·ªïi b·∫≠t nh·∫•t m√† c√°c ph∆∞∆°ng ph√°p n√†y ƒë√£ ch·ª©ng t·ªè hi·ªáu qu·∫£ v∆∞·ª£t tr·ªôi. Vi·ªác hu·∫•n luy·ªán c√°c m·∫°ng n∆°-ron s√¢u ƒë√≤i h·ªèi ph·∫£i t·ªëi ∆∞u c√°c h√†m m·∫•t m√°t phi l·ªìi v·ªõi h√†ng tri·ªáu ƒë·∫øn h√†ng t·ª∑ tham s·ªë - m·ªôt nhi·ªám v·ª• g·∫ßn nh∆∞ b·∫•t kh·∫£ thi n·∫øu kh√¥ng c√≥ c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a hi·ªáu qu·∫£ nh∆∞ SGD v√† c√°c bi·∫øn th·ªÉ c·ªßa n√≥.\n",
    "\n",
    "C√°c ph∆∞∆°ng ph√°p n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán nh·ªØng m√¥ h√¨nh hi·ªán ƒë·∫°i nh·∫•t trong c√°c b√†i to√°n nh∆∞ **nh·∫≠n di·ªán h√¨nh ·∫£nh, x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, nh·∫≠n d·∫°ng gi·ªçng n√≥i, v√† c√°c m√¥ h√¨nh sinh nh∆∞ GAN ho·∫∑c c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM)**. Mini-batch SGD ƒë·∫∑c bi·ªát h·ªØu √≠ch khi c√¢n b·∫±ng gi·ªØa t√≠nh nhi·ªÖu c·ªßa SGD thu·∫ßn t√∫y v√† chi ph√≠ t√≠nh to√°n cao c·ªßa GD to√†n b·ªô, nh·ªù ƒë√≥ tr·ªü th√†nh l·ª±a ch·ªçn ti√™u chu·∫©n trong h·∫ßu h·∫øt c√°c framework h·ªçc s√¢u hi·ªán nay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8242908-b3a8-47a2-a268-03dd1032d133",
   "metadata": {},
   "source": [
    "## M·ªôt s·ªë b√†i to√°n khoa h·ªçc m√°y t√≠nh ƒë√£ ƒë∆∞·ª£c gi·∫£i b·∫±ng c√°c ph∆∞∆°ng ph√°p n√†y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17220e0e-66d1-4616-9d8c-540774029dc3",
   "metadata": {},
   "source": [
    "Gradient Descent v√† c√°c bi·∫øn th·ªÉ c·ªßa n√≥ kh√¥ng ch·ªâ l√† c√¥ng c·ª• t·ªëi ∆∞u h√≥a m√† c√≤n ƒë√≥ng vai tr√≤ trung t√¢m trong vi·ªác gi·∫£i quy·∫øt nhi·ªÅu b√†i to√°n quan tr·ªçng trong khoa h·ªçc m√°y t√≠nh. M·ªôt s·ªë v√≠ d·ª• ti√™u bi·ªÉu k·ªÉ ƒë·∫øn nh∆∞:\n",
    "\n",
    "- **Hu·∫•n luy·ªán m·∫°ng n∆°-ron s√¢u (Deep Neural Networks):** C√°c m√¥ h√¨nh nh∆∞ ResNet, BERT, GPT,‚Ä¶ ƒë·ªÅu ƒë∆∞·ª£c hu·∫•n luy·ªán b·∫±ng mini-batch SGD ho·∫∑c c√°c bi·∫øn th·ªÉ n√¢ng cao c·ªßa n√≥ nh∆∞ Adam, RMSProp. Nh·ªØng m√¥ h√¨nh n√†y ƒë·∫°t hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi trong c√°c t√°c v·ª• nh∆∞ ph√¢n lo·∫°i ·∫£nh, d·ªãch m√°y, v√† t·∫°o sinh vƒÉn b·∫£n.\n",
    "\n",
    "- **H·ªá th·ªëng g·ª£i √Ω (Recommendation Systems):** T·ªëi ∆∞u h√≥a h√†m m·∫•t m√°t trong m√¥ h√¨nh ma tr·∫≠n ti·ªÅm ·∫©n ho·∫∑c m√¥ h√¨nh h·ªçc s√¢u g·ª£i √Ω (Deep Recommender Systems) ƒë·ªÅu c·∫ßn ƒë·∫øn c√°c thu·∫≠t to√°n t·ªëi ∆∞u nh∆∞ SGD.\n",
    "\n",
    "- **Th·ªã gi√°c m√°y t√≠nh (Computer Vision):** Trong c√°c b√†i to√°n nh∆∞ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng, ph√¢n ƒëo·∫°n ·∫£nh, v√† t·∫°o ·∫£nh, mini-batch SGD ƒë∆∞·ª£c d√πng ƒë·ªÉ t·ªëi ∆∞u c√°c m√¥ h√¨nh h·ªçc s√¢u ph·ª©c t·∫°p v·ªõi d·ªØ li·ªáu ·∫£nh quy m√¥ l·ªõn.\n",
    "\n",
    "- **H·ªçc tƒÉng c∆∞·ªùng (Reinforcement Learning):** C√°c thu·∫≠t to√°n nh∆∞ Policy Gradient, Deep Q-Learning s·ª≠ d·ª•ng SGD ƒë·ªÉ c·∫≠p nh·∫≠t ch√≠nh s√°ch ho·∫∑c h√†m gi√° tr·ªã d·ª±a tr√™n tr·∫£i nghi·ªám t·ª´ m√¥i tr∆∞·ªùng.\n",
    "\n",
    "Nh·ªØng v√≠ d·ª• tr√™n cho th·∫•y t·∫ßm quan tr·ªçng r·ªông kh·∫Øp c·ªßa GD, SGD v√† Mini-batch SGD trong vi·ªác gi·∫£i quy·∫øt c√°c b√†i to√°n c·ªët l√µi c·ªßa khoa h·ªçc m√°y t√≠nh hi·ªán ƒë·∫°i, ƒë·∫∑c bi·ªát khi d·ªØ li·ªáu v√† m√¥ h√¨nh ng√†y c√†ng l·ªõn v√† ph·ª©c t·∫°p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbf639-4c63-4693-8946-139fbc148da6",
   "metadata": {},
   "source": [
    "# 02. Gradient Descent\n",
    "Gradient descent l√† m·ªôt thu·∫≠t to√°n t·ªëi ∆∞u h√≥a th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√†o t·∫°o c√°c m√¥ h√¨nh h·ªçc m√°y v√† m·∫°ng n∆°-ron. N√≥ ƒë√†o t·∫°o c√°c m√¥ h√¨nh h·ªçc m√°y b·∫±ng c√°ch gi·∫£m thi·ªÉu l·ªói gi·ªØa k·∫øt qu·∫£ d·ª± ƒëo√°n v√† k·∫øt qu·∫£ th·ª±c t·∫ø.\n",
    "Thu·∫≠t to√°n ho·∫°t ƒë·ªông b·∫±ng c√°ch ƒëi·ªÅu ch·ªânh li√™n t·ª•c c√°c tham s·ªë c·ªßa m√¥ h√¨nh (nh∆∞ tr·ªçng s·ªë v√† ƒë·ªô l·ªách) theo h∆∞·ªõng l√†m gi·∫£m chi ph√≠ nhi·ªÅu nh·∫•t. H∆∞·ªõng n√†y ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng c√°ch t√≠nh to√°n ƒë·ªô d·ªëc (h∆∞·ªõng ƒë·∫øn m·ª©c tƒÉng chi ph√≠ nhi·ªÅu nh·∫•t) c·ªßa h√†m chi ph√≠ li√™n quan ƒë·∫øn c√°c tham s·ªë, sau ƒë√≥ di chuy·ªÉn c√°c tham s·ªë theo h∆∞·ªõng ng∆∞·ª£c l·∫°i.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf1d29-f160-4a3a-854b-3a8748e13307",
   "metadata": {},
   "source": [
    "## One-Dimensional Gradient Descent\n",
    "X√©t m·ªôt h√†m th·ª±c kh·∫£ vi li√™n t·ª•c $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ . S·ª≠ d·ª•ng khai tri·ªÉn Taylor:\n",
    "$$f(x+\\epsilon)=f(x)+\\epsilon f^{\\prime}(x)+O\\left(\\epsilon^2\\right). \\tag{1}$$ \n",
    "T·ª©c l√†, trong x·∫•p x·ªâ b·∫≠c m·ªôt, $f(x+\\epsilon)$ ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi gi√° tr·ªã h√†m $f(x)$ v√† ƒë·∫°o h√†m b·∫≠c m·ªôt $f^{\\prime}(x)$ t·∫°i $x$. C√≥ th·ªÉ gi·∫£ ƒë·ªãnh r·∫±ng v·ªõi $\\epsilon$ nh·ªè, vi·ªác di chuy·ªÉn theo h∆∞·ªõng gradient √¢m s·∫Ω gi·∫£m $f$. Ch·ªçn m·ªôt k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh $\\eta>0$ v√† ch·ªçn $\\epsilon=-\\eta f^{\\prime}(x)$. Thay v√†o khai tri·ªÉn Taylor ·ªü tr√™n:\n",
    "$$f\\left(x-\\eta f^{\\prime}(x)\\right)=f(x)-\\eta f^{\\prime 2}(x)+O\\left(\\eta^2 f^{\\prime 2}(x)\\right).\\tag{2}$$\n",
    "N·∫øu ƒë·∫°o h√†m $f^{\\prime}(x) \\neq 0$ kh√¥ng ti√™u bi·∫øn, ta ƒë·∫°t ƒë∆∞·ª£c b∆∞·ªõc ti·∫øn t·ªõi ƒëi·ªÉm t·ªëi ∆∞u v√¨ $\\eta f^{\\prime 2}(x)>0$. H∆°n n·ªØa, ta lu√¥n c√≥ th·ªÉ ch·ªçn $\\eta$ ƒë·ªß nh·ªè ƒë·ªÉ c√°c h·∫°ng b·∫≠c cao tr·ªü n√™n kh√¥ng ƒë√°ng k·ªÉ. Do ƒë√≥:\n",
    "$$f\\left(x-\\eta f^{\\prime}(x)\\right) \\lessapprox f(x). \\tag{3}$$\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†, n·∫øu ta s·ª≠ d·ª•ng:\n",
    "$$x \\leftarrow x-\\eta f^{\\prime}(x). \\tag{4}$$\n",
    "ƒë·ªÉ l·∫∑p $x$, gi√° tr·ªã c·ªßa h√†m $f(x)$ c√≥ th·ªÉ gi·∫£m. Do ƒë√≥, trong gradient descent, ta ƒë·∫ßu ti√™n ch·ªçn m·ªôt gi√° tr·ªã ban ƒë·∫ßu $x$ v√† m·ªôt h·∫±ng s·ªë $\\eta>0$, sau ƒë√≥ s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ l·∫∑p $x$ li√™n t·ª•c cho ƒë·∫øn khi ƒë·∫°t ƒëi·ªÅu ki·ªán d·ª´ng, v√≠ d·ª•, khi ƒë·ªô l·ªõn c·ªßa gradient $\\left|f^{\\prime}(x)\\right|$ ƒë·ªß nh·ªè ho·∫∑c s·ªë l·∫ßn l·∫∑p ƒë·∫°t m·ªôt gi√° tr·ªã nh·∫•t ƒë·ªãnh.\n",
    "### V√≠ d·ª• minh h·ªça\n",
    "Ta ch·ªçn h√†m m·ª•c ti√™u $f(x)=x^2 +5sin(x)$ ƒë·ªÉ minh h·ªça c√°ch th·ª±c hi·ªán gradient descent. ƒê·∫°o h√†m: $f^{\\prime}(x) =2x + 5cos(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5239bff-4d53-470c-98d1-8115d76c38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import d2l\n",
    "def f(x):  # objective function\n",
    "    return x**2 + 5 * np.sin(x)\n",
    "\n",
    "def f_grad(x):  # Gradient (derivative) of the objective function\n",
    "    return 2*x + 5 * np.cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f83e0",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ta s·ª≠ d·ª•ng $x=-5$ l√†m gi√° tr·ªã ban ƒë·∫ßu v√† gi·∫£ s·ª≠ $\\eta=0.1$. S·ª≠ d·ª•ng gradient descent ƒë·ªÉ l·∫∑p $x$ 15 l·∫ßn, ta c√≥ th·ªÉ th·∫•y r·∫±ng x xu·∫•t ph√°t t·ª´ b√™n tr√°i v√† cu·ªëi c√πng, gi√° tr·ªã c·ªßa $x$ ti·∫øn g·∫ßn ƒë·∫øn nghi·ªám t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(eta, f_grad, start_x, step):\n",
    "    x = start_x\n",
    "    results = [x]\n",
    "    for i in range(step):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(x)\n",
    "    print(f'epoch 11, x: {x:.6f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.1, f_grad,-5,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73410907",
   "metadata": {},
   "source": [
    "Ti·∫øn tr√¨nh t·ªëi ∆∞u h√≥a $x$ c√≥ th·ªÉ ƒë∆∞·ª£c v·∫Ω nh∆∞ sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = np.arange(-n, n, 0.01)\n",
    "    d2l.set_figsize()\n",
    "    d2l.plot([f_line, results], [[f(x) for x in f_line], [f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86e155-23d6-4686-ba20-e5492cd75f5e",
   "metadata": {},
   "source": [
    "Ta s·ª≠ d·ª•ng $x=4$ l√†m gi√° tr·ªã ban ƒë·∫ßu v√† gi·∫£ s·ª≠ $\\eta=0.1$. S·ª≠ d·ª•ng gradient descent ƒë·ªÉ l·∫∑p $x$ 30 l·∫ßn, ta c√≥ th·ªÉ th·∫•y r·∫±ng x xu·∫•t ph√°t t·ª´ b√™n ph·∫£i v√† ƒëi d·∫ßn t·ªõi nghi·ªám t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c8e2c-b80d-4786-9d64-b1418b7de522",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gd(0.1, f_grad,4,25)\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fea95",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "T·ªëc ƒë·ªô h·ªçc (learning rate) ùúÇ c√≥ th·ªÉ ƒë∆∞·ª£c thi·∫øt l·∫≠p b·ªüi ng∆∞·ªùi thi·∫øt k·∫ø thu·∫≠t to√°n. N·∫øu ch√∫ng ta s·ª≠ d·ª•ng m·ªôt t·ªëc ƒë·ªô h·ªçc qu√° nh·ªè, n√≥ s·∫Ω khi·∫øn `ùë•` c·∫≠p nh·∫≠t r·∫•t ch·∫≠m, ƒë√≤i h·ªèi nhi·ªÅu v√≤ng l·∫∑p h∆°n ƒë·ªÉ thu ƒë∆∞·ª£c nghi·ªám t·ªët h∆°n. ƒê·ªÉ minh h·ªça ƒëi·ªÅu x·∫£y ra trong tr∆∞·ªùng h·ª£p nh∆∞ v·∫≠y, h√£y xem x√©t ti·∫øn tr√¨nh trong c√πng b√†i to√°n t·ªëi ∆∞u v·ªõi ùúÇ = 0.02. Nh∆∞ ta c√≥ th·ªÉ th·∫•y, ngay c·∫£ sau 10 b∆∞·ªõc l·∫∑p, ch√∫ng ta v·∫´n c√≤n c√°ch xa nghi·ªám t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(0.02, f_grad,-5,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d3615",
   "metadata": {},
   "source": [
    "Ng∆∞·ª£c l·∫°i, n·∫øu ch√∫ng ta s·ª≠ d·ª•ng t·ªëc ƒë·ªô h·ªçc qu√° l·ªõn, gi√° tr·ªã `|ùúÇ ùëì'(ùë•)|` c√≥ th·ªÉ tr·ªü n√™n qu√° l·ªõn ƒë·ªëi v·ªõi c√¥ng th·ª©c khai tri·ªÉn Taylor b·∫≠c nh·∫•t. Nghƒ©a l√†, s·ªë h·∫°ng `O (ùúÇ¬≤ ùëì'¬≤(ùë•))` trong c√¥ng th·ª©c (2) c√≥ th·ªÉ tr·ªü n√™n ƒë√°ng k·ªÉ. Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta kh√¥ng th·ªÉ ƒë·∫£m b·∫£o r·∫±ng qu√° tr√¨nh c·∫≠p nh·∫≠t l·∫∑p c·ªßa `ùë•` s·∫Ω l√†m gi·∫£m gi√° tr·ªã c·ªßa h√†m `ùëì(ùë•)`. V√≠ d·ª•, khi ch√∫ng ta ƒë·∫∑t t·ªëc ƒë·ªô h·ªçc `ùúÇ = 1.1`, `ùë•` v∆∞·ª£t qu√° (overshoots) nghi·ªám t·ªëi ∆∞u `ùë• = 0` v√† d·∫ßn d·∫ßn ph√¢n k·ª≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(1.1, f_grad,-5,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c64e8b",
   "metadata": {},
   "source": [
    "### Local Minima\n",
    "ƒê·ªÉ minh h·ªça ƒëi·ªÅu g√¨ x·∫£y ra v·ªõi c√°c h√†m kh√¥ng l·ªìi, h√£y xem x√©t tr∆∞·ªùng h·ª£p $f(x) = 0.25x^4 - \\frac{1}{3}x^3 - 1.5x^2 + 2x$ . H√†m n√†y c√≥  c·ª±c ti·ªÉu c·ª•c b·ªô t·∫°i $x = 2$, v·ªõi $f(2) \\approx -0.67$ v√† c·ª±c ti·ªÉu to√†n c·ª•c n·∫±m t·∫°i $x \\approx -1.618$, v·ªõi $f(-1.618) \\approx -4.04$. T√πy thu·ªôc v√†o l·ª±a ch·ªçn t·ªëc ƒë·ªô h·ªçc v√† m·ª©c ƒë·ªô ƒëi·ªÅu ki·ªán c·ªßa b√†i to√°n, ta c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ªôt trong nhi·ªÅu nghi·ªám. V√≠ d·ª• d∆∞·ªõi ƒë√¢y minh h·ªça c√°ch m·ªôt t·ªëc ƒë·ªô h·ªçc cao s·∫Ω d·∫´n ƒë·∫øn m·ªôt c·ª±c ti·ªÉu c·ª•c b·ªô k√©m. V·ªõi t·ªëc ƒë·ªô h·ªçc 0.08 gi√° tr·ªã c·ªßa x b·∫Øt ƒë·∫ßu t·∫°i -4 v√† ƒëi v·ªÅ ph√≠a b√™n ph·∫£i v√† v∆∞·ª£t qua c·ª±c ti·ªÉu to√†n c·ª•c r·ªìi ƒëi t·ªõi c·ª±c ti·ªÉu c·ª•c b·ªô t·∫°i $x=2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):  # Objective function\n",
    "    return 0.25*x**4 - (1/3)*x**3 - 1.5*x**2 + 2*x\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return x**3 -x**2-3*x+2\n",
    "\n",
    "show_trace(gd(0.08, f_grad,-4,20), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c54212-5549-40c3-a71a-0f979d29431c",
   "metadata": {},
   "source": [
    "## Multivariate Gradient Descent\n",
    "Xem x√©t t√¨nh hu·ªëng m√† $\\mathbf{x}=\\left[x_1, x_2, \\ldots, x_d\\right]^{\\top}$. T·ª©c l√†, h√†m m·ª•c ti√™u $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ √°nh x·∫° c√°c vector th√†nh s·ªë th·ª±c. Gradient c·ªßa n√≥ c≈©ng l√† ƒëa bi·∫øn, l√† m·ªôt vector g·ªìm $d$ ƒë·∫°o h√†m ri√™ng:\n",
    "$$\\nabla f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\\right]^{\\top}. \\tag{5}$$\n",
    "M·ªói ph·∫ßn t·ª≠ ƒë·∫°o h√†m ri√™ng $\\partial f(\\mathbf{x}) / \\partial x_i$ trong gradient bi·ªÉu th·ªã t·ªëc ƒë·ªô thay ƒë·ªïi c·ªßa $f$ t·∫°i $\\mathbf{x}$ theo ƒë·∫ßu v√†o $x_i$. Nh∆∞ trong tr∆∞·ªùng h·ª£p m·ªôt bi·∫øn, ta c√≥ th·ªÉ s·ª≠ d·ª•ng x·∫•p x·ªâ Taylor ƒëa bi·∫øn ƒë·ªÉ c√≥ √Ω t∆∞·ªüng v·ªÅ vi·ªác n√™n l√†m g√¨. C·ª• th·ªÉ, ta c√≥:\n",
    "$$f(\\mathbf{x}+\\boldsymbol{\\epsilon})=f(\\mathbf{x})+\\boldsymbol{\\epsilon}^{\\top} \\nabla f(\\mathbf{x})+O\\left(|\\boldsymbol{\\epsilon}|^2\\right). \\tag{6}$$\n",
    "N√≥i c√°ch kh√°c, ƒë·∫øn c√°c h·∫°ng b·∫≠c hai trong $\\epsilon$, h∆∞·ªõng gi·∫£m nhanh nh·∫•t ƒë∆∞·ª£c cho b·ªüi gradient √¢m $-\\nabla f(\\mathbf{x})$. Ch·ªçn m·ªôt t·ªëc ƒë·ªô h·ªçc ph√π h·ª£p $\\eta>0$ cho ra thu·∫≠t to√°n gradient descent nguy√™n m·∫´u:\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathbf{x}). \\tag{7}$$\n",
    "L·∫•y v√≠ d·ª• h√†m m·ª•c ti√™u $f(\\mathbf{x})=x_1^2+10x_2^2$ v·ªõi vector hai chi·ªÅu $\\mathbf{x}=\\left[x_1, x_2\\right]^{\\top}$ l√†m ƒë·∫ßu v√†o v√† m·ªôt s·ªë th·ª±c l√†m ƒë·∫ßu ra. Gradient ƒë∆∞·ª£c cho b·ªüi $\\nabla f(\\mathbf{x})=\\left[2x_1, 20x_2\\right]^{\\top}$. Ta s·∫Ω quan s√°t qu·ªπ ƒë·∫°o c·ªßa $\\mathbf{x}$ b·∫±ng gradient descent t·ª´ v·ªã tr√≠ ban ƒë·∫ßu $[-5, -2]$.\n",
    "C·∫ßn hai h√†m h·ªó tr·ª£, h√†m ƒë·∫ßu ti√™n s·ª≠ d·ª•ng m·ªôt h√†m c·∫≠p nh·∫≠t v√† √°p d·ª•ng n√≥ 20 l·∫ßn cho gi√° tr·ªã ban ƒë·∫ßu. H√†m h·ªó tr·ª£ th·ª© hai tr·ª±c quan h√≥a qu·ªπ ƒë·∫°o c·ªßa $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f95578-6a5b-4d14-839c-c2b3618aba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2d(trainer, steps=20, f_grad=None):  # save\n",
    "    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n",
    "    # s1 and s2 are internal state variables that will be used in Momentum,\n",
    "    # Adagrad, RMSProp\n",
    "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
    "    results = [(x1, x2)]\n",
    "    for i in range(steps):\n",
    "        if f_grad:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
    "        else:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
    "        results.append((x1, x2))\n",
    "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
    "    return results\n",
    "\n",
    "def show_trace_2d(f, results):  # save\n",
    "    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
    "    x1, x2 = torch.meshgrid(torch.arange(-10, 1.0, 0.1),\n",
    "                            torch.arange(-3.0, 1.0, 0.1), indexing='ij')\n",
    "    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
    "    d2l.plt.xlabel('x1')\n",
    "    d2l.plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ce2ad",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ta quan s√°t qu·ªπ ƒë·∫°o c·ªßa bi·∫øn t·ªëi ∆∞u $\\mathbf{x}$ v·ªõi t·ªëc ƒë·ªô h·ªçc $\\eta=0.1$. Ta th·∫•y r·∫±ng sau 20 b∆∞·ªõc, gi√° tr·ªã c·ªßa $\\mathbf{x}$ ti·∫øn g·∫ßn ƒë·∫øn c·ª±c ti·ªÉu t·∫°i $[0, 0]$. Ti·∫øn tr√¨nh kh√° ·ªïn ƒë·ªãnh m·∫∑c d√π kh√° ch·∫≠m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2d(x1, x2): # Objective function\n",
    "    return x1 ** 2 + 10 * x2 ** 2\n",
    "def f_2d_grad(x1, x2): # Gradient of the objective function\n",
    "    return (2 * x1, 20 * x2)\n",
    "def gd_2d(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
    "eta = 0.02\n",
    "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b8123-eec4-40cf-921c-a9e71351ced9",
   "metadata": {},
   "source": [
    "## Adaptive Methods\n",
    "Vi·ªác ch·ªçn t·ªëc ƒë·ªô h·ªçc $\\eta$ \"v·ª´a ƒë√∫ng\" l√† m·ªôt vi·ªác kh√≥ khƒÉn. N·∫øu ch·ªçn qu√° nh·ªè, ti·∫øn b·ªô r·∫•t √≠t. N·∫øu ch·ªçn qu√° l·ªõn, nghi·ªám s·∫Ω dao ƒë·ªông v√† trong tr∆∞·ªùng h·ª£p x·∫•u nh·∫•t, c√≥ th·ªÉ ph√¢n k·ª≥. C√°c ph∆∞∆°ng ph√°p b·∫≠c hai, kh√¥ng ch·ªâ xem x√©t gi√° tr·ªã v√† gradient c·ªßa h√†m m·ª•c ti√™u m√† c√≤n xem x√©t ƒë·ªô cong c·ªßa n√≥, c√≥ gi√∫p √≠ch trong tr∆∞·ªùng h·ª£p n√†y. M·∫∑c d√π c√°c ph∆∞∆°ng ph√°p n√†y kh√¥ng th·ªÉ √°p d·ª•ng tr·ª±c ti·∫øp cho deep learning do chi ph√≠ t√≠nh to√°n, ch√∫ng cung c·∫•p tr·ª±c gi√°c h·ªØu √≠ch ƒë·ªÉ thi·∫øt k·∫ø c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a n√¢ng cao, m√¥ ph·ªèng nhi·ªÅu ƒë·∫∑c t√≠nh mong mu·ªën c·ªßa c√°c thu·∫≠t to√°n ƒë∆∞·ª£c tr√¨nh b√†y d∆∞·ªõi ƒë√¢y.\n",
    "### Ph∆∞∆°ng Ph√°p Newton\n",
    "Xem l·∫°i khai tri·ªÉn Taylor c·ªßa h√†m $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$, kh√¥ng c·∫ßn d·ª´ng l·∫°i sau h·∫°ng ƒë·∫ßu ti√™n:\n",
    "$$f(\\mathbf{x}+\\boldsymbol{\\epsilon})=f(\\mathbf{x})+\\boldsymbol{\\epsilon}^{\\top} \\nabla f(\\mathbf{x})+\\frac{1}{2} \\boldsymbol{\\epsilon}^{\\top} \\nabla^2 f(\\mathbf{x}) \\boldsymbol{\\epsilon}+O\\left(|\\boldsymbol{\\epsilon}|^3\\right).\\tag{8}$$\n",
    "ƒê·ªÉ tr√°nh k√Ω hi·ªáu ph·ª©c t·∫°p, ta ƒë·ªãnh nghƒ©a $\\mathbf{H} \\stackrel{\\text{def}}{=} \\nabla^2 f(\\mathbf{x})$ l√† ma tr·∫≠n Hessian c·ªßa $f$, m·ªôt ma tr·∫≠n $d \\times d$.\n",
    "Sau c√πng, c·ª±c ti·ªÉu c·ªßa $f$ th·ªèa m√£n $\\nabla f=0$. B·∫±ng c√°ch l·∫•y ƒë·∫°o h√†m c·ªßa (8) theo $\\epsilon$ v√† b·ªè qua c√°c h·∫°ng b·∫≠c cao:\n",
    "\n",
    "1. **S·ªë h·∫°ng ƒë·∫ßu ti√™n: $ f(x) $**  \n",
    "   H√†m $ f(x) $ l√† h·∫±ng s·ªë ƒë·ªëi v·ªõi $\\epsilon$ b·ªüi v√¨ $ x $ l√† c·ªë ƒë·ªãnh. Cho n√™n:\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\epsilon} f(x) = 0\n",
    "   $$\n",
    "\n",
    "2. **S·ªë h·∫°ng th·ª© 2: $ \\epsilon^T \\nabla f(x) $**\n",
    "\n",
    "    $$ \\boldsymbol{\\epsilon}^\\top \\nabla f(\\mathbf{x})\n",
    "    = \\epsilon_1 \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}\n",
    "    + \\epsilon_2 \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}\n",
    "    + \\cdots\n",
    "    + \\epsilon_d \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\n",
    "    = \\sum_{i=1}^{d} \\epsilon_i \\frac{\\partial f(\\mathbf{x})}{\\partial x_i} $$\n",
    "    \n",
    "    V·ªõi\n",
    "    \n",
    "    $$\n",
    "    \\nabla f(\\mathbf{x}) =\n",
    "    \\left[\n",
    "    \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\n",
    "    \\right]^\\top\n",
    "    $$\n",
    "    \n",
    "    l√† gradient c·ªßa  $f$ t·∫°i $\\mathbf{x}$. Suy ra\n",
    "\n",
    "   $$\n",
    "   \\epsilon^T \\nabla f(x) = \\sum_{i=1}^d \\epsilon_i \\frac{\\partial f(x)}{\\partial x_i}\n",
    "   $$\n",
    "    \n",
    "   T√≠nh to√°n ƒë·∫°o h√†m ri√™ng v·ªõi $\\epsilon_j$:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\epsilon_j} \\left( \\sum_{i=1}^d \\epsilon_i \\frac{\\partial f(x)}{\\partial x_i} \\right) = \\frac{\\partial f(x)}{\\partial x_j}\n",
    "   $$\n",
    "\n",
    "   Gradient l√†:\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\epsilon} (\\epsilon^T \\nabla f(x)) = \\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f(x)}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_d} \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "4. **S·ªë h·∫°ng th·ª© 3: $ \\frac{1}{2} \\epsilon^T \\nabla^2 f(x) \\epsilon $**  \n",
    "   $$\n",
    "   \\frac{1}{2} \\epsilon^T \\nabla^2 f(x) \\epsilon = \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\epsilon_i \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}\n",
    "   $$\n",
    "\n",
    "   ƒê·∫°o h√†m ri√™ng ƒë·ªëi v·ªõi $\\epsilon_k$:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\epsilon_k} \\left( \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\epsilon_i \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j} \\right) = \\frac{1}{2} \\left( \\sum_{j=1}^d \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_j} + \\sum_{i=1}^d \\epsilon_i \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_k} \\right)\n",
    "   $$\n",
    "\n",
    "   B·ªüi v√¨ $\\nabla^2 f(x)$ ƒë·ªëi x·ª©ng n√™n hai t·ªïng b·∫±ng nhau n√™n:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\epsilon_k} = \\sum_{j=1}^d \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_j} = \\left[ \\nabla^2 f(x) \\epsilon \\right]_k\n",
    "   $$\n",
    "\n",
    "   Gradient l√†:\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\epsilon} \\left( \\frac{1}{2} \\epsilon^T \\nabla^2 f(x) \\epsilon \\right) = \\nabla^2 f(x) \\epsilon\n",
    "   $$\n",
    "\n",
    "5. **K·∫øt h·ª£p v·ªõi nhau v√† b·ªè qua $ O(\\|\\epsilon\\|^3) $**  \n",
    "   $$\n",
    "   \\nabla_{\\epsilon} f(x + \\epsilon) = 0 + \\nabla f(x) + \\nabla^2 f(x) \\epsilon\n",
    "   $$\n",
    "\n",
    "Nh∆∞ ƒë√£ ƒë·ªãnh nghƒ©a tr∆∞·ªõc ƒë√≥ $\\mathbf{H} \\stackrel{\\text{def}}{=} \\nabla^2 f(\\mathbf{x})$ n√™n $\\nabla_{\\epsilon} f(x + \\epsilon) = \\nabla f(x) + \\mathbf{H} \\epsilon$ v√† ta c·∫ßn t√¨m gi√° tr·ªã $\\nabla_{\\epsilon} f(x + \\epsilon)=0$: $$\\nabla f(\\mathbf{x}) + H \\boldsymbol{\\epsilon} = 0$$v√† do ƒë√≥ $$\\boldsymbol{\\epsilon} = -H^{-1} \\nabla f(\\mathbf{x})$$ \n",
    "V√≠ d·ª• v·ªõi h√†m hyperbolic cosine l·ªìi $f(x)=\\cosh(cx)$ v·ªõi m·ªôt h·∫±ng s·ªë $c$, c·ª±c ti·ªÉu to√†n c·ª•c t·∫°i $x=0$ ƒë∆∞·ª£c ƒë·∫°t sau v√†i l·∫ßn l·∫∑p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25253aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.5)\n",
    "\n",
    "def f(x):  # objective function\n",
    "    return torch.cosh(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return c * torch.sinh(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return c ** 2 * torch.cosh(c * x)\n",
    "\n",
    "def newton(x,step,eta=1):\n",
    "    results = [x]\n",
    "    for i in range(step):\n",
    "        x -= eta * f_grad(x) / f_hess(x)\n",
    "        results.append(float(x))\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "show_trace(newton(10.0,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9babe",
   "metadata": {},
   "source": [
    "X√©t h√†m $f(x)=x \\cos(cx)$ v·ªõi m·ªôt h·∫±ng s·ªë $c$. Trong ph∆∞∆°ng ph√°p Newton, ta chia cho Hessian. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu ƒë·∫°o h√†m b·∫≠c hai √¢m, ta c√≥ th·ªÉ ƒëi theo h∆∞·ªõng l√†m tƒÉng gi√° tr·ªã c·ªßa $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6d17d-6adb-4733-8fba-3da795c64a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return -2 * c * torch.sin(c * x) - c ** 2 * x * torch.cos(c * x)\n",
    "\n",
    "show_trace(newton(10,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bf0ca",
   "metadata": {},
   "source": [
    "H√†m s·ªë kh√¥ng di chuy·ªÉn v·ªÅ c·ª±c ti·ªÉu do ƒë·∫°o h√†m b·∫≠c 2 √¢m. M·ªôt ph∆∞∆°ng ph√°p l√† l·∫•y Hessian b·∫±ng c√°ch l·∫•y gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa n√≥. M·ªôt chi·∫øn l∆∞·ª£c kh√°c l√† ƒë∆∞a l·∫°i t·ªëc ƒë·ªô h·ªçc. C√≥ th√¥ng tin b·∫≠c hai cho ph√©p th·∫≠n tr·ªçng b·∫•t c·ª© khi n√†o ƒë·ªô cong l·ªõn v√† th·ª±c hi·ªán c√°c b∆∞·ªõc d√†i h∆°n b·∫•t c·ª© khi n√†o h√†m m·ª•c ti√™u ph·∫≥ng h∆°n. V·ªõi t·ªëc ƒë·ªô h·ªçc nh·ªè h∆°n m·ªôt ch√∫t, ch·∫≥ng h·∫°n $\\eta=0.5$, thu·∫≠t to√°n ho·∫°t ƒë·ªông hi·ªáu qu·∫£ h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd52349",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(newton(10,10,0.5), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b2802-a726-4387-9290-a3539b963903",
   "metadata": {},
   "source": [
    "### V√≠ d·ª• minh h·ªça\n",
    "L·∫•y th√™m m·ªôt v√≠ d·ª• minh h·ªça v·ªõi $f(x) = x \\log(x)$. V·ªõi ph∆∞∆°ng ph√°p gradient descent th√¥ng th∆∞·ªùng v√† ph∆∞∆°ng ph√°p Newton ta c√≥ th·ªÉ th·∫•y ph∆∞∆°ng ph√°p Newton ti·∫øn v·ªÅ ƒëi·ªÉm t·ªëi ∆∞u nhanh h∆°n v·ªõi c√πng t·ªëc ƒë·ªô h·ªçc $\\eta=0.2$ v√† c√πng s·ªë b∆∞·ªõc l√† 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79316d51-ac51-4378-b725-98eef0c89840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.log(x)\n",
    "\n",
    "def f_grad(x):\n",
    "    x=torch.tensor([x])\n",
    "    return (torch.log(x) + 1).item()\n",
    "\n",
    "def f_hess(x):\n",
    "    return 1 / x\n",
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = np.arange(0.01, n, 0.01)\n",
    "    d2l.set_figsize()\n",
    "    d2l.plot([f_line, results], [[f(x) for x in f_line], [f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "show_trace(newton(5,10,0.2), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d85337-d0f6-4f4b-8510-a759b3478b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(eta, f_grad, start_x, step):\n",
    "    x = start_x\n",
    "    results = [x]\n",
    "    for i in range(step):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(x)\n",
    "    print(f'epoch 11, x: {x:.6f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.2, f_grad,5,10)\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31d0ef",
   "metadata": {},
   "source": [
    "### Ph√¢n T√≠ch H·ªôi T·ª•\n",
    "Ta ch·ªâ ph√¢n t√≠ch t·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa ph∆∞∆°ng ph√°p Newton cho m·ªôt h√†m m·ª•c ti√™u l·ªìi v√† kh·∫£ vi ba l·∫ßn, trong ƒë√≥ ƒë·∫°o h√†m b·∫≠c hai kh√°c kh√¥ng, t·ª©c l√† $f^{\\prime\\prime}>0$.\n",
    "G·ªçi $x^{(k)}$ l√† gi√° tr·ªã c·ªßa $x$ t·∫°i l·∫ßn l·∫∑p th·ª© $k$ v√† ƒë·∫∑t $e^{(k)} \\stackrel{\\text{def}}{=} x^{(k)}-x^*$ l√† kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm t·ªëi ∆∞u t·∫°i l·∫ßn l·∫∑p th·ª© $k$. B·∫±ng khai tri·ªÉn Taylor, ta c√≥ ƒëi·ªÅu ki·ªán $f^{\\prime}\\left(x^{(*)}\\right)=0$ c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt l√†:\n",
    "$$0=f^{\\prime}\\left(x^{(k)}-e^{(k)}\\right)=f^{\\prime}\\left(x^{(k)}\\right)-e^{(k)} f^{\\prime\\prime}\\left(x^{(k)}\\right)+\\frac{1}{2}\\left(e^{(k)}\\right)^2 f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right),\\tag{9}$$\n",
    "ƒëi·ªÅu n√†y ƒë√∫ng v·ªõi m·ªôt $\\xi^{(k)} \\in \\left[x^{(k)}-e^{(k)}, x^{(k)}\\right]$. Chia khai tri·ªÉn tr√™n cho $f^{\\prime\\prime}\\left(x^{(k)}\\right)$, ta ƒë∆∞·ª£c:\n",
    "$$e^{(k)}-\\frac{f^{\\prime}\\left(x^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)}=\\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)} .\\tag{10}$$\n",
    "Nh·ªõ r·∫±ng ta c√≥ c·∫≠p nh·∫≠t $x^{(k+1)}=x^{(k)}-f^{\\prime}\\left(x^{(k)}\\right) / f^{\\prime\\prime}\\left(x^{(k)}\\right)$ (ph∆∞∆°ng ph√°p Newton). Thay v√†o ph∆∞∆°ng tr√¨nh c·∫≠p nh·∫≠t n√†y v√† l·∫•y gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa c·∫£ hai v·∫ø, ta c√≥:\n",
    "$$\n",
    "e^{(k+1)}=x^{(k+1)} - x^*\n",
    "         =x^{(k)}-f^{\\prime}\\left(x^{(k)}\\right) / f^{\\prime\\prime}\\left(x^{(k)}\\right)- x^* = e^{(k)}-f^{\\prime}\\left(x^{(k)}\\right) / f^{\\prime\\prime}\\left(x^{(k)}\\right)= \\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)}\n",
    "$$\n",
    "L·∫•y tr·ªã tuy·ªát ƒë·ªëi 2 v·∫ø:\n",
    "$$\\left|e^{(k+1)}\\right|=\\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{\\left|f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)\\right|}{f^{\\prime\\prime}\\left(x^{(k)}\\right)} .\\tag{11}$$\n",
    "Do ƒë√≥, b·∫•t c·ª© khi n√†o ta ·ªü trong m·ªôt v√πng c√≥ $\\left|f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)\\right| /\\left(2 f^{\\prime\\prime}\\left(x^{(k)}\\right)\\right) \\leq c$, ta c√≥ sai s·ªë gi·∫£m b·∫≠c hai:\n",
    "$$\\left|e^{(k+1)}\\right| \\leq c\\left(e^{(k)}\\right)^2 .\\tag{12}$$\n",
    "L∆∞u √Ω r·∫±ng c√°c nh√† nghi√™n c·ª©u t·ªëi ∆∞u h√≥a g·ªçi ƒë√¢y l√† h·ªôi t·ª• tuy·∫øn t√≠nh, trong khi m·ªôt ƒëi·ªÅu ki·ªán nh∆∞ $\\left|e^{(k+1)}\\right| \\leq \\alpha\\left|e^{(k)}\\right|$ ƒë∆∞·ª£c g·ªçi l√† t·ªëc ƒë·ªô h·ªôi t·ª• h·∫±ng s·ªë. Ph√¢n t√≠ch n√†y ƒëi k√®m v·ªõi m·ªôt s·ªë l∆∞u √Ω. Th·ª© nh·∫•t, kh√¥ng th·ª±c s·ª± c√≥ ƒë·∫£m b·∫£o khi n√†o s·∫Ω ƒë·∫°t ƒë∆∞·ª£c v√πng h·ªôi t·ª• nhanh. Thay v√†o ƒë√≥ ch·ªâ bi·∫øt r·∫±ng m·ªôt khi ƒë·∫°t ƒë∆∞·ª£c, h·ªôi t·ª• s·∫Ω r·∫•t nhanh. Th·ª© hai, ph√¢n t√≠ch n√†y y√™u c·∫ßu $f$ c√≥ t√≠nh ch·∫•t t·ªët ƒë·∫øn c√°c ƒë·∫°o h√†m b·∫≠c cao. N√≥ ph·ª• thu·ªôc v√†o vi·ªác ƒë·∫£m b·∫£o r·∫±ng $f$ kh√¥ng c√≥ b·∫•t k·ª≥ ƒë·∫∑c t√≠nh \"b·∫•t ng·ªù\" n√†o v·ªÅ c√°ch n√≥ c√≥ th·ªÉ thay ƒë·ªïi gi√° tr·ªã."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e920ef",
   "metadata": {},
   "source": [
    "### Ti·ªÅn ƒêi·ªÅu Ki·ªán H√≥a\n",
    "Kh√¥ng ng·∫°c nhi√™n khi vi·ªác t√≠nh to√°n v√† l∆∞u tr·ªØ to√†n b·ªô Hessian r·∫•t t·ªën k√©m. M·ªôt c√°ch ƒë·ªÉ c·∫£i thi·ªán l√† ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a. N√≥ tr√°nh t√≠nh to√°n to√†n b·ªô Hessian m√† ch·ªâ t√≠nh c√°c ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn c√°c thu·∫≠t to√°n c·∫≠p nh·∫≠t d·∫°ng:\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\operatorname{diag}(\\mathbf{H})^{-1} \\nabla f(\\mathbf{x}) .$$\n",
    "M·∫∑c d√π ƒëi·ªÅu n√†y kh√¥ng t·ªët b·∫±ng ph∆∞∆°ng ph√°p Newton ƒë·∫ßy ƒë·ªß, n√≥ v·∫´n t·ªët h∆°n nhi·ªÅu so v·ªõi vi·ªác kh√¥ng s·ª≠ d·ª•ng. ƒê·ªÉ th·∫•y t·∫°i sao ƒë√¢y l√† √Ω t∆∞·ªüng t·ªët, h√£y xem x√©t m·ªôt t√¨nh hu·ªëng m√† m·ªôt bi·∫øn bi·ªÉu th·ªã chi·ªÅu cao t√≠nh b·∫±ng milimet v√† m·ªôt bi·∫øn kh√°c bi·ªÉu th·ªã chi·ªÅu cao t√≠nh b·∫±ng kil√¥m√©t. Gi·∫£ s·ª≠ r·∫±ng v·ªõi c·∫£ hai, t·ª∑ l·ªá t·ª± nhi√™n l√† m√©t, ta c√≥ s·ª± kh√¥ng kh·ªõp l·ªõn trong tham s·ªë h√≥a. May m·∫Øn thay, vi·ªác s·ª≠ d·ª•ng ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a s·∫Ω lo·∫°i b·ªè ƒëi·ªÅu n√†y. Hi·ªáu qu·∫£, ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a v·ªõi gradient descent t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác ch·ªçn m·ªôt t·ªëc ƒë·ªô h·ªçc kh√°c nhau cho m·ªói bi·∫øn (t·ªça ƒë·ªô c·ªßa vector $\\mathbf{x}$). Nh∆∞ ta s·∫Ω th·∫•y sau, ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a th√∫c ƒë·∫©y m·ªôt s·ªë c·∫£i ti·∫øn trong c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a gradient descent ng·∫´u nhi√™n.\n",
    "### Gradient descent v·ªõi T√¨m Ki·∫øm Tuy·∫øn\n",
    "M·ªôt trong nh·ªØng v·∫•n ƒë·ªÅ ch√≠nh trong h·∫° gradient l√† ta c√≥ th·ªÉ v∆∞·ª£t qu√° m·ª•c ti√™u ho·∫∑c ti·∫øn b·ªô kh√¥ng ƒë·ªß. M·ªôt c√°ch s·ª≠a ƒë∆°n gi·∫£n l√† s·ª≠ d·ª•ng t√¨m ki·∫øm tuy·∫øn k·∫øt h·ª£p v·ªõi gradient descent. T·ª©c l√†, ta s·ª≠ d·ª•ng h∆∞·ªõng ƒë∆∞·ª£c cho b·ªüi $\\nabla f(\\mathbf{x})$ v√† sau ƒë√≥ th·ª±c hi·ªán t√¨m ki·∫øm nh·ªã ph√¢n ƒë·ªÉ x√°c ƒë·ªãnh t·ªëc ƒë·ªô h·ªçc $\\eta$ n√†o t·ªëi ∆∞u h√≥a $f(\\mathbf{x}-\\eta \\nabla f(\\mathbf{x}))$.\n",
    "Thu·∫≠t to√°n n√†y h·ªôi t·ª• nhanh ch√≥ng. Tuy nhi√™n, ƒë·ªëi v·ªõi m·ª•c ƒë√≠ch h·ªçc s√¢u, ƒëi·ªÅu n√†y kh√¥ng th·ª±c s·ª± kh·∫£ thi, v√¨ m·ªói b∆∞·ªõc c·ªßa t√¨m ki·∫øm tuy·∫øn s·∫Ω y√™u c·∫ßu ƒë√°nh gi√° h√†m m·ª•c ti√™u tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu. ƒêi·ªÅu n√†y qu√° t·ªën k√©m ƒë·ªÉ th·ª±c hi·ªán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022be34-d4b4-4a4f-9aa2-7564b660f378",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "### 1. Th·ª≠ nghi·ªám v·ªõi c√°c t·ªëc ƒë·ªô h·ªçc v√† h√†m m·ª•c ti√™u kh√°c nhau ƒë·ªÉ gi·∫£m d·∫ßn ƒë·ªô d·ªëc.\n",
    "ƒê√£ th·ª±c hi·ªán trong qu√° tr√¨nh t√¨m hi·ªÉu\n",
    "### 2. Tri·ªÉn khai t√¨m ki·∫øm tuy·∫øn ƒë·ªÉ gi·∫£m thi·ªÉu m·ªôt h√†m l·ªìi trong kho·∫£ng [ùëé, ùëè].\n",
    "#### 1. B·∫°n c√≥ c·∫ßn ƒë·∫°o h√†m cho t√¨m ki·∫øm nh·ªã ph√¢n kh√¥ng, t·ª©c l√† ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·ªçn $[ùëé, (ùëé +\n",
    "ùëè)/2]$ hay $[(ùëé + ùëè)/2, ùëè]$.\n",
    "Kh√¥ng th·ª±c s·ª± c·∫ßn ƒë·∫°o h√†m cho t√¨m ki·∫øm nh·ªã ph√¢n, nh∆∞ng t√¨m ki·∫øm nh·ªã ph√¢n ƒë∆°n gi·∫£n tr√™n x s·∫Ω kh√¥ng ho·∫°t ƒë·ªông tr·ª±c ti·∫øp n·∫øu kh√¥ng c√≥ th√™m th√¥ng tin.\n",
    "- N·∫øu h√†m ƒë∆°n ƒëi·ªáu tr√™n kho·∫£ng $[a,b]$ th√¨ th·ª±c hi·ªán t√¨m ki·∫øm nh·ªã ph√¢n cho $f(x)=0$ s·∫Ω c√≥ hi·ªáu qu·∫£ v√† kh√¥ng c·∫ßn s·ª≠ d·ª•ng t·ªõi ƒë·∫°o h√†m\n",
    "- N·∫øu h√†m kh√¥ng ƒë∆°n ƒëi·ªáu trong kho·∫£ng $[a,b]$, ƒë·ªÉ t·ªëi thi·ªÉu h√≥a $f(x)$. N·∫øu ch·ªâ ƒë√°nh gi√° $f((a+b)/2)$ th√¨ s·∫Ω kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒëi·ªÉm c·ª±c ti·ªÉu n·∫±m ·ªü trong kho·∫£ng $[(a+b)/2,b]$ hay $[a,(a+b)/2]$. Gi·∫£ s·ª≠ $f((a+b)/2)<f(a)$ ta s·∫Ω kh√¥ng bi·∫øt ƒë∆∞·ª£c ƒëi·ªÉm c·ª±c ti·ªÉu n·∫±m trong kho·∫£ng $[(a+b)/2,b]$ hay $[a,(a+b)/2]$ \n",
    "##### Ph∆∞∆°ng ph√°p kh√¥ng s·ª≠ d·ª•ng ƒë·∫°o h√†m ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·ªçn kho·∫£ng trong tr∆∞·ªùng h·ª£p h√†m l·ªìi.\n",
    "-Ch·ªçn 2 ƒëi·ªÉm $x_1$, $x_2$:\n",
    "  - N·∫øu $f(x_1)<f(x_2)$ ƒëi·ªÉm c·ª±c ti·ªÉu s·∫Ω n·∫±m trong kho·∫£ng $[a,x_2]$\n",
    "  - N·∫øu $f(x_1)>f(x_2)$ ƒëi·ªÉm c·ª±c ti·ªÉu s·∫Ω n·∫±m trong kho·∫£ng $[x_1,b]$\n",
    "##### Ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ƒë·∫°o h√†m ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·ªçn kho·∫£ng trong tr∆∞·ªùng h·ª£p h√†m l·ªìi.\n",
    "- ƒê√°nh gi√° ƒë·∫°o h√†m $f^{\\prime}((a+b)/2)$\n",
    "  - N·∫øu $f^{\\prime}((a+b)/2)<0$ h√†m s·ªë ƒëang gi·∫£m t·∫°i ƒëi·ªÉm $(a+b)/2$ n√™n c·ª±c ti·ªÉu n·∫±m trong kho·∫£ng $[(a+b)/2, b]$\n",
    "  - N·∫øu $f^{\\prime}((a+b)/2)>0$ h√†m s·ªë ƒëang tƒÉng t·∫°i ƒëi·ªÉm $(a+b)/2$ n√™n c·ª±c ti·ªÉu n·∫±m trong kho·∫£ng $[a,(a+b)/2]$\n",
    "#### 2. T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa thu·∫≠t to√°n nhanh nh∆∞ th·∫ø n√†o?\n",
    "- T·ªëc ƒë·ªô h·ªôi t·ª• l√† **tuy·∫øn t√≠nh**. ƒê·ªô r·ªông c·ªßa kho·∫£ng ƒë∆∞·ª£c gi·∫£m theo m·ªôt h·ªá s·ªë kh√¥ng ƒë·ªïi sau m·ªói b∆∞·ªõc l·∫∑p, h·ªá s·ªë n√†y l√† $ \\frac{1}{\\varphi} \\approx 0{,}618$ (v·ªõi $\\varphi$, x·∫•p x·ªâ $1{,}618 $.ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†, ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c **m·ªôt ch·ªØ s·ªë th·∫≠p ph√¢n ch√≠nh x√°c h∆°n** (t·ª©c l√† gi·∫£m ƒë·ªô r·ªông kho·∫£ng t√¨m ki·∫øm ƒëi 10 l·∫ßn), ta c·∫ßn kho·∫£ng:$\\frac{\\log(10)}{\\log(\\varphi)} \\approx 4{,}78$ b∆∞·ªõc l·∫∑p.\n",
    "- N·∫øu ƒë·∫°o h√†m c·ªßa h√†m $f(x)$ c√≥ s·∫µn, ta c√≥ th·ªÉ th·ª±c hi·ªán **t√¨m ki·∫øm nh·ªã ph√¢n tr√™n $f'(x)$**. Ph∆∞∆°ng ph√°p n√†y c≈©ng c√≥ **t·ªëc ƒë·ªô h·ªôi t·ª• tuy·∫øn t√≠nh**, nh∆∞ng h·ªá s·ªë gi·∫£m ƒë·ªô r·ªông kho·∫£ng t·∫°i m·ªói b∆∞·ªõc l√† $0{,}5$ (t·ª©c l√† chia ƒë√¥i kho·∫£ng). ƒêi·ªÅu n√†y nhanh h∆°n so v·ªõi ph∆∞∆°ng ph√°p **T√¨m ki·∫øm theo T·ª∑ l·ªá V√†ng** (Golden Section Search).ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c **m·ªôt ch·ªØ s·ªë th·∫≠p ph√¢n ch√≠nh x√°c h∆°n** (t·ª©c l√† gi·∫£m ƒë·ªô r·ªông kho·∫£ng ƒëi 10 l·∫ßn), c·∫ßn kho·∫£ng:$\\frac{\\log(10)}{\\log(2)} \\approx 3{,}32$ b∆∞·ªõc l·∫∑p.\n",
    "#### 3. Tri·ªÉn khai thu·∫≠t to√°n v√† √°p d·ª•ng n√≥ ƒë·ªÉ t·ªëi thi·ªÉu log(exp(ùë•) + exp(‚àí2ùë• ‚àí 3))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5d854-c7fc-4bb1-9863-3c53935973c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Function to minimize: f(x) = log(exp(x) + exp(-2x - 3))\"\"\"\n",
    "    return np.log(np.exp(x) + np.exp(-2 * x - 3))\n",
    "\n",
    "def f_prime(x):\n",
    "    \"\"\"Derivative of f(x)\"\"\"\n",
    "    return (np.exp(x) - 2 * np.exp(-2 * x - 3)) / (np.exp(x) + np.exp(-2 * x - 3))\n",
    "\n",
    "def binary_search_derivative(f_prime, a, b, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Find the root of f'(x) = 0 in [a, b] using binary search, assuming f'(a) and f'(b) have opposite signs.\n",
    "    Returns the approximate minimum point x, number of iterations, and list of midpoints.\n",
    "    \"\"\"\n",
    "    if f_prime(a) * f_prime(b) >= 0:\n",
    "        raise ValueError(\"f'(a) and f'(b) must have opposite signs\")\n",
    "    \n",
    "    iterations = 0\n",
    "    midpoints = []\n",
    "    \n",
    "    while (b - a) > tol:\n",
    "        m = (a + b) / 2\n",
    "        fm = f_prime(m)\n",
    "        midpoints.append(m)\n",
    "        iterations += 1\n",
    "        \n",
    "        if abs(fm) < tol:  # If derivative is close to zero, stop\n",
    "            break\n",
    "        elif fm > 0:\n",
    "            b = m  # Root is in [a, m]\n",
    "        else:\n",
    "            a = m  # Root is in [m, b]\n",
    "    \n",
    "    x_min = (a + b) / 2\n",
    "    return x_min, iterations, midpoints\n",
    "\n",
    "# Run Binary Search\n",
    "a, b = -1, 0\n",
    "tol = 1e-5\n",
    "x_min, iterations, midpoints = binary_search_derivative(f_prime, a, b, tol)\n",
    "\n",
    "# Print results\n",
    "print(f\"Approximate minimum at x = {x_min:.6f}\")\n",
    "print(f\"Function value f(x) = {f(x_min):.6f}\")\n",
    "print(f\"Number of iterations: {iterations}\")\n",
    "\n",
    "# Plot the function and search progress\n",
    "x = np.linspace(-2, 1, 1000)\n",
    "y = f(x)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(x, y, '-', label='f(x) = log(exp(x) + exp(-2x - 3))')\n",
    "\n",
    "# Plot the midpoints from the first few iterations\n",
    "plt.plot(-1, f(-1), 'o', color='black', label='a')\n",
    "plt.plot(0, f(0), 'o', color='black', label='b')\n",
    "for i, m in enumerate(midpoints[:5]):  # Show first 5 iterations\n",
    "    plt.plot(m, f(m), 'o', label=f'Iteration {i+1}' if i < 5 else '')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Binary Search on Derivative')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5088a08-ba48-4c94-af3d-5134c2897506",
   "metadata": {},
   "source": [
    "### 3. Thi·∫øt k·∫ø m·ªôt h√†m m·ª•c ti√™u x√°c ƒë·ªãnh tr√™n $\\mathbb{R}^2$ m√† gradient descent r·∫•t ch·∫≠m.\n",
    "#### H√†m $f(x_1, x_2)= x_1^2 + Sx_2^2$\n",
    "T·∫°i sao h√†m $f(x_1, x_2)= x_1^2 + Sx_2^2$ l·∫°i h·ªôi t·ª• ch·∫≠m:\n",
    "- C·ª±c ti·ªÉu to√†n c·ª•c c·ªßa h√†m n√†y r√µ r√†ng n·∫±m t·∫°i $(x_1, x_2) = (0, 0)$, v·ªõi $f(0, 0) = 0$.\n",
    "- $\\nabla f(x_1, x_2) = [2x_1,\\ 2Sx_2]$\n",
    "- $H = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2S\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### S·ªë ƒëi·ªÅu ki·ªán (Condition Number):\n",
    "\n",
    "C√°c gi√° tr·ªã ri√™ng c·ªßa ma tr·∫≠n Hessian l√†:\n",
    "\n",
    "- $\\lambda_1 = 2$\n",
    "- $\\lambda_2 = 2S$\n",
    "\n",
    "V·∫≠y s·ªë ƒëi·ªÅu ki·ªán c·ªßa Hessian l√†:\n",
    "$$\n",
    "\\kappa(H) = \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} = \\frac{2S}{2} = S\n",
    "$$\n",
    "N·∫øu $S$ l·ªõn (v√≠ d·ª• $S = 100$), s·ªë ƒëi·ªÅu ki·ªán l√† 100. Khi $S$ l·ªõn, c√°c ellipse s·∫Ω b·ªã k√©o d√£n m·∫°nh. N·∫øu $S > 1$, ch√∫ng b·ªã k√©o d√£n theo tr·ª•c $x_1$ (nghƒ©a l√† \"thung l≈©ng\" s·∫Ω h·∫πp theo ph∆∞∆°ng $x_2$ v√† d√†i theo ph∆∞∆°ng $x_1$). H√†m tƒÉng r·∫•t nhanh theo h∆∞·ªõng $x_2$ so v·ªõi h∆∞·ªõng $x_1$.\n",
    "\n",
    "### H√†nh vi c·ªßa Gradient Descent:\n",
    "\n",
    "Gradient $\\nabla f = [2x_1,\\ 2Sx_2]$ s·∫Ω c√≥ th√†nh ph·∫ßn l·ªõn h∆°n nhi·ªÅu ·ªü h∆∞·ªõng $x_2$ (do h·ªá s·ªë $2S$) khi $x_2 \\ne 0$.\n",
    "\n",
    "Khi Gradient Descent c·∫≠p nh·∫≠t:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\alpha \\nabla f\n",
    "$$\n",
    "\n",
    "th√¨ thay ƒë·ªïi c·ªßa $x_2$ s·∫Ω l·ªõn h∆°n ƒë√°ng k·ªÉ so v·ªõi $x_1$.\n",
    "\n",
    "ƒêi·ªÅu n√†y khi·∫øn thu·∫≠t to√°n dao ƒë·ªông (zig-zag) qua l·∫°i nhanh trong ph·∫ßn h·∫πp c·ªßa thung l≈©ng (h∆∞·ªõng $x_2$), trong khi ti·∫øn tri·ªÉn d·ªçc theo ph·∫ßn b·∫±ng ph·∫≥ng (h∆∞·ªõng $x_1$) r·∫•t ch·∫≠m.\n",
    "\n",
    "ƒê·ªÉ tr√°nh sai l·ªách do th√†nh ph·∫ßn gradient $x_2$ qu√° l·ªõn, t·ªëc ƒë·ªô h·ªçc $\\alpha$ ph·∫£i ƒë∆∞·ª£c gi·ªØ r·∫•t nh·ªè, ƒëi·ªÅu n√†y c√†ng l√†m ch·∫≠m b∆∞·ªõc ƒëi theo h∆∞·ªõng $x_1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993998-f94f-4aa0-baf3-6cd77062543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S = 100  # Scaling factor\n",
    "\n",
    "def f(x1, x2):\n",
    "  return x1**2 + S * x2**2\n",
    "\n",
    "def grad_f(x1, x2):\n",
    "  return np.array([2 * x1, 2 * S * x2])\n",
    "\n",
    "# --- Gradient Descent Implementation ---\n",
    "def gradient_descent(grad_f, start_point, learning_rate, iterations):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()] # Store the path\n",
    "    for i in range(iterations):\n",
    "        grad = grad_f(x[0], x[1])\n",
    "        x = x - learning_rate * grad\n",
    "        path.append(x.copy())\n",
    "        # Optional: Check for divergence or convergence\n",
    "        if np.linalg.norm(grad) < 1e-6:\n",
    "            print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "        if np.any(np.abs(x) > 1e5): # Crude divergence check\n",
    "            print(f\"Diverged at iteration {i+1}\")\n",
    "            break\n",
    "    return np.array(path)\n",
    "\n",
    "# --- Parameters ---\n",
    "start_x = np.array([10.0, 1.0])   \n",
    "learning_rate_eta = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "# --- Run Gradient Descent ---\n",
    "path = gradient_descent(\n",
    "    grad_f,\n",
    "    start_x,\n",
    "    learning_rate_eta,\n",
    "    num_iterations\n",
    ")\n",
    "def draw_function(path):\n",
    "    \n",
    "    x1_vals = np.linspace(min(path[:, 0].min(), -10) - 1, max(path[:, 0].max(), 10) + 1, 200)\n",
    "    x2_vals = np.linspace(min(path[:, 1].min(), -1.5) - 0.2, max(path[:, 1].max(), 1.5) + 0.2, 200)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "    Z = f(X1, X2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    levels = np.logspace(0, np.log10(Z.max() if Z.max() > 0 else 1), 30) if Z.max() > 0 else 10\n",
    "    contour = plt.contour(X1, X2, Z, levels=levels, cmap='viridis')\n",
    "    plt.colorbar(contour, label='f(x‚ÇÅ, x‚ÇÇ)')\n",
    "    \n",
    "    # Plot the path of gradient descent\n",
    "    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, linewidth=1, label=f'GD Path (Œ∑={learning_rate_eta})')\n",
    "    \n",
    "    plt.scatter(0, 0, color='black', marker='*', s=150, label='Minimum (0,0)', zorder=5)\n",
    "    plt.scatter(start_x[0], start_x[1], color='blue', s=100, label='Start Point', zorder=4)\n",
    "    \n",
    "    plt.title(f'Gradient Descent on f(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬≤ + {S}x‚ÇÇ¬≤')\n",
    "    plt.xlabel('x‚ÇÅ')\n",
    "    plt.ylabel('x‚ÇÇ')\n",
    "    plt.legend()\n",
    "    plt.axis('equal') \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "draw_function(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8604d6-d4b5-4724-aca1-197ec6e127f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---\n",
    "start_x = np.array([10.0, 1.0])   \n",
    "learning_rate_eta = 0.005\n",
    "num_iterations = 100\n",
    "\n",
    "# --- Run Gradient Descent ---\n",
    "path = gradient_descent(\n",
    "    grad_f,\n",
    "    start_x,\n",
    "    learning_rate_eta,\n",
    "    num_iterations\n",
    ")\n",
    "draw_function(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefcc5d-c02f-4f5d-8dd2-821bdc7f926b",
   "metadata": {},
   "source": [
    "### 4. Tri·ªÉn khai phi√™n b·∫£n c·ªßa ph∆∞∆°ng ph√°p Newton b·∫±ng c√°ch s·ª≠ d·ª•ng ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a.\n",
    "Ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o (Diagonal Preconditioner) $M$:\n",
    "\n",
    "Ta l·∫•y c√°c ph·∫ßn t·ª≠ tr√™n ƒë∆∞·ªùng ch√©o ch√≠nh c·ªßa Hessian: $\\text{diag}(H) = [2,\\ 2S]$.\n",
    "\n",
    "D√πng gi√° tr·ªã tuy·ªát ƒë·ªëi (m·∫∑c d√π trong tr∆∞·ªùng h·ª£p n√†y v·ªõi $S > 0$, ch√∫ng ƒë√£ l√† s·ªë d∆∞∆°ng): \n",
    "\n",
    "$$\n",
    "M_{\\text{diag}} = [|2|,\\ |2S|] = [2,\\ 2S]\n",
    "$$\n",
    "\n",
    "Ma tr·∫≠n ti·ªÅn ƒëi·ªÅu ki·ªán $M$ (n·∫øu vi·∫øt ƒë·∫ßy ƒë·ªß, d√π ta ch·ªâ c·∫ßn ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o ƒë·ªÉ t√≠nh $M^{-1} \\nabla f$):\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2S\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Ngh·ªãch ƒë·∫£o c·ªßa ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o $M^{-1}$ (√°p d·ª•ng t·ª´ng ph·∫ßn t·ª≠):\n",
    "\n",
    "N·∫øu $M$ l√† ma tr·∫≠n ch√©o v·ªõi ph·∫ßn t·ª≠ $m_{ii}$, th√¨ $M^{-1}$ c≈©ng l√† ch√©o v·ªõi ph·∫ßn t·ª≠ $1/m_{ii}$.\n",
    "\n",
    "V√¨ v·∫≠y, ti·ªÅn ƒëi·ªÅu ki·ªán c√≥ t√°c d·ª•ng **chia t·ª´ng th√†nh ph·∫ßn c·ªßa gradient cho ph·∫ßn t·ª≠ t∆∞∆°ng ·ª©ng tr√™n ƒë∆∞·ªùng ch√©o c·ªßa Hessian**.\n",
    "\n",
    "---\n",
    "\n",
    "### Quy t·∫Øc c·∫≠p nh·∫≠t:\n",
    "\n",
    "Gradient descent c√≥ ti·ªÅn ƒëi·ªÅu ki·ªán chu·∫©n l√†:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\eta \\cdot M^{-1} \\nabla f\n",
    "$$\n",
    "\n",
    "V·ªõi $M$ l√† ma tr·∫≠n ch√©o, ta c√≥:\n",
    "\n",
    "- $x_1^{\\text{new}} = x_1^{\\text{old}} - \\eta \\cdot \\left( \\frac{\\partial f / \\partial x_1}{|H_{11}|} \\right)$  \n",
    "- $x_2^{\\text{new}} = x_2^{\\text{old}} - \\eta \\cdot \\left( \\frac{\\partial f / \\partial x_2}{|H_{22}|} \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### Thay c√°c gi√° tr·ªã c·ª• th·ªÉ v√†o:\n",
    "\n",
    "- $x_1^{\\text{new}} = x_1^{\\text{old}} - \\eta \\cdot \\left( \\frac{2x_1^{\\text{old}}}{|2|} \\right) = x_1^{\\text{old}} - \\eta \\cdot x_1^{\\text{old}} = x_1^{\\text{old}} (1 - \\eta)$\n",
    "\n",
    "- $x_2^{\\text{new}} = x_2^{\\text{old}} - \\eta \\cdot \\left( \\frac{2Sx_2^{\\text{old}}}{|2S|} \\right) = x_2^{\\text{old}} - \\eta \\cdot x_2^{\\text{old}} = x_2^{\\text{old}} (1 - \\eta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01d3c8-59dd-45f5-b550-a0d550707fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def diag_hessian_abs_f(x1, x2):\n",
    "\n",
    "  h11 = 2.0\n",
    "  h22 = 2.0 * S\n",
    "  epsilon = 1e-8\n",
    "  return np.array([abs(h11) + epsilon, abs(h22) + epsilon])\n",
    "\n",
    "def preconditioned_gradient_descent(\n",
    "    grad_f, diag_hess_abs_f, start_point, learning_rate_eta, iterations\n",
    "):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()] # Store the path\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = grad_f(x[0], x[1])\n",
    "        diag_H_abs = diag_hess_abs_f(x[0], x[1])\n",
    "\n",
    "        # Element-wise division for preconditioning\n",
    "        preconditioned_grad = grad / diag_H_abs\n",
    "\n",
    "        x = x - learning_rate_eta * preconditioned_grad\n",
    "        path.append(x.copy())\n",
    "\n",
    "        # Check for convergence or divergence\n",
    "        if np.linalg.norm(grad) < 1e-7: # Check original gradient for convergence\n",
    "            print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "        if np.any(np.abs(x) > 1e6): # Crude divergence check\n",
    "            print(f\"Diverged at iteration {i+1}\")\n",
    "            break\n",
    "    return np.array(path)\n",
    "\n",
    "# --- Parameters ---\n",
    "start_x = np.array([10.0, 1.0])   # Start pointt\n",
    "learning_rate_eta = 0.5 \n",
    "num_iterations = 50\n",
    "\n",
    "\n",
    "path_preconditioned = preconditioned_gradient_descent(\n",
    "    grad_f,\n",
    "    diag_hessian_abs_f,\n",
    "    start_x,\n",
    "    learning_rate_eta,\n",
    "    num_iterations\n",
    ")\n",
    "draw_function(path_preconditioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26e41e-5847-4251-a027-7b726067b493",
   "metadata": {},
   "source": [
    "V·ªõi t·ªëc ƒë·ªô h·ªçc $\\eta=0.5$ ta th·∫•y sau l·∫ßn l·∫∑p ƒë·∫ßu ti√™n gi√° tr·ªã c·ªßa $x_1, x_2$ ƒë√£ gi·∫£m 1 n·ª≠a do \n",
    "- $x_1^{\\text{new}} = x_1^{\\text{old}} (1 - \\eta)=x_1^{\\text{old}} (1 - 0.5)$ \n",
    "- $x_2^{\\text{new}} = x_2^{\\text{old}} (1 - \\eta)=x_2^{\\text{old}} (1 - 0.5)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826da86-828d-4a54-bfa7-ab2de1d89b54",
   "metadata": {},
   "source": [
    "### √Åp d·ª•ng thu·∫≠t to√°n tr√™n cho m·ªôt s·ªë h√†m m·ª•c ti√™u (l·ªìi ho·∫∑c kh√¥ng). ƒêi·ªÅu g√¨ x·∫£y ra n·∫øu b·∫°n xoay t·ªça ƒë·ªô 45 ƒë·ªô?\n",
    "#### 1. Elliptic Paraboloid:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 100x_2^2\n",
    "$$\n",
    "\n",
    "- $\\text{diag}_H^{\\text{abs}} = [2,\\ 200]$ (h·∫±ng s·ªë)\n",
    "- **K·ª≥ v·ªçng**: H·ªôi t·ª• nhanh v√† tr·ª±c ti·∫øp (1 b∆∞·ªõc n·∫øu $\\eta = 1$)\n",
    "#### 2. Circular Paraboloid:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + x_2^2\n",
    "$$\n",
    "\n",
    "- $\\text{diag}_H^{\\text{abs}} = [2,\\ 2]$ (h·∫±ng s·ªë)\n",
    "- **K·ª≥ v·ªçng**: H·ªôi t·ª• nhanh v√† tr·ª±c ti·∫øp (1 b∆∞·ªõc n·∫øu $\\eta = 1$).  \n",
    "#### 3. H√†m Rosenbrock (Kh√¥ng l·ªìi):\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2\n",
    "$$\n",
    "\n",
    "- $\\frac{\\partial^2 f}{\\partial x_1^2} = 2 - 400(x_2 - x_1^2) + 1200x_1^2$\n",
    "- $\\frac{\\partial^2 f}{\\partial x_2^2} = 200$\n",
    "\n",
    "- $\\text{diag}_H^{\\text{abs}} = [|2 - 400(x_2 - x_1^2) + 1200x_1^2|,\\ 200]$  *($H_{11}$ c√≥ th·ªÉ √¢m!)*\n",
    "\n",
    "- **K·ª≥ v·ªçng**: Th√†nh ph·∫ßn $H_{11}$ r·∫•t ph·ª©c t·∫°p v√† ph·ª• thu·ªôc v√†o $x_1, x_2$.  \n",
    "\n",
    "#### 4. H√†m kh√¥ng l·ªìi ƒë∆°n gi·∫£n (Hai ƒëi·ªÉm c·ª±c ti·ªÉu):\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^4 - 2x_1^2 + x_2^2\n",
    "$$\n",
    "\n",
    "- C·ª±c ti·ªÉu t·∫°i $(\\pm1,\\ 0)$, ƒëi·ªÉm y√™n t·∫°i $(0,\\ 0)$\n",
    "- $\\frac{\\partial^2 f}{\\partial x_1^2} = 12x_1^2 - 4$\n",
    "- $\\frac{\\partial^2 f}{\\partial x_2^2} = 2$\n",
    "- $\\text{diag}_H^{\\text{abs}} = [|12x_1^2 - 4|,\\ 2]$  *( $H_{11}$ c√≥ th·ªÉ √¢m ho·∫∑c b·∫±ng 0)*\n",
    "- **K·ª≥ v·ªçng**: Thu·∫≠t to√°n s·∫Ω h·ªôi t·ª• ƒë·∫øn m·ªôt c·ª±c ti·ªÉu t√πy theo ƒëi·ªÉm kh·ªüi ƒë·∫ßu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167b94e-1736-47e2-860b-d03890feca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Helper: Generic Optimization and Plotting ---\n",
    "def run_optimizer(\n",
    "    optimizer_func,\n",
    "    f_obj,\n",
    "    grad_f,\n",
    "    diag_hess_abs_f, # Specific to preconditioned GD\n",
    "    start_point,\n",
    "    learning_rate_eta,\n",
    "    iterations,\n",
    "    title_prefix=\"\",\n",
    "    S_param=None # For functions that use S\n",
    "):\n",
    "    if S_param is not None: # If the function needs S, curry it\n",
    "        obj_func_to_plot = lambda x1, x2: f_obj(x1, x2, S_param)\n",
    "    else:\n",
    "        obj_func_to_plot = f_obj\n",
    "\n",
    "    path = optimizer_func(\n",
    "        grad_f,\n",
    "        diag_hess_abs_f, # Pass this\n",
    "        start_point,\n",
    "        learning_rate_eta,\n",
    "        iterations,\n",
    "        S_param # Pass S if needed for grad/hessian\n",
    "    )\n",
    "\n",
    "    print(f\"{title_prefix} - Starting at: {path[0]}\")\n",
    "    print(f\"{title_prefix} - Ending at after {len(path)-1} iterations: {path[-1]}\")\n",
    "\n",
    "    # Visualization\n",
    "    # Adjust plot ranges based on path and known features of the function\n",
    "    x_min_plot = min(path[:, 0].min() - 1, -2.5 if \"Rosenbrock\" in title_prefix else -3)\n",
    "    x_max_plot = max(path[:, 0].max() + 1, 2.5 if \"Rosenbrock\" in title_prefix else 3)\n",
    "    y_min_plot = min(path[:, 1].min() - 1, -1.5 if \"Rosenbrock\" in title_prefix else -3)\n",
    "    y_max_plot = max(path[:, 1].max() + 1, 3.5 if \"Rosenbrock\" in title_prefix else 3)\n",
    "\n",
    "    x1_vals = np.linspace(x_min_plot, x_max_plot, 200)\n",
    "    x2_vals = np.linspace(y_min_plot, y_max_plot, 200)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "    Z = obj_func_to_plot(X1, X2)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    levels = np.logspace(np.log10(max(Z.min(), 0.01)), np.log10(Z.max() if Z.max() > 0 else 1), 30) if Z.min() < Z.max() else 15\n",
    "    try:\n",
    "        contour = plt.contour(X1, X2, Z, levels=levels, cmap='viridis')\n",
    "        plt.colorbar(contour, label='f(x‚ÇÅ, x‚ÇÇ)')\n",
    "    except Exception as e:\n",
    "        print(f\"Contour plot error for {title_prefix}: {e}\")\n",
    "        plt.contour(X1, X2, Z, cmap='viridis') # Fallback\n",
    "\n",
    "\n",
    "    plt.plot(path[:, 0], path[:, 1], 'g-o', markersize=3, linewidth=1, label=f'Preconditioned GD (Œ∑={learning_rate_eta})')\n",
    "    plt.scatter(path[0, 0], path[0, 1], color='blue', s=100, label='Start', zorder=4)\n",
    "    # Add known minima if applicable\n",
    "    if \"Ill-Conditioned\" in title_prefix or \"Well-Conditioned\" in title_prefix:\n",
    "        plt.scatter(0, 0, color='black', marker='*', s=150, label='Minimum (0,0)', zorder=5)\n",
    "    elif \"Rosenbrock\" in title_prefix:\n",
    "        plt.scatter(1, 1, color='black', marker='*', s=150, label='Minimum (1,1)', zorder=5)\n",
    "    elif \"Two Minima\" in title_prefix:\n",
    "        plt.scatter([1, -1], [0, 0], color='black', marker='*', s=150, label='Minima (¬±1,0)', zorder=5)\n",
    "\n",
    "\n",
    "    plt.title(title_prefix)\n",
    "    plt.xlabel('x‚ÇÅ')\n",
    "    plt.ylabel('x‚ÇÇ')\n",
    "    plt.legend()\n",
    "    plt.axis('equal' if \"Ill-Conditioned\" in title_prefix or \"Well-Conditioned\" in title_prefix else 'tight')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return path \n",
    "    \n",
    "\n",
    "# --- Diagonal Preconditioned Gradient Descent (from previous example) ---\n",
    "def preconditioned_gradient_descent(\n",
    "    grad_f, diag_hess_abs_f, start_point, learning_rate_eta, iterations, S_param=None\n",
    "):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()]\n",
    "    epsilon_hess = 1e-8 # For numerical stability if diag_H is zero\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Pass S if the gradient/Hessian functions need it\n",
    "        current_grad = grad_f(x[0], x[1], S_param) if S_param is not None else grad_f(x[0], x[1])\n",
    "        current_diag_H_abs = diag_hess_abs_f(x[0], x[1], S_param) if S_param is not None else diag_hess_abs_f(x[0], x[1])\n",
    "\n",
    "        # Ensure diagonal elements are not zero (add epsilon)\n",
    "        preconditioner = np.maximum(current_diag_H_abs, epsilon_hess)\n",
    "        preconditioned_grad = current_grad / preconditioner\n",
    "\n",
    "        x = x - learning_rate_eta * preconditioned_grad\n",
    "        path.append(x.copy())\n",
    "\n",
    "        if np.linalg.norm(current_grad) < 1e-7:\n",
    "            # print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "        if np.any(np.abs(x) > 1e7): # Divergence check\n",
    "            print(f\"Diverged at iteration {i+1}\")\n",
    "            break\n",
    "    return np.array(path)\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "# 1. Ill-Conditioned Convex (Elliptical Paraboloid)\n",
    "S_ill = 100\n",
    "def f_ill(x1, x2, S=S_ill): return x1**2 + S * x2**2\n",
    "def grad_f_ill(x1, x2, S=S_ill): return np.array([2 * x1, 2 * S * x2])\n",
    "def diag_hess_abs_f_ill(x1, x2, S=S_ill): return np.array([abs(2.0), abs(2.0 * S)])\n",
    "\n",
    "# 2. Well-Conditioned Convex (Circular Paraboloid)\n",
    "def f_well(x1, x2): return x1**2 + x2**2\n",
    "def grad_f_well(x1, x2): return np.array([2 * x1, 2 * x2])\n",
    "def diag_hess_abs_f_well(x1, x2): return np.array([abs(2.0), abs(2.0)])\n",
    "\n",
    "# 3. Rosenbrock Function (Non-Convex)\n",
    "def f_rosen(x1, x2): return (1 - x1)**2 + 100 * (x2 - x1**2)**2\n",
    "def grad_f_rosen(x1, x2):\n",
    "    g1 = -2 * (1 - x1) - 400 * x1 * (x2 - x1**2)\n",
    "    g2 = 200 * (x2 - x1**2)\n",
    "    return np.array([g1, g2])\n",
    "def diag_hess_abs_f_rosen(x1, x2):\n",
    "    h11 = 2 - 400 * (x2 - x1**2) + 1200 * x1**2 # Corrected from 800 to 1200\n",
    "    h22 = 200.0\n",
    "    return np.array([abs(h11), abs(h22)])\n",
    "\n",
    "# 4. Simple Non-Convex (Two Minima)\n",
    "def f_two_min(x1, x2): return x1**4 - 2*x1**2 + x2**2\n",
    "def grad_f_two_min(x1, x2): return np.array([4*x1**3 - 4*x1, 2*x2])\n",
    "def diag_hess_abs_f_two_min(x1, x2):\n",
    "    h11 = 12*x1**2 - 4\n",
    "    h22 = 2.0\n",
    "    return np.array([abs(h11), abs(h22)])\n",
    "\n",
    "\n",
    "# --- Run Experiments ---\n",
    "eta = 0.5 # A reasonably robust learning rate for this preconditioned method\n",
    "# For quadratics, eta=1 is often optimal, but 0.5 is safer for non-quadratics\n",
    "iters = 200\n",
    "\n",
    "print(\"--- 1. Ill-Conditioned Elliptical Paraboloid ---\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_ill, grad_f_ill, diag_hess_abs_f_ill,\n",
    "              [10.0, 1.0], 1.0, 50, \"Ill-Conditioned Convex\", S_param=S_ill) # eta=1 is good here\n",
    "\n",
    "print(\"\\n--- 2. Well-Conditioned Circular Paraboloid ---\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_well, grad_f_well, diag_hess_abs_f_well,\n",
    "              [10.0, 1.0], 1.0, 50, \"Well-Conditioned Convex\") # eta=1 is good here\n",
    "\n",
    "print(\"\\n--- 3. Rosenbrock Function ---\")\n",
    "# Rosenbrock needs smaller eta and more iterations\n",
    "run_optimizer(preconditioned_gradient_descent, f_rosen, grad_f_rosen, diag_hess_abs_f_rosen,\n",
    "              [-1.5, 1.5], 0.1, 500, \"Rosenbrock (Non-Convex)\") # Try smaller eta\n",
    "\n",
    "print(\"\\n--- 4. Two Minima Function ---\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_two_min, grad_f_two_min, diag_hess_abs_f_two_min,\n",
    "              [0.5, 0.5], eta, iters, \"Two Minima (Start near saddle)\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_two_min, grad_f_two_min, diag_hess_abs_f_two_min,\n",
    "              [2.0, 0.5], eta, iters, \"Two Minima (Start near x1=1 min)\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_two_min, grad_f_two_min, diag_hess_abs_f_two_min,\n",
    "              [-2.0, -0.5], eta, iters, \"Two Minima (Start near x1=-1 min)\")\n",
    "\n",
    "S_rot = 100 # Use the same S as in the ill-conditioned example\n",
    "\n",
    "# Rotated function f_rot(u1, u2)\n",
    "def f_rotated(u1, u2, S=S_rot):\n",
    "    return 0.5 * ((1+S)*u1**2 + (1+S)*u2**2 + 2*(S-1)*u1*u2)\n",
    "\n",
    "def grad_f_rotated(u1, u2, S=S_rot):\n",
    "    g_u1 = (1+S)*u1 + (S-1)*u2\n",
    "    g_u2 = (S-1)*u1 + (1+S)*u2\n",
    "    return np.array([g_u1, g_u2])\n",
    "\n",
    "def diag_hess_abs_f_rotated(u1, u2, S=S_rot):\n",
    "    # Diagonal elements of H_rot are both (1+S)\n",
    "    h_diag = 1.0 + S\n",
    "    return np.array([abs(h_diag), abs(h_diag)])\n",
    "\n",
    "print(\"\\n--- 5. Rotated Ill-Conditioned Function (45 degrees) ---\")\n",
    "# Start point in the rotated coordinate system\n",
    "# If original start was (10,1), rotated start could be approx (10/‚àö2 + 1/‚àö2, -10/‚àö2 + 1/‚àö2)\n",
    "# Or just pick a challenging start like [10.0, 1.0] in u1, u2 space\n",
    "rotated_start = [7.0, -7.0] # Example start in u1, u2 space\n",
    "\n",
    "# Let's compare with standard GD as well for this one\n",
    "def standard_gradient_descent(grad_f, start_point, learning_rate_eta, iterations, S_param=None):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()]\n",
    "    for i in range(iterations):\n",
    "        current_grad = grad_f(x[0], x[1], S_param) if S_param is not None else grad_f(x[0], x[1])\n",
    "        x = x - learning_rate_eta * current_grad\n",
    "        path.append(x.copy())\n",
    "        if np.linalg.norm(current_grad) < 1e-7: break\n",
    "        if np.any(np.abs(x) > 1e7): print(\"Std GD Diverged\"); break\n",
    "    return np.array(path)\n",
    "\n",
    "eta_rotated = 0.005 # Need a smaller eta for the rotated version (both methods)\n",
    "iters_rotated = 300\n",
    "\n",
    "path_prec_rot = run_optimizer(preconditioned_gradient_descent, f_rotated, grad_f_rotated, diag_hess_abs_f_rotated,\n",
    "                  rotated_start, eta_rotated, iters_rotated, \"Rotated Ill-Conditioned (Preconditioned GD)\", S_param=S_rot)\n",
    "\n",
    "print(\"\\nComparing with Standard GD on Rotated function:\")\n",
    "path_std_gd_rot = standard_gradient_descent(grad_f_rotated, rotated_start, eta_rotated, iters_rotated, S_param=S_rot)\n",
    "\n",
    "# Visualization for comparison\n",
    "plt.figure(figsize=(10, 7))\n",
    "x1_vals_rot = np.linspace(min(path_prec_rot[:,0].min(), path_std_gd_rot[:,0].min()) -1, max(path_prec_rot[:,0].max(), path_std_gd_rot[:,0].max()) +1, 200)\n",
    "x2_vals_rot = np.linspace(min(path_prec_rot[:,1].min(), path_std_gd_rot[:,1].min()) -1, max(path_prec_rot[:,1].max(), path_std_gd_rot[:,1].max()) +1, 200)\n",
    "X1_rot, X2_rot = np.meshgrid(x1_vals_rot, x2_vals_rot)\n",
    "Z_rot = f_rotated(X1_rot, X2_rot, S_rot)\n",
    "\n",
    "levels_rot = np.logspace(np.log10(max(Z_rot.min(),0.01)), np.log10(Z_rot.max() if Z_rot.max() > 0 else 1), 30) if Z_rot.min() < Z_rot.max() else 15\n",
    "contour_rot = plt.contour(X1_rot, X2_rot, Z_rot, levels=levels_rot, cmap='viridis')\n",
    "plt.colorbar(contour_rot, label='f_rot(u‚ÇÅ, u‚ÇÇ)')\n",
    "\n",
    "plt.plot(path_prec_rot[:, 0], path_prec_rot[:, 1], 'g-o', markersize=3, linewidth=1, label=f'Preconditioned GD (Œ∑={eta_rotated})')\n",
    "plt.plot(path_std_gd_rot[:, 0], path_std_gd_rot[:, 1], 'm--x', markersize=3, linewidth=1, label=f'Standard GD (Œ∑={eta_rotated})')\n",
    "\n",
    "plt.scatter(0, 0, color='black', marker='*', s=150, label='Minimum (0,0)', zorder=5) # Minimum is still at origin\n",
    "plt.scatter(rotated_start[0], rotated_start[1], color='blue', s=100, label='Start', zorder=4)\n",
    "plt.title(f'Comparison on Rotated f(x‚ÇÅ,x‚ÇÇ) = x‚ÇÅ¬≤ + {S_rot}x‚ÇÇ¬≤')\n",
    "plt.xlabel('u‚ÇÅ (rotated coord)')\n",
    "plt.ylabel('u‚ÇÇ (rotated coord)')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc75a3-99ed-42e7-b104-3fa855c178d1",
   "metadata": {},
   "source": [
    "#### V·∫•n ƒë·ªÅ: ·∫¢nh h∆∞·ªüng c·ªßa ph√©p quay ƒë·∫øn c·∫•u tr√∫c Hessian\n",
    "\n",
    "Khi quay m·ªôt h√†m nh∆∞:$f(x_1, x_2) = x_1^2 + 100x_2^2$ m·ªôt g√≥c 45 ƒë·ªô, h√†m m·ªõi (v·ªõi t·ªça ƒë·ªô g·ªçi l√† $u_1, u_2$) s·∫Ω c√≥ c√°c **th√†nh ph·∫ßn ngo√†i ƒë∆∞·ªùng ch√©o** m√† gi√° tr·ªã c·ªßa ch√∫ng l·ªõn h∆°n gi√° tr·ªã c·ªßa th√†nh ph·∫ßn tr√™n ƒë∆∞·ªùng ch√©o trong ma tr·∫≠n Hessian.\n",
    "\n",
    "C√°c **tr·ª•c ch√≠nh** c·ªßa c√°c ƒë∆∞·ªùng ƒë·ªìng m·ª©c h√¨nh elip **s·∫Ω kh√¥ng c√≤n th·∫≥ng h√†ng** v·ªõi c√°c tr·ª•c $u_1$ v√† $u_2$ n·ªØa.\n",
    "\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† h√¨nh d·∫°ng c·ªßa b√†i to√°n b·ªã \"nghi√™ng\", l√†m cho c√°c ph∆∞∆°ng ph√°p nh∆∞ Gradient Descent hay ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o d·ª±a tr√™n c√°c ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o **m·∫•t hi·ªáu qu·∫£** r√µ r·ªát.\n",
    "\n",
    "#### H√†m g·ªëc (t·ªça ƒë·ªô $x_1, x_2$):\n",
    "\n",
    "Gi·∫£ s·ª≠ ta c√≥ h√†m $f(x_1, x_2) = x_1^2 + Sx_2^2$. Hessian $H = \\begin{bmatrix} H_{11} & 0 \\\\ 0 & H_{22} \\end{bmatrix}$\n",
    "\n",
    "L√† ma tr·∫≠n ch√©o. B·ªô ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o (diagonal preconditioner) ho·∫°t ƒë·ªông t·ªët v√¨:$M_{\\text{diag}} = [|H_{11}|, |H_{22}|]$ x√°c ƒë·ªãnh ch√≠nh x√°c c√°c t·ªâ l·ªá thay ƒë·ªïi theo t·ª´ng h∆∞·ªõng kh√°c nhau n√†y v√† chu·∫©n h√≥a ch√∫ng.\n",
    "\n",
    "#### H√†m sau khi quay (t·ªça ƒë·ªô $u_1, u_2$):\n",
    "\n",
    "Ph√©p bi·∫øn ƒë·ªïi quay 45 ƒë·ªô:$x_1 = \\frac{u_1 - u_2}{\\sqrt{2}}, \\quad x_2 = \\frac{u_1 + u_2}{\\sqrt{2}}$\n",
    "Thay v√†o h√†m: $f(x_1, x_2) = x_1^2 + Sx_2^2$\n",
    "\n",
    "Ta ƒë∆∞·ª£c h√†m sau khi quay:$f_{\\text{rot}}(u_1, u_2) = \\left( \\frac{u_1 - u_2}{\\sqrt{2}} \\right)^2 + S \\left( \\frac{u_1 + u_2}{\\sqrt{2}} \\right)^2$. \n",
    "\n",
    "R√∫t g·ªçn: \n",
    "$f_{\\text{rot}}(u_1, u_2) = \\frac{1}{2}(u_1^2 - 2u_1u_2 + u_2^2) + \\frac{S}{2}(u_1^2 + 2u_1u_2 + u_2^2)\n",
    "= \\frac{1}{2}(1 + S)u_1^2 + \\frac{1}{2}(1 + S)u_2^2 + (S - 1)u_1u_2$\n",
    "\n",
    "---\n",
    "\n",
    "#### Ma tr·∫≠n Hessian sau khi quay:\n",
    "\n",
    "$$\n",
    "H_{\\text{rot}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial u_1^2} & \\frac{\\partial^2 f}{\\partial u_1 \\partial u_2} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial u_2 \\partial u_1} & \\frac{\\partial^2 f}{\\partial u_2^2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 + S & S - 1 \\\\\n",
    "S - 1 & 1 + S\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ·∫¢nh h∆∞·ªüng ƒë·∫øn ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o:\n",
    "\n",
    "B·ªô ti·ªÅn ƒëi·ªÅu ki·ªán ch·ªâ d√πng ph·∫ßn t·ª≠ ch√©o c·ªßa Hessian:\n",
    "\n",
    "$$\n",
    "\\text{diag}(H_{\\text{rot}}) = [|1 + S|,\\ |1 + S|]\n",
    "$$\n",
    "\n",
    "N√≥ **b·ªè qua ho√†n to√†n** ph·∫ßn t·ª≠ ngo√†i ƒë∆∞·ªùng ch√©o $(S - 1)$, ƒë√¢y l√† th√†nh ph·∫ßn quan tr·ªçng khi $S$ l·ªõn.\n",
    "\n",
    "V√≠ d·ª•, n·∫øu $S = 100$ th√¨ $S - 1 = 99$ ‚Äî cho th·∫•y m·ªëi t∆∞∆°ng quan m·∫°nh gi·ªØa $u_1$ v√† $u_2$ m√† ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c. ƒêi·ªÅu n√†y khi·∫øn d·∫´n t·ªõi:\n",
    "- Hi·ªáu su·∫•t c·ªßa ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o s·∫Ω **gi·∫£m s√∫t ƒë√°ng k·ªÉ**.\n",
    "- Thu·∫≠t to√°n s·∫Ω **dao ƒë·ªông** tr·ªü l·∫°i nh∆∞ Gradient Descent th√¥ng th∆∞·ªùng.\n",
    "- T·ªëc ƒë·ªô h·ªôi t·ª• s·∫Ω ch·∫≠m l·∫°i do v·∫•n ƒë·ªÅ t·ª∑ l·ªá co gi√£n kh√¥ng c√≤n n·∫±m theo tr·ª•c t·ªça ƒë·ªô."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500338e-f41f-4614-a493-e90e0ef4f602",
   "metadata": {},
   "source": [
    "# 03. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ed53f-736b-4850-af0a-a118197e6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f423-afc9-4927-9865-400be8d7f102",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9796f1-a271-441e-afb5-005b72be94b6",
   "metadata": {},
   "source": [
    "Trong h·ªçc s√¢u (deep learning), h√†m m·ª•c ti√™u th∆∞·ªùng l√† trung b√¨nh c·ªßa c√°c h√†m m·∫•t m√°t (loss function) cho t·ª´ng m·∫´u trong t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán.\n",
    "\n",
    "Gi·∫£ s·ª≠ c√≥ m·ªôt t·∫≠p hu·∫•n luy·ªán g·ªìm $n$ m·∫´u, g·ªçi $f_i(\\mathbf{x})$ l√† h√†m m·∫•t m√°t t∆∞∆°ng ·ª©ng v·ªõi m·∫´u hu·∫•n luy·ªán th·ª© $i$, trong ƒë√≥ $\\mathbf{x}$ l√† vector tham s·ªë.\n",
    "Khi ƒë√≥, ta c√≥ h√†m m·ª•c ti√™u:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Gradient c·ªßa h√†m m·ª•c ti√™u t·∫°i $\\mathbf{x}$ ƒë∆∞·ª£c t√≠nh b·∫±ng:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "N·∫øu s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p **Gradient Descent**, chi ph√≠ t√≠nh to√°n cho m·ªói v√≤ng l·∫∑p c·∫≠p nh·∫≠t tham s·ªë s·∫Ω l√† $\\mathcal{O}(n)$, t·ª©c l√† **tƒÉng tuy·∫øn t√≠nh** theo $n$. Do ƒë√≥, khi t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán **c√†ng l·ªõn** th√¨ **chi ph√≠** cho m·ªói b∆∞·ªõc l·∫∑p c·ªßa gradient descent **c√†ng cao**.\n",
    "\n",
    "Ph∆∞∆°ng ph√°p **Stochastic Gradient Descent (SGD)** gi√∫p gi·∫£m chi ph√≠ t√≠nh to√°n ·ªü m·ªói b∆∞·ªõc l·∫∑p. ·ªû m·ªói b∆∞·ªõc c·ªßa SGD, ta ch·ªçn **ng·∫´u nhi√™n m·ªôt m·∫´u** $i \\in \\{1, \\ldots, n\\}$ t·ª´ t·∫≠p d·ªØ li·ªáu, v√† t√≠nh gradient $\\nabla f_i(\\mathbf{x})$ ƒë·ªÉ c·∫≠p nh·∫≠t tham s·ªë $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "trong ƒë√≥ $\\eta$ l√† t·ªëc ƒë·ªô h·ªçc (learning rate). Ta th·∫•y r·∫±ng chi ph√≠ t√≠nh to√°n cho m·ªói b∆∞·ªõc l·∫∑p gi·∫£m t·ª´ $\\mathcal{O}(n)$ (c·ªßa gradient descent) xu·ªëng c√≤n h·∫±ng s·ªë $\\mathcal{O}(1)$.\n",
    "\n",
    "Ngo√†i ra, c·∫ßn nh·∫•n m·∫°nh r·∫±ng gradient ng·∫´u nhi√™n $\\nabla f_i(\\mathbf{x})$ l√† m·ªôt ∆∞·ªõc l∆∞·ª£ng kh√¥ng ch·ªách (unbiased estimate) c·ªßa gradient ƒë·∫ßy ƒë·ªß $\\nabla f(\\mathbf{x})$ v√¨:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_i \\nabla f_i(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†, nh√¨n chung, gradient ng·∫´u nhi√™n l√† m·ªôt ∆∞·ªõc l∆∞·ª£ng t·ªët cho gradient th·ª±c s·ª±.\n",
    "\n",
    "B√¢y gi·ªù, ch√∫ng ta s·∫Ω so s√°nh n√≥ v·ªõi gradient descent b·∫±ng c√°ch th√™m nhi·ªÖu ng·∫´u nhi√™n c√≥ k·ª≥ v·ªçng b·∫±ng 0 v√† ph∆∞∆°ng sai b·∫±ng 1 v√†o gradient ƒë·ªÉ m√¥ ph·ªèng stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef6152-b278-4f7c-8eae-df2f70e032a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):  # Objective function\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_grad(x1, x2):  # Gradient of the objective function\n",
    "    return 2 * x1, 4 * x2\n",
    "\n",
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    # Simulate noisy gradient\n",
    "    g1 += torch.normal(0.0, 1, (1,)).item()\n",
    "    g2 += torch.normal(0.0, 1, (1,)).item()\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "def constant_lr():\n",
    "    return 1\n",
    "\n",
    "eta = 0.1\n",
    "lr = constant_lr  # Constant learning rate\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0443f-6eb0-4665-b99e-2dc924a66a22",
   "metadata": {},
   "source": [
    "Nh∆∞ ch√∫ng ta c√≥ th·ªÉ th·∫•y, qu·ªπ ƒë·∫°o c·ªßa c√°c bi·∫øn trong ph∆∞∆°ng ph√°p stochastic gradient descent nhi·ªÖu h∆°n nhi·ªÅu so v·ªõi qu·ªπ ƒë·∫°o quan s√°t ƒë∆∞·ª£c trong gradient descent. ƒêi·ªÅu n√†y l√† do b·∫£n ch·∫•t ng·∫´u nhi√™n c·ªßa gradient. C·ª• th·ªÉ, ngay c·∫£ khi ch√∫ng ta ƒë√£ ti·∫øn g·∫ßn ƒë·∫øn ƒëi·ªÉm c·ª±c ti·ªÉu, ta v·∫´n b·ªã ·∫£nh h∆∞·ªüng b·ªüi s·ª± b·∫•t ƒë·ªãnh c·ªßa gradient t·ª©c th·ªùi $\\eta \\nabla f_i(\\mathbf{x})$. Ngay c·∫£ sau 50 b∆∞·ªõc, ch·∫•t l∆∞·ª£ng nghi·ªám v·∫´n ch∆∞a t·ªët. Th·∫≠m ch√≠ t·ªá h∆°n, n√≥ s·∫Ω kh√¥ng ƒë∆∞·ª£c c·∫£i thi·ªán th√™m d√π c√≥ th·ª±c hi·ªán th√™m nhi·ªÅu b∆∞·ªõc n·ªØa.\n",
    "\n",
    "ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn l·ª±a ch·ªçn duy nh·∫•t: thay ƒë·ªïi t·ªëc ƒë·ªô h·ªçc $\\eta$. Tuy nhi√™n, n·∫øu ch·ªçn gi√° tr·ªã qu√° nh·ªè, ta s·∫Ω kh√¥ng ƒë·∫°t ƒë∆∞·ª£c ti·∫øn tri·ªÉn ƒë√°ng k·ªÉ ban ƒë·∫ßu. Ng∆∞·ª£c l·∫°i, n·∫øu ch·ªçn qu√° l·ªõn, ta s·∫Ω kh√¥ng thu ƒë∆∞·ª£c nghi·ªám t·ªët, nh∆∞ ƒë√£ th·∫•y ·ªü tr√™n. C√°ch duy nh·∫•t ƒë·ªÉ gi·∫£i quy·∫øt m√¢u thu·∫´n n√†y l√† gi·∫£m t·ªëc ƒë·ªô h·ªçc m·ªôt c√°ch *ƒë·ªông* khi qu√° tr√¨nh t·ªëi ∆∞u h√≥a ti·∫øn tri·ªÉn.\n",
    "\n",
    "ƒê√¢y c≈©ng l√† l√Ω do v√¨ sao c·∫ßn th√™m m·ªôt h√†m t·ªëc ƒë·ªô h·ªçc `lr` v√†o h√†m c·∫≠p nh·∫≠t `sgd`. Trong v√≠ d·ª• tr√™n, ta ƒë√£ ƒë·∫∑t h√†m `lr` t∆∞∆°ng ·ª©ng l√† h·∫±ng s·ªë."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021715d3-2623-4468-84fd-d2cff032b85e",
   "metadata": {},
   "source": [
    "## Dynamic Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0f72e-cf0c-4e90-8002-58a4e034c858",
   "metadata": {},
   "source": [
    "Thay th·∫ø $\\eta$ b·∫±ng m·ªôt t·ªëc ƒë·ªô h·ªçc ph·ª• thu·ªôc v√†o th·ªùi gian $\\eta(t)$ l√†m tƒÉng ƒë·ªô ph·ª©c t·∫°p trong vi·ªác ki·ªÉm so√°t s·ª± h·ªôi t·ª• c·ªßa m·ªôt thu·∫≠t to√°n t·ªëi ∆∞u. C·ª• th·ªÉ, ch√∫ng ta c·∫ßn x√°c ƒë·ªãnh t·ªëc ƒë·ªô gi·∫£m c·ªßa $\\eta$. N·∫øu gi·∫£m qu√° nhanh, qu√° tr√¨nh t·ªëi ∆∞u s·∫Ω d·ª´ng l·∫°i qu√° s·ªõm. N·∫øu gi·∫£m qu√° ch·∫≠m, ta s·∫Ω l√£ng ph√≠ qu√° nhi·ªÅu th·ªùi gian cho vi·ªác t·ªëi ∆∞u.\n",
    "\n",
    "Sau ƒë√¢y l√† m·ªôt v√†i chi·∫øn l∆∞·ª£c c∆° b·∫£n ƒë·ªÉ ƒëi·ªÅu ch·ªânh $\\eta$ theo th·ªùi gian:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta(t) & = \\eta_i \\textrm{ n·∫øu } t_i \\leq t \\leq t_{i+1}  && \\textrm{h·∫±ng theo t·ª´ng kho·∫£ng (piecewise constant)} \\\\\n",
    "    \\eta(t) & = \\eta_0 \\cdot e^{-\\lambda t} && \\textrm{gi·∫£m theo h√†m m≈© (exponential decay)} \\\\\n",
    "    \\eta(t) & = \\eta_0 \\cdot (\\beta t + 1)^{-\\alpha} && \\textrm{gi·∫£m theo ƒëa th·ª©c (polynomial decay)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Trong chi·∫øn l∆∞·ª£c *h·∫±ng theo t·ª´ng kho·∫£ng*, ch√∫ng ta c√≥ th·ªÉ gi·∫£m t·ªëc ƒë·ªô h·ªçc m·ªói khi qu√° tr√¨nh t·ªëi ∆∞u h√≥a kh√¥ng c√≤n c·∫£i thi·ªán. ƒê√¢y l√† m·ªôt chi·∫øn l∆∞·ª£c ph·ªï bi·∫øn khi hu·∫•n luy·ªán c√°c m·∫°ng n∆°-ron s√¢u. Ngo√†i ra, ch√∫ng ta c√≥ th·ªÉ gi·∫£m m·∫°nh h∆°n b·∫±ng c√°ch *gi·∫£m theo h√†m m≈©*. Tuy nhi√™n, ƒëi·ªÅu n√†y th∆∞·ªùng d·∫´n ƒë·∫øn vi·ªác d·ª´ng t·ªëi ∆∞u qu√° s·ªõm tr∆∞·ªõc khi thu·∫≠t to√°n h·ªôi t·ª•.\n",
    "\n",
    "M·ªôt l·ª±a ch·ªçn ph·ªï bi·∫øn l√† *gi·∫£m theo ƒëa th·ª©c* v·ªõi $\\alpha = 0.5$. Trong tr∆∞·ªùng h·ª£p t·ªëi ∆∞u h√≥a l·ªìi (convex optimization), c√≥ nhi·ªÅu ch·ª©ng minh cho th·∫•y t·ªëc ƒë·ªô gi·∫£m n√†y ho·∫°t ƒë·ªông hi·ªáu qu·∫£ v√† ·ªïn ƒë·ªãnh.\n",
    "\n",
    "H√£y c√πng xem vi·ªác gi·∫£m theo h√†m m≈© tr√¥ng nh∆∞ th·∫ø n√†o trong th·ª±c t·∫ø."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed384d7-2972-4be0-846f-c580c1ae5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return math.exp(-0.1 * t)\n",
    "\n",
    "t = 1\n",
    "lr = exponential_lr\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6af05-4b24-426e-9e56-bb86a728058a",
   "metadata": {},
   "source": [
    "Nh∆∞ d·ª± ƒëo√°n, ph∆∞∆°ng sai trong c√°c tham s·ªë ƒë√£ gi·∫£m ƒë√°ng k·ªÉ. Tuy nhi√™n, ƒëi·ªÅu n√†y ph·∫£i ƒë√°nh ƒë·ªïi b·∫±ng vi·ªác kh√¥ng h·ªôi t·ª• ƒë·∫øn nghi·ªám t·ªëi ∆∞u $\\mathbf{x} = (0, 0)$. Ngay c·∫£ sau 1000 b∆∞·ªõc l·∫∑p, ch√∫ng ta v·∫´n c√≤n r·∫•t xa so v·ªõi nghi·ªám t·ªëi ∆∞u. Th·ª±c t·∫ø, thu·∫≠t to√°n kh√¥ng h·ªôi t·ª• ch√∫t n√†o.\n",
    "\n",
    "Ng∆∞·ª£c l·∫°i, n·∫øu ch√∫ng ta s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p gi·∫£m theo ƒëa th·ª©c, trong ƒë√≥ t·ªëc ƒë·ªô h·ªçc gi·∫£m theo ngh·ªãch ƒë·∫£o cƒÉn b·∫≠c hai c·ªßa s·ªë b∆∞·ªõc, th√¨ kh·∫£ nƒÉng h·ªôi t·ª• ƒë∆∞·ª£c c·∫£i thi·ªán r√µ r·ªát ch·ªâ sau 50 b∆∞·ªõc l·∫∑p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05c250-d44b-40ff-8896-0dbc4c198aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return (1 + 0.1 * t) ** (-0.5)\n",
    "\n",
    "t = 1\n",
    "lr = polynomial_lr\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c143ea-bc8c-452b-b616-1a04c7b6cc93",
   "metadata": {},
   "source": [
    "C√≥ r·∫•t nhi·ªÅu c√°ch kh√°c nhau ƒë·ªÉ thi·∫øt l·∫≠p t·ªëc ƒë·ªô h·ªçc. V√≠ d·ª•, ta c√≥ th·ªÉ b·∫Øt ƒë·∫ßu v·ªõi m·ªôt t·ªëc ƒë·ªô h·ªçc nh·ªè, sau ƒë√≥ tƒÉng nhanh r·ªìi l·∫°i gi·∫£m d·∫ßn, tuy nhi√™n gi·∫£m ch·∫≠m h∆°n. Th·∫≠m ch√≠, ta c√≥ th·ªÉ xen k·∫Ω gi·ªØa t·ªëc ƒë·ªô h·ªçc nh·ªè v√† l·ªõn. C√≥ r·∫•t nhi·ªÅu chi·∫øn l∆∞·ª£c ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô h·ªçc nh∆∞ v·∫≠y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52573c3-24bf-4bd1-b96c-4bcb8d3a85d9",
   "metadata": {},
   "source": [
    "## Convergence Analysis for Convex Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a44124-9f84-4832-bc85-6402bcfd64ac",
   "metadata": {},
   "source": [
    "Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω ph√¢n t√≠ch h·ªôi t·ª• c·ªßa ph∆∞∆°ng ph√°p stochastic gradient descent (SGD) ƒë·ªëi v·ªõi h√†m m·ª•c ti√™u l·ªìi t√πy ch·ªçn.\n",
    "\n",
    "Gi·∫£ s·ª≠ h√†m m·ª•c ti√™u $f(\\boldsymbol{\\xi}, \\mathbf{x})$ l√† l·ªìi theo $\\mathbf{x}$ v·ªõi m·ªçi m·∫´u $\\boldsymbol{\\xi}$. C·ª• th·ªÉ, ta x√©t c√¥ng th·ª©c c·∫≠p nh·∫≠t c·ªßa SGD:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - \\eta_t \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x}),\n",
    "$$\n",
    "\n",
    "trong ƒë√≥ $f(\\boldsymbol{\\xi}_t, \\mathbf{x})$ l√† h√†m m·ª•c ti√™u t∆∞∆°ng ·ª©ng v·ªõi m·∫´u hu·∫•n luy·ªán $\\boldsymbol{\\xi}_t$ ƒë∆∞·ª£c l·∫•y ng·∫´u nhi√™n t·∫°i b∆∞·ªõc $t$, v√† $\\mathbf{x}$ l√† tham s·ªë m√¥ h√¨nh. G·ªçi:\n",
    "\n",
    "$$\n",
    "R(\\mathbf{x}) = E_{\\boldsymbol{\\xi}}[f(\\boldsymbol{\\xi}, \\mathbf{x})]\n",
    "$$\n",
    "\n",
    "l√† r·ªßi ro k·ª≥ v·ªçng, v√† $R^*$ l√† gi√° tr·ªã t·ªëi thi·ªÉu c·ªßa n√≥ theo $\\mathbf{x}$. G·ªçi $\\mathbf{x}^*$ l√† ƒëi·ªÉm t·ªëi ∆∞u (gi·∫£ ƒë·ªãnh t·ªìn t·∫°i trong mi·ªÅn x√°c ƒë·ªãnh c·ªßa $\\mathbf{x}$). Khi ƒë√≥ ta c√≥ th·ªÉ theo d√µi kho·∫£ng c√°ch gi·ªØa tham s·ªë hi·ªán t·∫°i $\\mathbf{x}_t$ t·∫°i th·ªùi ƒëi·ªÉm $t$ v√† ƒëi·ªÉm t·ªëi ∆∞u $\\mathbf{x}^*$ ƒë·ªÉ xem li·ªáu c√≥ c·∫£i thi·ªán theo th·ªùi gian kh√¥ng:\n",
    "\n",
    "$$\\begin{aligned}    &\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 \\\\ =& \\|\\mathbf{x}_{t} - \\eta_t \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x}) - \\mathbf{x}^*\\|^2 \\\\    =& \\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 + \\eta_t^2 \\|\\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\|^2 - 2 \\eta_t    \\left\\langle \\mathbf{x}_t - \\mathbf{x}^*, \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\right\\rangle. \\end{aligned} \\tag{1} $$\n",
    "\n",
    "Gi·∫£ s·ª≠ chu·∫©n $\\ell_2$ c·ªßa gradient ng·∫´u nhi√™n b·ªã ch·∫∑n b·ªüi m·ªôt h·∫±ng s·ªë $L$, ta c√≥:\n",
    "\n",
    "$$\n",
    "\\eta_t^2 \\|\\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\|^2 \\leq \\eta_t^2 L^2. \\tag{2}\n",
    "$$\n",
    "\n",
    "Ch√∫ng ta quan t√¢m ch·ªß y·∫øu ƒë·∫øn vi·ªác, trung b√¨nh, kho·∫£ng c√°ch gi·ªØa $\\mathbf{x}_t$ v√† $\\mathbf{x}^*$ thay ƒë·ªïi nh∆∞ th·∫ø n√†o. Tr√™n th·ª±c t·∫ø, kho·∫£ng c√°ch n√†y c√≥ th·ªÉ tƒÉng ho·∫∑c gi·∫£m, t√πy thu·ªôc v√†o m·∫´u $\\boldsymbol{\\xi}_t$ m√† ta g·∫∑p ph·∫£i. Do ƒë√≥, ta c·∫ßn ch·∫∑n t√≠ch v√¥ h∆∞·ªõng.\n",
    "\n",
    "V·ªõi m·ªçi h√†m l·ªìi $f$, ta c√≥ [(first-order condition)](https://machinelearningcoban.com/2017/03/12/convexity/#-first-order-condition):\n",
    "\n",
    "$$\n",
    "f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\langle f'(\\mathbf{x}), \\mathbf{y} - \\mathbf{x} \\rangle.\n",
    "$$\n",
    "\n",
    "√Åp d·ª•ng v·ªõi $\\mathbf{x}_t$ v√† $\\mathbf{x}^*$, ta ƒë∆∞·ª£c:\n",
    "<!-- $$\\begin{aligned}\n",
    " f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) &\\geq f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) + \\left\\langle \\mathbf{x}^* - \\mathbf{x}_t, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle\\\\ \\left\\langle \\mathbf{x}^* - \\mathbf{x}_t, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle &\\leq f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t)\\\\ - \\left\\langle \\mathbf{x}_t - \\mathbf{x}^*, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle &\\leq f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\label{eq:bat_dang_thuc_first_order_condition} \\tag{2}\n",
    "\\end{aligned}$$ -->\n",
    "$$ f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) \\geq f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) + \\left\\langle \\mathbf{x}^* - \\mathbf{x}_t, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle $$\n",
    "$$ \\left\\langle \\mathbf{x}^* - \\mathbf{x}_t, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle \\leq f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) $$\n",
    "$$ - \\left\\langle \\mathbf{x}_t - \\mathbf{x}^*, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle \\leq f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\tag{3} $$\n",
    "Thay hai b·∫•t ƒë·∫≥ng th·ª©c $(2)$ v√† $(3)$ v√†o bi·ªÉu th·ª©c $(1)$, ta thu ƒë∆∞·ª£c:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 &\\leq \\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 + \\eta_t^2 L^2 - 2 \\eta_t (f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t))\\\\\n",
    "\\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 &\\geq 2 \\eta_t (f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*)) - \\eta_t^2 L^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† ta ƒëang ti·∫øn g·∫ßn ƒë·∫øn ƒëi·ªÉm t·ªëi ∆∞u mi·ªÖn l√† s·ª± kh√°c bi·ªát gi·ªØa gi√° tr·ªã m·∫•t m√°t (loss) hi·ªán t·∫°i v√† gi√° tr·ªã m·∫•t m√°t t·ªëi ∆∞u l·ªõn h∆°n $\\eta_t L^2 / 2$. Do s·ª± kh√°c bi·ªát n√†y s·∫Ω ti·∫øn d·∫ßn v·ªÅ 0 n√™n t·ªëc ƒë·ªô h·ªçc $\\eta_t$ c≈©ng c·∫ßn *gi·∫£m d·∫ßn*.\n",
    "\n",
    "Ti·∫øp theo, l·∫•y k·ª≥ v·ªçng c·ªßa b·∫•t ƒë·∫≥ng th·ª©c tr√™n:\n",
    "\n",
    "$$\n",
    "E\\left[\\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2\\right] - E\\left[\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2\\right] \\geq 2 \\eta_t [E[R(\\mathbf{x}_t)] - R^*] -  \\eta_t^2 L^2.\n",
    "$$\n",
    "\n",
    "T·ªïng c√°c b·∫•t ƒë·∫≥ng th·ª©c n√†y v·ªõi $t = 1, \\ldots, T$, v√† b·ªè ƒëi ph·∫ßn √¢m, ta ƒë∆∞·ª£c:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}_1 - \\mathbf{x}^*\\|^2 \\geq 2 \\left (\\sum_{t=1}^T   \\eta_t \\right) [E[R(\\mathbf{x}_t)] - R^*] - L^2 \\sum_{t=1}^T \\eta_t^2.\n",
    "$$\n",
    "\n",
    "Do $\\mathbf{x}_1$ l√† gi√° tr·ªã ƒë√£ bi·∫øt n√™n ta b·ªè k·ª≥ v·ªçng. ƒê·ªãnh nghƒ©a:\n",
    "\n",
    "$$\n",
    "\\bar{\\mathbf{x}} := \\frac{\\sum_{t=1}^T \\eta_t \\mathbf{x}_t}{\\sum_{t=1}^T \\eta_t}.\n",
    "$$\n",
    "\n",
    "Ta c√≥:\n",
    "\n",
    "$$\n",
    "E\\left(\\frac{\\sum_{t=1}^T \\eta_t R(\\mathbf{x}_t)}{\\sum_{t=1}^T \\eta_t}\\right) = \\frac{\\sum_{t=1}^T \\eta_t E[R(\\mathbf{x}_t)]}{\\sum_{t=1}^T \\eta_t} = E[R(\\mathbf{x}_t)].\n",
    "$$\n",
    "\n",
    "Do b·∫•t ƒë·∫≥ng th·ª©c Jensen v√† t√≠nh l·ªìi c·ªßa $R$, ta c√≥:\n",
    "\n",
    "$$\n",
    "E[R(\\mathbf{x}_t)] \\geq E[R(\\bar{\\mathbf{x}})],\n",
    "\\Rightarrow \\sum_{t=1}^T \\eta_t E[R(\\mathbf{x}_t)] \\geq \\sum_{t=1}^T \\eta_t  E\\left[R(\\bar{\\mathbf{x}})\\right].\n",
    "$$\n",
    "\n",
    "Thay v√†o b·∫•t ƒë·∫≥ng th·ª©c tr√™n, ta ƒë∆∞·ª£c:\n",
    "\n",
    "$$\n",
    "\\left[E[R(\\bar{\\mathbf{x}})]\\right] - R^* \\leq \\frac{r^2 + L^2 \\sum_{t=1}^T \\eta_t^2}{2 \\sum_{t=1}^T \\eta_t},\n",
    "$$\n",
    "\n",
    "v·ªõi $r^2 := \\|\\mathbf{x}_1 - \\mathbf{x}^*\\|^2$ l√† ƒë·ªô ch√™nh l·ªách gi·ªØa ƒëi·ªÉm kh·ªüi ƒë·∫ßu v√† ƒëi·ªÉm t·ªëi ∆∞u. T√≥m l·∫°i, t·ªëc ƒë·ªô h·ªôi t·ª• ph·ª• thu·ªôc v√†o vi·ªác gradient ng·∫´u nhi√™n ƒë∆∞·ª£c ch·∫∑n nh∆∞ th·∫ø n√†o ($L$) v√† ƒëi·ªÉm b·∫Øt ƒë·∫ßu c√°ch xa t·ªëi ∆∞u bao nhi√™u ($r$).\n",
    "\n",
    "Khi $r, L$, v√† $T$ ƒë√£ bi·∫øt, ta c√≥ th·ªÉ ch·ªçn t·ªëc ƒë·ªô h·ªçc $\\eta = r/(L \\sqrt{T})$. Khi ƒë√≥, ta c√≥ ch·∫∑n tr√™n l√† $rL/\\sqrt{T}$, t·ª©c ta h·ªôi t·ª• v·ªõi t·ªëc ƒë·ªô $\\mathcal{O}(1/\\sqrt{T})$ ƒë·∫øn nghi·ªám t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64582163-40de-4db8-8aad-65148ef8e17e",
   "metadata": {},
   "source": [
    "## Stochastic Gradients and Finite Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321623a-710c-43c7-999e-01b02362994b",
   "metadata": {},
   "source": [
    "Cho ƒë·∫øn gi·ªù, ch√∫ng ta ƒë√£ n√≥i v·ªÅ **stochastic gradient descent (SGD)** m·ªôt c√°ch kh√° ƒë∆°n gi·∫£n v√† s∆° l∆∞·ª£c. Ch√∫ng ta gi·∫£ ƒë·ªãnh r·∫±ng ta l·∫•y c√°c m·∫´u $x_i$, th∆∞·ªùng ƒëi k√®m v·ªõi nh√£n $y_i$, t·ª´ m·ªôt ph√¢n ph·ªëi n√†o ƒë√≥ $p(x, y)$ v√† s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ c·∫≠p nh·∫≠t c√°c tham s·ªë c·ªßa m√¥ h√¨nh theo m·ªôt c√°ch n√†o ƒë√≥. C·ª• th·ªÉ h∆°n, v·ªõi m·ªôt t·∫≠p d·ªØ li·ªáu h·ªØu h·∫°n, ta ƒë√£ l·∫≠p lu·∫≠n r·∫±ng ph√¢n ph·ªëi r·ªùi r·∫°c $p(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}(x) \\delta_{y_i}(y)$\n",
    "v·ªõi m·ªôt s·ªë h√†m $\\delta_{x_i}$ v√† $\\delta_{y_i}$\n",
    "cho ph√©p ta th·ª±c hi·ªán SGD tr√™n ph√¢n ph·ªëi ƒë√≥.\n",
    "\n",
    "Tuy nhi√™n, th·ª±c t·∫ø kh√¥ng ho√†n to√†n nh∆∞ v·∫≠y. Trong c√°c v√≠ d·ª• minh h·ªça ƒë∆°n gi·∫£n ·ªü ph·∫ßn n√†y, ta ƒë∆°n gi·∫£n ch·ªâ th√™m nhi·ªÖu v√†o gradient kh√¥ng ng·∫´u nhi√™n, t·ª©c l√† ta gi·∫£ v·ªù nh∆∞ ƒëang c√≥ c√°c c·∫∑p $(x_i, y_i)$. ƒêi·ªÅu n√†y l√† h·ª£p l√Ω trong ng·ªØ c·∫£nh ·ªü ƒë√¢y (xem b√†i t·∫≠p ƒë·ªÉ hi·ªÉu chi ti·∫øt h∆°n). ƒêi·ªÅu ƒë√°ng lo ng·∫°i h∆°n l√† trong c√°c ph·∫ßn tr∆∞·ªõc, r√µ r√†ng ta kh√¥ng l√†m nh∆∞ v·∫≠y. Thay v√†o ƒë√≥, ta **duy·ªát qua t·∫•t c·∫£ c√°c m·∫´u hu·∫•n luy·ªán ƒë√∫ng m·ªôt l·∫ßn**. ƒê·ªÉ th·∫•y t·∫°i sao c√°ch l√†m n√†y t·ªët h∆°n, h√£y x√©t t√¨nh hu·ªëng ng∆∞·ª£c l·∫°i: gi·∫£ s·ª≠ ta ch·ªçn ng·∫´u nhi√™n $n$ quan s√°t t·ª´ ph√¢n ph·ªëi r·ªùi r·∫°c **c√≥ ho√†n l·∫°i**. X√°c su·∫•t ƒë·ªÉ ch·ªçn m·ªôt ph·∫ßn t·ª≠ $i$ b·∫•t k·ª≥ l√† $1/n$. Do ƒë√≥, x√°c su·∫•t ƒë·ªÉ **ch·ªçn ƒë∆∞·ª£c √≠t nh·∫•t m·ªôt l·∫ßn** l√†:\n",
    "\n",
    "$P(\\textrm{ch·ªçn~} i) = 1 - P(\\textrm{kh√¥ng ch·ªçn~} i) = 1 - (1-1/n)^n \\approx 1-e^{-1} \\approx 0.63.$\n",
    "\n",
    "L·∫≠p lu·∫≠n t∆∞∆°ng t·ª± cho th·∫•y x√°c su·∫•t ch·ªçn m·ªôt m·∫´u **ch√≠nh x√°c m·ªôt l·∫ßn duy nh·∫•t** l√†:\n",
    "\n",
    "${n \\choose 1} \\frac{1}{n} \\left(1-\\frac{1}{n}\\right)^{n-1} = \\frac{n}{n-1} \\left(1-\\frac{1}{n}\\right)^{n} \\approx e^{-1} \\approx 0.37.$\n",
    "\n",
    "Vi·ªác l·∫•y m·∫´u **c√≥ ho√†n l·∫°i** l√†m tƒÉng ph∆∞∆°ng sai v√† gi·∫£m hi·ªáu qu·∫£ s·ª≠ d·ª•ng d·ªØ li·ªáu so v·ªõi l·∫•y m·∫´u **kh√¥ng ho√†n l·∫°i**. Do ƒë√≥, trong th·ª±c t·∫ø, ta th∆∞·ªùng d√πng c√°ch l·∫•y m·∫´u **kh√¥ng ho√†n l·∫°i** (v√† ƒë√¢y c≈©ng l√† c√°ch ƒë∆∞·ª£c m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng xuy√™n su·ªët trong cu·ªën s√°ch n√†y). Cu·ªëi c√πng, l∆∞u √Ω r·∫±ng n·∫øu duy·ªát l·∫°i t·∫≠p hu·∫•n luy·ªán nhi·ªÅu l·∫ßn th√¨ m·ªói l·∫ßn nh∆∞ v·∫≠y s·∫Ω duy·ªát qua t·∫≠p d·ªØ li·ªáu theo **m·ªôt th·ª© t·ª± ng·∫´u nhi√™n kh√°c nhau**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f04a1e-302e-4752-8383-44cbb2d74af9",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a4dae-2fac-4779-bf48-a861dd649eee",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2640f78-8dea-40aa-a8c9-344c16727f34",
   "metadata": {},
   "source": [
    "Experiment with different learning rate schedules for stochastic gradient descent and with different numbers of iterations. In particular, plot the distance from the optimal solution $(0, 0)$ as a function of the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd588328-307f-43da-b70b-edc7c6b98b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    dist_to_optimum.append(d2l.np.linalg.norm([x1, x2]).item())\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    # Simulate noisy gradient\n",
    "    g1 += torch.normal(0.0, 1, (1,)).item()\n",
    "    g2 += torch.normal(0.0, 1, (1,)).item()\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "# CONSTANT LEARNING RATE\n",
    "dist_to_optimum = []\n",
    "eta = 0.1\n",
    "lr = constant_lr  # Constant learning rate\n",
    "d2l.train_2d(sgd, steps=50, f_grad=f_grad)\n",
    "\n",
    "# Plotting\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('Constant learning rate: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()\n",
    "\n",
    "# EXPONENTIAL LEARNING RATE\n",
    "dist_to_optimum = []\n",
    "t = 1\n",
    "lr = exponential_lr\n",
    "d2l.train_2d(sgd, steps=50, f_grad=f_grad)\n",
    "\n",
    "# Plotting\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('Exponential learning rate: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()\n",
    "\n",
    "# POLYNOMIAL LEARNING RATE\n",
    "dist_to_optimum = []\n",
    "t = 1\n",
    "lr = polynomial_lr\n",
    "d2l.train_2d(sgd, steps=50, f_grad=f_grad)\n",
    "\n",
    "# Plotting\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('Polynomial learning rate: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094e35d-8ebe-4bf6-8ef3-022eac747065",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313de73f-acf8-4922-aeeb-573c181baece",
   "metadata": {},
   "source": [
    "Prove that for the function $f(x_1, x_2) = x_1^2 + 2 x_2^2$ adding normal noise to the gradient is equivalent to minimizing a loss function $f(\\mathbf{x}, \\mathbf{w}) = (x_1 - w_1)^2 + 2 (x_2 - w_2)^2$ where $\\mathbf{x}$ is drawn from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581b45d",
   "metadata": {},
   "source": [
    "Ta c√≥ gradient c·ªßa h√†m $f(x_1,x_2)$:\n",
    "$$ \\nabla f(x_1, x_2) = \\begin{bmatrix} 2x_1 \\\\ 4x_2 \\end{bmatrix} \\tag{1} $$\n",
    "\n",
    "gradient sau khi ƒë∆∞·ª£c th√™m nhi·ªÖu:\n",
    "$$\\tilde{\\nabla} f(x) = \\nabla f(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$$\n",
    "\n",
    "Th∆∞c hi·ªán l·∫•y gradient c·ªßa h√†m $f(x,w)$:\n",
    "\n",
    "Ta c√≥ \n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} L(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial}{\\partial w_1} (x_1 - w_1)^2 \\\\ \\frac{\\partial}{\\partial w_2} 2(x_2 - w_2)^2 \\end{bmatrix}\n",
    "=  \\begin{bmatrix} -2(x_1 - w_1) \\\\ -4(x_2 - w_2) \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Th·ª±c hi√™n l·∫•y k·ª≥ v·ªçng c·ªßa c√¥ng th·ª©c tr√™n ta c√≥:\n",
    "$$ \\nabla_{\\mathbf{w}} L(\\mathbf{w}) = \\mathbb{E} \\left[ \\begin{bmatrix} -2(x_1 - w_1) \\\\ -4(x_2 - w_2) \\end{bmatrix} \\right] $$ \n",
    "\n",
    "T∆∞∆°ng ƒë∆∞∆°ng \n",
    "$$ \\begin{bmatrix} -2(\\mathbb{E} \\left[x_1\\right] - w_1) \\\\ -4(\\mathbb{E} \\left[x_2\\right] - w_2) \\end{bmatrix} $$ \n",
    "\n",
    "V√¨ $x$ c√≥ ph√¢n ph·ªëi chu·∫©n $\\mathbf{x} \\sim \\mathcal{N}(0, I)$ v√† x1 v√† x2 l√† hai gi√° tr·ªã ng·∫´u nhi√™n n√™n $\\mathbb{E} \\left[x_1\\right] =  \\left[x_2\\right] = 0$:\n",
    "$$ \\nabla_{\\mathbf{w}} L(\\mathbf{w}) = \\begin{bmatrix} 2(w_1) \\\\ 4( w_2) \\end{bmatrix} \\tag{2} $$ \n",
    "T·ª´ $(1)$ v√† $(2)$ c√≥ th·ªÉ th·∫•y h√†m $f(x_1, x_2)$ = $x_1^2 + x_2^2$ c√≥ th√™m nhi·ªÖu v√†o gradient c√≥ th·ªÉ t∆∞∆°ng ƒë∆∞∆°ng v·ªõi gradient c·ªßa h√†m $f(x,w) = (x_1-w_1)^2 +2(x_2-w_2)^2$ d√πng ƒë·ªÉ c·∫≠p nh·∫≠t tham s·ªë v√† t√¨m ƒëi·ªÉm c·ª±c ti·ªÉu c·ªßa h√†m.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec09cc-7b25-45b2-9fd9-5ffbc2b62e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for f(x1,x2) \n",
    "def f1(x1,x2):\n",
    "    return x1**2 + 2*x2**2\n",
    "\n",
    "# Define gradient of f(x1,x2)\n",
    "def f1_grad(x1,x2):\n",
    "    return 2*x1, 4*x2 \n",
    "\n",
    "# Define gradient of f(x,w)\n",
    "def f2(x,w):\n",
    "    return (x[0] - w[0])**2 + 2*(x[1] - w[1])**2\n",
    "\n",
    "# Define gradient of f(x,w)\n",
    "def f2_grad(x,w):\n",
    "    return -2*(x[0] - w[0]), -4*(x[1] - w[1])\n",
    "\n",
    "g_f1= []\n",
    "# Gradient computing and weight updating for f(x1,x2)\n",
    "def sgd_f1(x1, x2, s1, s2, f1_grad):\n",
    "    g1, g2 = f1_grad(x1, x2)\n",
    "    # Simulate noisy gradient\n",
    "    g1 += torch.normal(0.0, 1, (1,)).item() #Add normal noise\n",
    "    g2 += torch.normal(0.0, 1, (1,)).item() #Add normal noise\n",
    "    g_f1.append(torch.tensor([g1,g2]))\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "noise_std = 1.0\n",
    "g_f2= []\n",
    "x_sample = []\n",
    "# Gradient computing and weight updating for f(x,w)\n",
    "def sgd_f2(x1, x2, s1, s2, f2_grad):\n",
    "    x= torch.randn(2)\n",
    "    x_sample.append(x)\n",
    "    w = torch.tensor([x1,x2])\n",
    "    g1, g2 = f2_grad(x, w)\n",
    "    g_f2.append(torch.tensor([g1,g2]))\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "\n",
    "eta = 0.1\n",
    "lr = polynomial_lr  # Constant learning rate\n",
    "result_f1 = d2l.train_2d(sgd_f1, steps=1000, f_grad=f1_grad)\n",
    "result_f2 = d2l.train_2d(sgd_f2, steps=1000, f_grad=f2_grad)\n",
    "print(\"Average gradient for f(x1,x2) when adding noise gradient:\", torch.stack(g_f1).mean(dim=0))\n",
    "print(\"Average gradient for f(x,w)\",torch.stack(g_f2).mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d6095-b7eb-493b-bc3f-da9466804e06",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fbfab-42b4-40d7-9f32-02c4b51cf49c",
   "metadata": {},
   "source": [
    "Compare convergence of stochastic gradient descent when you sample from $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$ with replacement and when you sample without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cfd0f",
   "metadata": {},
   "source": [
    "Ta c√≥ b·∫•t ƒë·∫≥ng th·ª©c ƒë√°nh gi√° v·ªÅ t·ªëc ƒë·ªô h·ªôi t·ª• t·ª´ ph·∫ßn 12.4.3: \n",
    "$$\n",
    "\\left[E[R(\\bar{\\mathbf{x}})]\\right] - R^* \\leq \\frac{r^2 + L^2 \\sum_{t=1}^T \\eta_t^2}{2 \\sum_{t=1}^T \\eta_t},\n",
    "$$\n",
    "\n",
    "V·ªõi vi·ªác l·∫•y m·∫´u c√≥ Ho√†n L·∫°i:\n",
    "V·ªõi t·ªëc ƒë·ªô h·ªçc c·ªë ƒë·ªãnh $\\eta = \\frac{r}{L\\sqrt{T}}$:\n",
    "$$\\sum_{t=1}^{T} \\eta_t = T \\cdot \\frac{r}{L\\sqrt{T}} = \\frac{rT}{L\\sqrt{T}} = \\frac{r\\sqrt{T}}{L}$$\n",
    "\n",
    "$$\\sum_{t=1}^{T} \\eta_t^2 = T \\cdot \\left(\\frac{r}{L\\sqrt{T}}\\right)^2 = T \\cdot \\frac{r^2}{L^2T} = \\frac{r^2}{L^2}$$\n",
    "Thay v√†o bi·ªÉu th·ª©c gi·ªõi h·∫°n ban ƒë·∫ßu:\n",
    "$$[E[R(\\bar{x})]] - R^* \\leq \\frac{r^2 + L^2 \\cdot \\frac{r^2}{L^2}}{2 \\cdot \\frac{r\\sqrt{T}}{L}} = \\frac{r^2 + r^2}{2 \\cdot \\frac{r\\sqrt{T}}{L}} = \\frac{2r^2}{\\frac{2r\\sqrt{T}}{L}} = \\frac{rL}{\\sqrt{T}}$$\n",
    "ƒêi·ªÅu n√†y cho ch√∫ng ta t·ªëc ƒë·ªô h·ªôi t·ª• r√µ r√†ng $O\\left(\\frac{rL}{\\sqrt{T}}\\right)$ ƒë·ªëi v·ªõi ph∆∞∆°ng ph√°p l·∫•y m·∫´u c√≥ ho√†n l·∫°i.\n",
    "\n",
    "V·ªõi vi·ªác l·∫•y m·∫´u kh√¥ng ho√†n l·∫°i v·ªõi $\\eta = r/(L \\sqrt{T})$: \n",
    "\n",
    "Ch√∫ng ta t√≠nh to√°n h·ªá s·ªë gi·∫£m ph∆∞∆°ng sai:\n",
    "+ V·ªõi $\\frac{n-1}{n}$ ƒë∆∞·ª£c xem l√† ph·∫ßn nhi·ªÖu ·∫£nh h∆∞·ªüng ƒë·∫øn t·ªëc ƒë·ªô h·ªôi t·ª• gi·ªØa c√°c b∆∞·ªõc v√¨ khi l·∫•y m·∫´u kh√¥ng thay th·∫ø, c√°c m·∫´u kh√°c nhau ph·ª• thu·ªôc v√†o nhau (sau khi ch·ªçn m·ªôt ƒëi·ªÉm, kh·∫£ nƒÉng ch·ªçn l·∫°i gi·∫£m c√≤n $n-1$ trong t·ªïng $n$ => c√°c m·∫´u kh√¥ng c√≤n ƒë·ªôc l·∫≠p v·ªõi nhau )\n",
    "\n",
    "$$[E[R(\\bar{x})]] - R^* \\leq \\frac{r^2 + L^2 \\cdot \\frac{n-1}{n} \\cdot \\frac{r^2}{L^2}}{2 \\cdot \\frac{r\\sqrt{T}}{L}}$$ \n",
    "\n",
    "$$= \\frac{r^2 + r^2 \\cdot \\frac{n-1}{n}}{2 \\cdot \\frac{r\\sqrt{T}}{L}}$$\n",
    "\n",
    "$$= \\frac{r^2 \\cdot \\left(1 + \\frac{n-1}{n}\\right)}{2 \\cdot \\frac{r\\sqrt{T}}{L}}$$\n",
    "\n",
    "$$= \\frac{r^2 \\cdot \\frac{2n-1}{n}}{2 \\cdot \\frac{r\\sqrt{T}}{L}}$$\n",
    "\n",
    "$$= \\frac{rL}{\\sqrt{T}} \\cdot \\frac{2n-1}{2n}$$\n",
    "\n",
    "V·ªõi $n$ l·ªõn, $\\frac{2n-1}{2n} \\approx \\frac{2n}{2n} - \\frac{1}{2n} = 1 - \\frac{1}{2n}$, v√¨ v·∫≠y ch√∫ng ta c√≥:\n",
    "\n",
    "$$[E[R(\\bar{x})]] - R^* \\leq \\frac{rL}{\\sqrt{T}} \\cdot \\left(1 - \\frac{1}{2n}\\right)$$\n",
    "\n",
    "V·∫≠y ta c√≥:\n",
    "\n",
    "+ **L·∫•y m·∫´u c√≥ ho√†n l·∫°i**: $[E[R(\\bar{x})]] - R^* \\leq \\frac{rL}{\\sqrt{T}}$\n",
    "\n",
    "+ **L·∫•y m·∫´u kh√¥ng ho√†n l·∫°i**: $[E[R(\\bar{x})]] - R^* \\leq \\frac{rL}{\\sqrt{T}} \\cdot \\left(1 - \\frac{1}{2n}\\right)$\n",
    "\n",
    "=> **T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa ph∆∞∆°ng ph√°p l·∫•y m·∫´u kh√¥ng ho√†n l·∫°i c·∫£i thi·ªán theo h·ªá s·ªë kh√¥ng ƒë·ªïi x·∫•p x·ªâ $\\left(1 - \\frac{1}{2n}\\right)$ so v·ªõi ph∆∞∆°ng ph√°p l·∫•y m·∫´u c√≥ ho√†n l·∫°i**\n",
    "\n",
    "H·ªá s·ªë c·∫£i thi·ªán $(1 - \\frac{1}{2n})$ ph·ª• thu·ªôc v√†o k√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu $n$:\n",
    "- V·ªõi $n$ nh·ªè, m·ª©c c·∫£i thi·ªán ƒë√°ng k·ªÉ h∆°n\n",
    "- Khi $n$ tƒÉng l√™n, m·ª©c c·∫£i thi·ªán ti·ªám c·∫≠n ƒë·∫øn m·ªôt h·ªá s·ªë kh√¥ng ƒë·ªïi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51711f-ebf6-443d-9e46-f21da08a9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def show_trace_2d_custom(x_subsample, results, color, label):\n",
    "    \"\"\"Show the trace of 2D variables during optimization.\n",
    "\n",
    "    Defined in :numref:`subsec_gd-learningrate`\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    d2l.plt.plot(*zip(*results), '-o', color=color, label=label)\n",
    "    x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),\n",
    "                          d2l.arange(-3.0, 1.0, 0.1), indexing='ij')\n",
    "    w = [x1,x2]\n",
    "    d2l.plt.contour(x1, x2, f_ex3(x_subsample, w), colors='#1f77b4')\n",
    "    d2l.plt.xlabel('x1')\n",
    "    d2l.plt.ylabel('x2')\n",
    "    d2l.plt.legend()\n",
    "\n",
    "x_sample = torch.randn(200,2)\n",
    "x_choices = [x_sample[0]]\n",
    "x_subsample=[x_sample[0]]\n",
    "eta = 0.01\n",
    "t = 1\n",
    "\n",
    "# Define gradient of f(x,w)\n",
    "def f_ex3(x,w):\n",
    "    return (x[0] - w[0])**2 + 2*(x[1] - w[1])**2\n",
    "\n",
    "# Define gradient of f(x,w)\n",
    "def f_ex3_grad(x,w):\n",
    "    return -2*(x[0] - w[0]), -4*(x[1] - w[1])\n",
    "\n",
    "\n",
    "def sgd_replacement(x1, x2, s1, s2, f2_grad):\n",
    "    dist_to_optimum.append(d2l.np.linalg.norm([x1, x2]).item())\n",
    "    x= random.choice(x_sample)\n",
    "    x_choices.append(x)\n",
    "    w = torch.tensor([x1,x2])\n",
    "    g1, g2 = f2_grad(x, w)\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "i=0\n",
    "def sgd_without_replacement(x1, x2, s1, s2, f2_grad):\n",
    "    global i \n",
    "    i += 1\n",
    "    x = x_sample[i]\n",
    "    x_subsample.append(x)\n",
    "    dist_to_optimum.append(d2l.np.linalg.norm([x1, x2]).item())\n",
    "    w = torch.tensor([x1,x2])\n",
    "    g1, g2 = f2_grad(x, w)\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "\n",
    "# show_trace_2d_custom(x_subsample[-1], d2l.train_2d(sgd_without_replacement, steps=999, f_grad=f_ex3_grad),\"#0000ff\", \"Without replacement\")\n",
    "# show_trace_2d_custom(x_choices[-1], d2l.train_2d(sgd_replacement, steps=999, f_grad=f_ex3_grad),\"#ff7f0e\",\"With replacement\")\n",
    "dist_to_optimum=[]\n",
    "d2l.train_2d(sgd_without_replacement, steps=199, f_grad=f_ex3_grad)\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('Without replacement: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()\n",
    "\n",
    "dist_to_optimum=[]\n",
    "d2l.train_2d(sgd_replacement, steps=199, f_grad=f_ex3_grad)\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('With replacement: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53da6724-9c69-4023-b80d-b0f0df780b04",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50574902-dfa1-4584-a969-0ed1b94f6c4f",
   "metadata": {},
   "source": [
    "How would you change the stochastic gradient descent solver if some gradient (or rather some coordinate associated with it) was consistently larger than all the other gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb92d31-eb43-4f4a-9e25-2900799e72ab",
   "metadata": {},
   "source": [
    "Trong qu√° tr√¨nh t·ªëi ∆∞u b·∫±ng Stochastic Gradient Descent (SGD), n·∫øu m·ªôt th√†nh ph·∫ßn (t·ªça ƒë·ªô) c·ªßa gradient lu√¥n c√≥ gi√° tr·ªã l·ªõn h∆°n ƒë√°ng k·ªÉ so v·ªõi c√°c th√†nh ph·∫ßn c√≤n l·∫°i, ƒëi·ªÅu n√†y c√≥ th·ªÉ khi·∫øn vi·ªác c·∫≠p nh·∫≠t tham s·ªë b·ªã l·ªách, d·∫´n ƒë·∫øn qu√° tr√¨nh h·ªôi t·ª• tr·ªü n√™n **k√©m hi·ªáu qu·∫£** ho·∫∑c th·∫≠m ch√≠ **kh√¥ng ·ªïn ƒë·ªãnh**.\n",
    "\n",
    "ƒê·ªÉ kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ n√†y, ta c√≥ th·ªÉ √°p d·ª•ng m·ªôt trong hai gi·∫£i ph√°p sau:\n",
    "\n",
    "1. **ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô h·ªçc theo t·ª´ng t·ªça ƒë·ªô**:\n",
    "   Gi·∫£m t·ªëc ƒë·ªô h·ªçc (learning rate) cho c√°c t·ªça ƒë·ªô c√≥ gradient l·ªõn h∆°n nh·∫±m h·∫°n ch·∫ø vi·ªác c·∫≠p nh·∫≠t qu√° m·∫°nh ·ªü c√°c chi·ªÅu ƒë√≥, gi√∫p qu√° tr√¨nh t·ªëi ∆∞u tr·ªü n√™n c√¢n b·∫±ng v√† ·ªïn ƒë·ªãnh h∆°n.\n",
    "1. **Gradient clipping** (c·∫Øt gradient):\n",
    "   Gi·ªõi h·∫°n ƒë·ªô l·ªõn t·ªëi ƒëa c·ªßa gradient, b·∫±ng c√°ch c·∫Øt ng·∫Øn (rescale) vector gradient n·∫øu n√≥ v∆∞·ª£t qu√° m·ªôt ng∆∞·ª°ng cho tr∆∞·ªõc. ƒêi·ªÅu n√†y gi√∫p tr√°nh vi·ªác c√°c gradient c·ª±c l·ªõn g√¢y ra nh·ªØng b∆∞·ªõc nh·∫£y qu√° l·ªõn trong qu√° tr√¨nh c·∫≠p nh·∫≠t tham s·ªë."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f472e-1196-48cc-9c97-40f816648189",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54a65c-0bd3-4557-922b-24d5dc869c37",
   "metadata": {},
   "source": [
    "Assume that $f(x) = x^2 (1 + \\sin x)$. How many local minima does $f$ have? Can you change $f$ in such a way that to minimize it one needs to evaluate all the local minima?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0c079-49d0-4dbd-b109-2abbb1291797",
   "metadata": {},
   "source": [
    "Mi·ªÅn x√°c ƒë·ªãnh c·ªßa $f(x)$ l√† $\\mathbb{R}$.\n",
    "\n",
    "Ta c√≥:\n",
    "$$\n",
    "f'(x) = 2x(1 + \\sin x) + x^2 \\cos x\n",
    "$$\n",
    "$f'(x) = 0$:\n",
    "$$\n",
    "2x(1 + \\sin x) + x^2 \\cos x = 0\n",
    "$$\n",
    "Ta bi·∫øn ƒë·ªïi th√†nh:\n",
    "$$\n",
    "x [2(1 + \\sin x) + x \\cos x] = 0\n",
    "$$\n",
    "\n",
    "V·∫≠y c√°c ƒëi·ªÉm t·ªõi h·∫°n x·∫£y ra khi:\n",
    "- $x = 0$, ho·∫∑c\n",
    "- $2(1 + \\sin x) + x \\cos x = 0$\n",
    "  \n",
    "·ªû ph∆∞∆°ng tr√¨nh th·ª© hai, ta th·∫•y r·∫±ng:\n",
    "- $\\sin x$, $\\cos x$ dao ƒë·ªông gi·ªØa $[-1,1]$\n",
    "- Khi $|x| \\to \\infty$, bi√™n ƒë·ªô c·ªßa $x \\cos x$ tƒÉng l√™n\n",
    "\n",
    "ƒêi·ªÅu n√†y cho th·∫•y ph∆∞∆°ng tr√¨nh $ 2(1 + \\sin x) + x \\cos x = 0 $ c√≥ **v√¥ s·ªë nghi·ªám**, do b·∫£n ch·∫•t dao ƒë·ªông c·ªßa $ \\sin x $, $ \\cos x $.\n",
    "\n",
    "Nhi·ªÅu nghi·ªám trong s·ªë ƒë√≥ l√† c·ª±c ti·ªÉu ƒë·ªãa ph∆∞∆°ng (tr√™n ƒë·ªì th·ªã, c√≥ √≠t nh·∫•t m·ªôt c·ª±c ti·ªÉu trong m·ªói kho·∫£ng $ [2\\pi n, 2\\pi(n+1)] $).\n",
    "\n",
    "V·∫≠y h√†m $ f(x) $ c√≥ **v√¥ s·ªë c·ª±c ti·ªÉu ƒë·ªãa ph∆∞∆°ng**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309cd8a2-96e3-41aa-8e2e-19db846d3cfc",
   "metadata": {},
   "source": [
    "# 04. Minibatch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c5e57-966a-43b7-bd5a-9758081390fe",
   "metadata": {},
   "source": [
    "Trong h·ªçc s√¢u, vi·ªác t·ªëi ∆∞u h√≥a c√°c tham s·ªë c·ªßa m√¥ h√¨nh l√† m·ªôt b∆∞·ªõc quan tr·ªçng ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c hi·ªáu su·∫•t t·ªët nh·∫•t. C√≥ nhi·ªÅu ph∆∞∆°ng ph√°p t·ªëi ∆∞u h√≥a kh√°c nhau, trong ƒë√≥ Gradient Descent (GD) v√† Stochastic Gradient Descent (SGD) l√† hai ph∆∞∆°ng ph√°p ph·ªï bi·∫øn. Tuy nhi√™n, c·∫£ hai ƒë·ªÅu c√≥ nh·ªØng h·∫°n ch·∫ø ri√™ng. Minibatch Stochastic Gradient Descent (Minibatch SGD) ƒë∆∞·ª£c xem l√† m·ªôt gi·∫£i ph√°p c√¢n b·∫±ng gi·ªØa hai ph∆∞∆°ng ph√°p n√†y, mang l·∫°i hi·ªáu qu·∫£ c·∫£ v·ªÅ m·∫∑t t√≠nh to√°n v√† th·ªëng k√™."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5374c-173b-45b0-8528-a4211cefcff8",
   "metadata": {},
   "source": [
    "## Vectorization and Caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71c36e",
   "metadata": {},
   "source": [
    "Tr·ªçng t√¢m c·ªßa quy·∫øt ƒë·ªãnh s·ª≠ d·ª•ng minibatches l√† hi·ªáu qu·∫£ t√≠nh to√°n. ƒêi·ªÅu n√†y d·ªÖ hi·ªÉu nh·∫•t khi xem x√©t song song v·ªõi nhi·ªÅu GPU v√† nhi·ªÅu m√°y ch·ªß. Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta c·∫ßn g·ª≠i √≠t nh·∫•t m·ªôt h√¨nh ·∫£nh cho m·ªói GPU. V·ªõi 8 GPU tr√™n m·ªói m√°y ch·ªß v√† 16 m√°y ch·ªß, ta c√≥ minibatch k√≠ch th∆∞·ªõc 128. \n",
    "\n",
    "V·∫•n ƒë·ªÅ tr·ªü n√™n nh·∫°y c·∫£m h∆°n ƒë·ªëi v·ªõi GPU ƒë∆°n hay ngay c·∫£ CPU ƒë∆°n. C√°c thi·∫øt b·ªã n√†y c√≥ nhi·ªÅu lo·∫°i b·ªô nh·ªõ, th∆∞·ªùng l√† nhi·ªÅu lo·∫°i ƒë∆°n v·ªã t√≠nh to√°n v√† h·∫°n ch·∫ø bƒÉng th√¥ng kh√°c nhau gi·ªØa ch√∫ng. V√≠ d·ª•, CPU c√≥ m·ªôt s·ªë l∆∞·ª£ng nh·ªè c√°c thanh ghi v√† sau ƒë√≥ l√† L1, L2 v√† trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p th·∫≠m ch√≠ b·ªô nh·ªõ cache L3 (ƒë∆∞·ª£c chia s·∫ª gi·ªØa c√°c l√µi b·ªô x·ª≠ l√Ω kh√°c nhau). C√°c b·ªô nh·ªõ ƒë·ªám n√†y c√≥ k√≠ch th∆∞·ªõc v√† ƒë·ªô tr·ªÖ ng√†y c√†ng tƒÉng (ƒë·ªìng th·ªùi ch√∫ng gi·∫£m bƒÉng th√¥ng). N√≥ ƒë·ªß ƒë·ªÉ n√≥i, b·ªô x·ª≠ l√Ω c√≥ kh·∫£ nƒÉng th·ª±c hi·ªán nhi·ªÅu ho·∫°t ƒë·ªông h∆°n so v·ªõi nh·ªØng g√¨ giao di·ªán b·ªô nh·ªõ ch√≠nh c√≥ th·ªÉ cung c·∫•p. \n",
    "\n",
    "* CPU 2GHz v·ªõi 16 l√µi v√† vectorization AVX-512 c√≥ th·ªÉ x·ª≠ l√Ω l√™n ƒë·∫øn $2 \\cdot 10^9 \\cdot 16 \\cdot 32 = 10^{12}$ byte m·ªói gi√¢y. Kh·∫£ nƒÉng c·ªßa GPU d·ªÖ d√†ng v∆∞·ª£t qu√° con s·ªë n√†y theo h·ªá s·ªë 100. M·∫∑t kh√°c, m·ªôt b·ªô x·ª≠ l√Ω m√°y ch·ªß t·∫ßm trung c√≥ th·ªÉ kh√¥ng c√≥ nhi·ªÅu h∆°n 100 Gb/s bƒÉng th√¥ng, t·ª©c l√†, √≠t h∆°n m·ªôt ph·∫ßn m∆∞·ªùi nh·ªØng g√¨ s·∫Ω ƒë∆∞·ª£c y√™u c·∫ßu ƒë·ªÉ gi·ªØ cho b·ªô x·ª≠ l√Ω ƒÉn. V·∫•n ƒë·ªÅ c√≤n t·ªìi t·ªá h∆°n khi ta x√©t ƒë·∫øn vi·ªác kh√¥ng ph·∫£i kh·∫£ nƒÉng truy c·∫≠p b·ªô nh·ªõ n√†o c≈©ng nh∆∞ nhau: ƒë·∫ßu ti√™n, giao di·ªán b·ªô nh·ªõ th∆∞·ªùng r·ªông 64 bit ho·∫∑c r·ªông h∆°n (v√≠ d·ª•, tr√™n GPU l√™n ƒë·∫øn 384 bit), do ƒë√≥ vi·ªác ƒë·ªçc m·ªôt byte duy nh·∫•t v·∫´n s·∫Ω ph·∫£i ch·ªãu chi ph√≠ gi·ªëng nh∆∞ truy c·∫≠p m·ªôt kho·∫£ng b·ªô nh·ªõ r·ªông h∆°n.\n",
    "* T·ªïng chi ph√≠ cho l·∫ßn truy c·∫≠p ƒë·∫ßu ti√™n l√† kh√° l·ªõn, trong khi truy c·∫≠p li√™n ti·∫øp th∆∞·ªùng hao t·ªïn √≠t. C√≥ r·∫•t nhi·ªÅu ƒëi·ªÅu c·∫ßn l∆∞u √Ω, v√≠ d·ª• nh∆∞ l∆∞u tr·ªØ ƒë·ªám khi ta c√≥ nhi·ªÅu ƒëi·ªÉm truy c·∫≠p cu·ªëi, chiplet v√† c√°c c·∫•u tr√∫c kh√°c...\n",
    "\n",
    "C√°ch ƒë·ªÉ gi·∫£m b·ªõt nh·ªØng h·∫°n ch·∫ø n√†y l√† s·ª≠ d·ª•ng m·ªôt h·ªá th·ªëng ph√¢n c·∫•p c·ªßa b·ªô nh·ªõ cache CPU th·ª±c s·ª± ƒë·ªß nhanh ƒë·ªÉ cung c·∫•p cho b·ªô x·ª≠ l√Ω d·ªØ li·ªáu. ƒê√¢y l√† *ƒë·ªông l·ª±c* ƒë·∫±ng sau vi·ªác s·ª≠ d·ª•ng batch trong h·ªçc s√¢u. ƒêƒê·ªÉ ƒë∆°n gi·∫£n, x√©t ph√©p nh√¢n hai ma tr·∫≠n $\\mathbf{A} = \\mathbf{B}\\mathbf{C}$. ƒê·ªÉ t√≠nh $\\mathbf{A}$ ta c√≥ kh√° nhi·ªÅu l·ª±a ch·ªçn, v√≠ d·ª• nh∆∞: \n",
    "\n",
    "1. Ta c√≥ th·ªÉ t√≠nh $\\mathbf{A}_{ij} = \\mathbf{B}_{i,:} \\mathbf{C}_{:,j}^\\top$, t·ª©c l√† t√≠nh t·ª´ng ph·∫ßn t·ª≠ b·∫±ng t√≠ch v√¥ h∆∞·ªõng.\n",
    "1. Ta c√≥ th·ªÉ t√≠nh $\\mathbf{A}_{:,j} = \\mathbf{B} \\mathbf{C}_{:,j}^\\top$, tt·ª©c l√† t√≠nh theo t·ª´ng c·ªôt. T∆∞∆°ng t·ª±, ta c√≥ th·ªÉ t√≠nh $\\mathbf{A}$ theo t·ª´ng h√†ng $\\mathbf{A}_{i,:}$.\n",
    "1. Ta ƒë∆°n gi·∫£n c√≥ th·ªÉ t√≠nh $\\mathbf{A} = \\mathbf{B} \\mathbf{C}$.\n",
    "1. Ta c√≥ th·ªÉ chia $\\mathbf{B}$ v√† $\\mathbf{C}$ th√†nh c√°c ma tr·∫≠n kh·ªëi nh·ªè h∆°n v√† t√≠nh to√°n $\\mathbf{A}$ theo t·ª´ng kh·ªëi m·ªôt.\n",
    "\n",
    "N·∫øu s·ª≠ d·ª•ng c√°ch ƒë·∫ßu ti√™n, ta c·∫ßn sao ch√©p m·ªôt vector c·ªôt v√† m·ªôt vector h√†ng v√†o CPU cho m·ªói l·∫ßn t√≠nh ph·∫ßn t·ª≠ $\\mathbf{A}_{ij}$. T·ªá h∆°n n·ªØa, do c√°c ph·∫ßn t·ª≠ c·ªßa ma tr·∫≠n ƒë∆∞·ª£c l∆∞u th√†nh m·ªôt d√£y li√™n t·ª•c d∆∞·ªõi b·ªô nh·ªõ, ta bu·ªôc ph·∫£i truy c·∫≠p nhi·ªÅu v√πng nh·ªõ r·ªùi r·∫°c khi ƒë·ªçc m·ªôt trong hai vector t·ª´ b·ªô nh·ªõ. C√°ch th·ª© hai t·ªët h∆°n nhi·ªÅu. Theo c√°ch n√†y, ta c√≥ th·ªÉ gi·ªØ vector c·ªôt $\\mathbf{C}_{:,j}$ trong v√πng nh·ªõ ƒë·ªám c·ªßa CPU trong khi ta ti·∫øp t·ª•c qu√©t qua $\\mathbf{B}$. C√°ch n√†y ch·ªâ c·∫ßn n·ª≠a bƒÉng th√¥ng c·∫ßn thi·∫øt c·ªßa b·ªô nh·ªõ, do ƒë√≥ truy c·∫≠p nhanh h∆°n. ƒê∆∞∆°ng nhi√™n c√°ch th·ª© ba l√† t·ªët nh·∫•t. ƒê√°ng ti·∫øc r·∫±ng ƒëa s·ªë ma tr·∫≠n qu√° l·ªõn ƒë·ªÉ c√≥ th·ªÉ ƒë∆∞a v√†o v√πng nh·ªõ ƒë·ªám (d√π sao ƒë√¢y c≈©ng ch√≠nh l√† ƒëi·ªÅu ta ƒëang th·∫£o lu·∫≠n). C√°ch th·ª© t∆∞ l√† m·ªôt ph∆∞∆°ng ph√°p thay th·∫ø kh√° t·ªët: ƒë∆∞a c√°c kh·ªëi c·ªßa ma tr·∫≠n v√†o v√πng nh·ªõ ƒë·ªám v√† th·ª±c hi·ªán ph√©p nh√¢n c·ª•c b·ªô. C√°c th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u s·∫Ω th·ª±c hi·ªán vi·ªác n√†y gi√∫p ch√∫ng ta. H√£y xem x√©t hi·ªáu su·∫•t c·ªßa t·ª´ng ph∆∞∆°ng ph√°p trong th·ª±c t·∫ø. \n",
    "\n",
    "Ngo√†i hi·ªáu su·∫•t t√≠nh to√°n, chi ph√≠ t√≠nh to√°n ph√°t sinh ƒë·∫øn t·ª´ Python v√† framework h·ªçc s√¢u c≈©ng ƒë√°ng c√¢n nh·∫Øc. M·ªói l·∫ßn ta th·ª±c hi·ªán m·ªôt c√¢u l·ªánh, b·ªô th√¥ng d·ªãch Python g·ª≠i m·ªôt c√¢u l·ªánh ƒë·∫øn MXNet ƒë·ªÉ ch√®n c√¢u l·ªánh ƒë√≥ v√†o ƒë·ªì th·ªã t√≠nh to√°n v√† th·ª±c thi n√≥ theo ƒë√∫ng l·ªãnh tr√¨nh. Chi ph√≠ ƒë√≥ c√≥ th·ªÉ kh√° b·∫•t l·ª£i. N√≥i ng·∫Øn g·ªçn, n√™n √°p d·ª•ng vector h√≥a (v√† ma tr·∫≠n) b·∫•t c·ª© khi n√†o c√≥ th·ªÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f54e8-d696-42e4-bab6-a8b5aa1a32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import d2l\n",
    "\n",
    "A = torch.zeros(512, 512)\n",
    "B = torch.randn(512, 512)\n",
    "C = torch.randn(512, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3788f",
   "metadata": {},
   "source": [
    "V√¨ ch√∫ng ta s·∫Ω th∆∞·ªùng xuy√™n ƒëo th·ªùi gian ch·∫°y trong ph·∫ßn c√≤n l·∫°i c·ªßa b√°o c√°o, h√£y ƒë·ªãnh nghƒ©a m·ªôt b·ªô ƒë·∫øm th·ªùi gian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecd54f",
   "metadata": {},
   "source": [
    "G√°n t·ª´ng ph·∫ßn t·ª≠ ƒë∆°n gi·∫£n l√† l·∫∑p qua t·∫•t c·∫£ c√°c h√†ng v√† c·ªôt c·ªßa $\\mathbf{B}$ v√† $\\mathbf{C}$ t∆∞∆°ng ·ª©ng ƒë·ªÉ g√°n gi√° tr·ªã cho $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c00df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute A = BC one element at a time\n",
    "timer.start()\n",
    "for i in range(512):\n",
    "    for j in range(512):\n",
    "        A[i, j] = torch.dot(B[i, :], C[:, j])\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85adbf56",
   "metadata": {},
   "source": [
    "M·ªôt chi·∫øn l∆∞·ª£c nhanh h∆°n l√† th·ª±c hi·ªán g√°n theo c·ªôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute A = BC one column at a time\n",
    "timer.start()\n",
    "for j in range(512):\n",
    "    A[:, j] = torch.mv(B, C[:, j])\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c9312",
   "metadata": {},
   "source": [
    "Cu·ªëi c√πng, ph∆∞∆°ng ph√°p hi·ªáu qu·∫£ nh·∫•t l√† th·ª±c hi·ªán to√†n b·ªô ph√©p to√°n trong m·ªôt kh·ªëi duy nh·∫•t.\n",
    "L∆∞u √Ω r·∫±ng vi·ªác nh√¢n hai ma tr·∫≠n $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ v√† $\\mathbf{C} \\in \\mathbb{R}^{n \\times p}$ c·∫ßn kho·∫£ng $2mnp$ ph√©p to√°n d·∫•u ph·∫©y ƒë·ªông,\n",
    "khi ph√©p nh√¢n v√† c·ªông v√¥ h∆∞·ªõng ƒë∆∞·ª£c t√≠nh l√† hai ph√©p to√°n ri√™ng bi·ªát (m·∫∑c d√π tr√™n th·ª±c t·∫ø th∆∞·ªùng ƒë∆∞·ª£c g·ªôp l·∫°i).\n",
    "Do ƒë√≥, vi·ªác nh√¢n hai ma tr·∫≠n k√≠ch th∆∞·ªõc $512 \\times 512$ c·∫ßn kho·∫£ng $0.27$ t·ª∑ ph√©p to√°n d·∫•u ph·∫©y ƒë·ªông.\n",
    "B√¢y gi·ªù, h√£y c√πng xem t·ªëc ƒë·ªô t∆∞∆°ng ·ª©ng c·ªßa c√°c ph√©p to√°n n√†y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f66219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute A = BC in one go\n",
    "timer.start()\n",
    "A = torch.mm(B, C)\n",
    "timer.stop()\n",
    "\n",
    "gigaflops = [0.27 / i for i in timer.times]\n",
    "print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n",
    "      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3843f26-b110-4f3b-a324-32eb903d710a",
   "metadata": {},
   "source": [
    "## Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8f643",
   "metadata": {},
   "source": [
    "·ªû c√°c ph·∫ßn tr∆∞·ªõc ta ƒë·ªçc d·ªØ li·ªáu theo *minibatches* thay v√¨ t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu ƒë∆°n l·∫ª ƒë·ªÉ c·∫≠p nh·∫≠t c√°c tham s·ªë. Ta c√≥ th·ªÉ gi·∫£i th√≠ch ng·∫Øn g·ªçn m·ª•c ƒë√≠ch nh∆∞ sau. X·ª≠ l√Ω t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu ƒë∆°n l·∫ª ƒë√≤i h·ªèi ph·∫£i th·ª±c hi·ªán r·∫•t nhi·ªÅu ph√©p nh√¢n ma tr·∫≠n v·ªõi vector (hay th·∫≠m ch√≠ vector v·ªõi vector). C√°ch n√†y kh√° t·ªën k√©m v√† ƒë·ªìng th·ªùi ph·∫£i ch·ªãu th√™m chi ph√≠ kh√° l·ªõn ƒë·∫øn t·ª´ c√°c framework h·ªçc s√¢u b√™n d∆∞·ªõi. V·∫•n ƒë·ªÅ n√†y x·∫£y ra ·ªü c·∫£ l√∫c ƒë√°nh gi√° m·ªôt m·∫°ng v·ªõi d·ªØ li·ªáu m·ªõi v√† khi t√≠nh to√°n gradient ƒë·ªÉ c·∫≠p nh·∫≠t c√°c tham s·ªë. T·ª©c l√† v·∫•n ƒë·ªÅ x·∫£y ra m·ªói khi ta th·ª±c hi·ªán $\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g}_t$  trong ƒë√≥ \n",
    "\n",
    "$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} f(\\mathbf{x}_{t}, \\mathbf{w})$$\n",
    "\n",
    "Ta c√≥ th·ªÉ tƒÉng hi·ªáu su·∫•t *t√≠nh to√°n* c·ªßa ph√©p t√≠nh n√†y b·∫±ng c√°ch √°p d·ª•ng n√≥ tr√™n m·ªói minibatch d·ªØ li·ªáu. T·ª©c l√† ta thay th·∫ø gradient $\\mathbf{g}_t$ tr√™n m·ªôt ƒëi·ªÉm d·ªØ li·ªáu ƒë∆°n l·∫ª b·∫±ng gradient tr√™n m·ªôt batch nh·ªè. \n",
    "\n",
    "$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w})$$\n",
    "\n",
    "H√£y th·ª≠ xem ph∆∞∆°ng ph√°p tr√™n t√°c ƒë·ªông th·∫ø n√†o ƒë·∫øn c√°c t√≠nh ch·∫•t th·ªëng k√™ c·ªßa $\\mathbf{g}_t$: v√¨ c·∫£ $\\mathbf{x}_t$ v√† t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ trong minibatch $\\mathcal{B}_t$ ƒë∆∞·ª£c l·∫•y ra t·ª´ t·∫≠p hu·∫•n luy·ªán v·ªõi x√°c su·∫•t nh∆∞ nhau, k·ª≥ v·ªçng c·ªßa gradient l√† kh√¥ng ƒë·ªïi. M·∫∑t kh√°c, ph∆∞∆°ng sai gi·∫£m m·ªôt c√°ch ƒë√°ng k·ªÉ. Do gradient c·ªßa minibatch l√† trung b√¨nh c·ªßa $b := |\\mathcal{B}_t|$ gradient ƒë·ªôc l·∫≠p, ƒë·ªô l·ªách chu·∫©n c·ªßa n√≥ gi·∫£m ƒëi theo h·ªá s·ªë $b^{-\\frac{1}{2}}$. ƒê√¢y l√† m·ªôt ƒëi·ªÅu t·ªët, c√°ch c·∫≠p nh·∫≠t n√†y c√≥ ƒë·ªô tin c·∫≠y g·∫ßn b·∫±ng vi·ªác l·∫•y gradient tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu.\n",
    "\n",
    "T·ª´ √Ω tr√™n, ta s·∫Ω nhanh ch√≥ng cho r·∫±ng ch·ªçn minibatch $\\mathcal{B}_t$ l·ªõn lu√¥n l√† t·ªët nh·∫•t. Ti·∫øc r·∫±ng ƒë·∫øn m·ªôt m·ª©c ƒë·ªô n√†o ƒë√≥, ƒë·ªô l·ªách chu·∫©n s·∫Ω gi·∫£m kh√¥ng ƒë√°ng k·ªÉ so v·ªõi chi ph√≠ t√≠nh to√°n tƒÉng tuy·∫øn t√≠nh. Do ƒë√≥ trong th·ª±c t·∫ø, ta s·∫Ω ch·ªçn k√≠ch th∆∞·ªõc minibatch ƒë·ªß l·ªõn ƒë·ªÉ hi·ªáu su·∫•t t√≠nh to√°n cao trong khi v·∫´n ƒë·ªß ƒë·ªÉ ƒë∆∞a v√†o b·ªô nh·ªõ c·ªßa GPU. ƒê·ªÉ minh ho·∫° qu√° tr√¨nh l∆∞u tr·ªØ n√†y, h√£y xem ƒëo·∫°n m√£ ngu·ªìn d∆∞·ªõi ƒë√¢y. Trong ƒë√≥ ta v·∫´n th·ª±c hi·ªán ph√©p nh√¢n ma tr·∫≠n v·ªõi ma tr·∫≠n, tuy nhi√™n l·∫ßn n√†y ta t√°ch th√†nh t·ª´ng minibatch 64 c·ªôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51db8c-7f3b-4ca1-8a10-419d99dfe483",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start()\n",
    "for j in range(0, 512, 64):\n",
    "    A[:, j:j+64] = torch.mm(B, C[:, j:j+64])\n",
    "timer.stop()\n",
    "print(f'performance in Gigaflops: block {0.27 / timer.times[3]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ad56f",
   "metadata": {},
   "source": [
    "C√≥ th·ªÉ th·∫•y qu√° tr√¨nh t√≠nh to√°n tr√™n minibatch v·ªÅ c∆° b·∫£n c√≥ hi·ªáu su·∫•t g·∫ßn b·∫±ng th·ª±c hi·ªán tr√™n to√†n ma tr·∫≠n. Tuy nhi√™n, c·∫ßn l∆∞u √Ω r·∫±ng Trong v√≠ d·ª• tr∆∞·ªõc ƒë√≥ ta s·ª≠ d·ª•ng m·ªôt lo·∫°i ƒëi·ªÅu chu·∫©n ph·ª• thu·ªôc ch·∫∑t ch·∫Ω v√†o ph∆∞∆°ng sai c·ªßa minibatch. khi tƒÉng k√≠ch th∆∞·ªõc minibatch, ph∆∞∆°ng sai gi·∫£m xu·ªëng v√† c√πng v·ªõi ƒë√≥ l√† l·ª£i √≠ch c·ªßa vi·ªác th√™m nhi·ªÖu (noise-injection) c≈©ng gi·∫£m theo do ph∆∞∆°ng ph√°p chu·∫©n h√≥a theo batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76883d6-a469-4153-91d7-931be6f011fd",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba4801",
   "metadata": {},
   "source": [
    "Ch√∫ng ta h√£y xem c√°ch minibatches ƒë∆∞·ª£c t·∫°o ra hi·ªáu qu·∫£ t·ª´ d·ªØ li·ªáu. Sau ƒë√¢y ch√∫ng t√¥i s·ª≠ d·ª•ng m·ªôt t·∫≠p d·ªØ li·ªáu do NASA ph√°t tri·ªÉn ƒë·ªÉ ki·ªÉm tra m·ª©c ƒë·ªô ti·∫øng ·ªìn do c√°nh m√°y bay t·∫°o ra trong ƒëi·ªÅu ki·ªán kh√≠ ƒë·ªông h·ªçc c·ª• th·ªÉ [noise from different aircraft](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise) ƒë·ªÉ so s√°nh c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a n√†y. ƒê·ªÉ thu·∫≠n ti·ªán, ch√∫ng t√¥i ch·ªâ s·ª≠ d·ª•ng c√°c v√≠ d·ª• $1,500$ ƒë·∫ßu ti√™n. D·ªØ li·ªáu ƒë∆∞·ª£c l√†m tr·∫Øng ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªõc, t·ª©c l√†, ch√∫ng t√¥i lo·∫°i b·ªè trung b√¨nh v√† gi·∫£i th√≠ch ph∆∞∆°ng sai th√†nh $1$ cho m·ªói t·ªça ƒë·ªô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d525e-e0e8-4d44-96de-5be729da4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n",
    "                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
    "\n",
    "#@save\n",
    "def get_data_ch11(batch_size=10, n=1500):\n",
    "    data = np.genfromtxt(d2l.download('airfoil'),\n",
    "                         dtype=np.float32, delimiter='\\t')\n",
    "    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\n",
    "    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n",
    "                               batch_size, is_train=True)\n",
    "    return data_iter, data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ac24e8-5c1d-4ecb-bd87-a888caabd119",
   "metadata": {},
   "source": [
    "## Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7debfd6d",
   "metadata": {},
   "source": [
    "Ta s·∫Ω s·ª≠ d·ª•ng m√¥ h√¨nh H·ªìi quy Tuy·∫øn t√≠nh v·ªõi l·∫ßn l∆∞·ª£t c√°c thu·∫≠t to√°n t·ªëi ∆∞u GD, SGD v√† SGD theo minibatch ƒë·ªÉ so s√°nh ƒë·ªô hi·ªáu qu·∫£."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039bbab",
   "metadata": {},
   "source": [
    "### Nh·∫Øc l·∫°i v·ªÅ H·ªìi quy Tuy·∫øn t√≠nh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ac641",
   "metadata": {},
   "source": [
    "#### ƒê·ªãnh nghƒ©a\n",
    "H·ªìi quy Tuy·∫øn t√≠nh (Linear Regression) l√† m·ªôt ph∆∞∆°ng ph√°p h·ªçc m√°y c∆° b·∫£n thu·ªôc nh√≥m h·ªçc c√≥ gi√°m s√°t (Supervised learning). Ph∆∞∆°ng ph√°p n√†y bao g·ªìm 2 y·∫øu t·ªë l√† H·ªìi quy v√† Tuy·∫øn t√≠nh. Ph√©p H·ªìi quy ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa m·ªôt ho·∫∑c nhi·ªÅu bi·∫øn ƒë·∫ßu v√†o (bi·∫øn ƒë·ªôc l·∫≠p) v√† m·ªôt bi·∫øn ƒë·∫ßu ra (bi·∫øn ph·ª• thu·ªôc). Trong khi ƒë√≥, ph√©p Tuy·∫øn t√≠nh ch·ªâ m·ªëi quan h·ªá t·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa c√°c bi·∫øn ƒë·∫ßu v√†o m√† kh√¥ng c√≥ s·ª± xu·∫•t hi·ªán c·ªßa c√°c h√†m l≈©y th·ª´a hay phi tuy·∫øn nh∆∞ sin, log, relu, ... Nh∆∞ v·∫≠y, H·ªìi quy Tuy·∫øn t√≠nh l√† ph∆∞∆°ng ph√°p h·ªçc m√°y v√† th·ªëng k√™ gi√∫p m√¥ h√¨nh h√≥a m·ªëi quan h·ªá **tuy·∫øn t√≠nh** gi·ªØa m·ªôt ho·∫∑c nhi·ªÅu bi·∫øn ƒë·∫ßu v√†o (bi·∫øn ƒë·ªôc l·∫≠p) v√† m·ªôt bi·∫øn ƒë·∫ßu ra (bi·∫øn ph·ª• thu·ªôc), t·ª´ ƒë√≥ gi√∫p d·ª± ƒëo√°n gi√° tr·ªã c·ªßa bi·∫øn ph·ª• thu·ªôc d·ª±a tr√™n gi√° tr·ªã c·ªßa c√°c bi·∫øn ƒë·ªôc l·∫≠p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd40a3",
   "metadata": {},
   "source": [
    "#### M√¥ h√¨nh Tuy·∫øn t√≠nh\n",
    "M√¥ h√¨nh H·ªìi quy Tuy·∫øn t√≠nh cho `d` ƒë·∫∑c tr∆∞ng (bi·∫øn ƒë·∫ßu v√†o) c√≥ d·∫°ng:\n",
    "$$\n",
    "y = \\omega_1*x_1 + \\omega_2*x_2 + ... + \\omega_d*x_d + b\n",
    "$$\n",
    "Thu th·∫≠p to√†n b·ªô c√°c ƒë·∫∑c tr∆∞ng v√†o m·ªôt vector $\\mathbf{x}$ v√† to√†n b·ªô c√°c tr·ªçng s·ªë v√†o m·ªôt vector $\\mathbf{w}$, ta c√≥ th·ªÉ bi·ªÉu di·ªÖn m√¥ h√¨nh d∆∞·ªõi d·∫°ng t√≠ch v√¥ h∆∞·ªõng c·ªßa 2 vector:\n",
    "$$\n",
    "y = \\mathbf{w}^T*\\mathbf{x}\n",
    "$$\n",
    "Trong ƒë√≥:\n",
    "- $\\mathbf{x}$: vector ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o, $\\mathbf{x} \\in R^d$ (input features)\n",
    "- $\\mathbf{w}$: vector tr·ªçng s·ªë (weights) c·∫ßn hu·∫•n luy·ªán\n",
    "- $b$: ƒë·ªô l·ªách (bias)\n",
    "- $y$: gi√° tr·ªã ƒë·∫ßu ra (output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ed6f3",
   "metadata": {},
   "source": [
    "#### H√†m m·∫•t m√°t\n",
    "ƒê·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô kh·ªõp gi·ªØa m√¥ h√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng v√† d·ªØ li·ªáu, ta s·ª≠ d·ª•ng h√†m m·∫•t m√°t. H√†m m·∫•t m√°t ƒë·ªãnh l∆∞·ª£ng kho·∫£ng c√°ch gi·ªØa gi√° tr·ªã th·ª±c $y$ v√† gi√° tr·ªã d·ª± ƒëo√°n $\\hat{y}$ c·ªßa m·ª•c ti√™u. ƒê·ªô m·∫•t m√°t th∆∞·ªùng l√† m·ªôt s·ªë kh√¥ng √¢m v√† c√≥ gi√° tr·ªã c√†ng nh·ªè c√†ng t·ªët. Khi c√°c d·ª± ƒëo√°n ho√†n h·∫£o, ch√∫ng s·∫Ω c√≥ ƒë·ªô m·∫•t m√°t s·∫Ω b·∫±ng **0**. H√†m m·∫•t m√°t th√¥ng d·ª•ng nh·∫•t trong c√°c b√†i to√°n h·ªìi quy l√† h√†m t·ªïng b√¨nh ph∆∞∆°ng c√°c l·ªói - Mean Squared Error (MSE):\n",
    "$$\n",
    "L_i = \\frac{1}{2} MSE(y_i, \\hat{y}_i) = \\frac{1}{2} E[(y_i - \\hat{y}_i)^2] = \\frac{1}{2} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "H·∫±ng s·ªë **1/2** kh√¥ng t·∫°o ra s·ª± kh√°c bi·ªát th·ª±c s·ª± n√†o nh∆∞ng s·∫Ω gi√∫p thu·∫≠n ti·ªán h∆°n v·ªÅ m·∫∑t k√Ω hi·ªáu: n√≥ s·∫Ω ƒë∆∞·ª£c tri·ªát ti√™u khi l·∫•y ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t.\n",
    "\n",
    "L∆∞u √Ω r·∫±ng khi hi·ªáu gi·ªØa gi√° tr·ªã th·ª±c $y_i$ v√† gi√° tr·ªã ∆∞·ªõc l∆∞·ª£ng $\\hat{y}_i$ l·ªõn, gi√° tr·ªã h√†m m·∫•t m√°t s·∫Ω tƒÉng r·∫•t l·ªõn cho s·ª± ph·ª• thu·ªôc b·∫≠c 2. ƒê·ªÉ ƒëo ch·∫•t l∆∞·ª£ng c·ªßa m√¥ h√¨nh tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu, ta ƒë∆°n thu·∫ßn l·∫•y trung b√¨nh (hay t∆∞∆°ng ƒë∆∞∆°ng l√† l·∫•y t·ªïng) c√°c gi√° tr·ªã m·∫•t m√°t c·ªßa t·ª´ng m·∫´u trong t·∫≠p hu·∫•n luy·ªán.\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum_{i=1}^{n} L_i = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} (y_i - \\hat{y}_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494f2ea",
   "metadata": {},
   "source": [
    "#### M·ª•c ti√™u\n",
    "Khi hu·∫•n luy·ªán m√¥ h√¨nh, ta mu·ªën t√¨m c√°c tham s·ªë $\\mathbf{w}^*$ v√† $b^*$ sao cho t·ªïng ƒë·ªô m·∫•t m√°t tr√™n to√†n b·ªô c√°c m·∫´u hu·∫•n luy·ªán ƒë∆∞·ª£c c·ª±c ti·ªÉu h√≥a:\n",
    "$$\n",
    "\\mathbf{w}^*, b^* = \\underset{\\mathbf{w}, b}{\\text{argmin}}\\:L(\\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8e56b",
   "metadata": {},
   "source": [
    "K·ªπ thu·∫≠t ch√≠nh ƒë·ªÉ t·ªëi ∆∞u h√≥a m√¥ h√¨nh n√†y, c≈©ng nh∆∞ c√°c m√¥ h√¨nh h·ªçc s√¢u kh√°c, bao g·ªìm vi·ªác gi·∫£m thi·ªÉu l·ªói qua c√°c v√≤ng l·∫∑p b·∫±ng c√°ch c·∫≠p nh·∫≠t tham s·ªë theo h∆∞·ªõng l√†m gi·∫£m g·∫ßn h√†m m√°t m√°t. V·ªõi c√°c h√†m m·∫•t m√°t m·∫∑t l·ªìi, gi√° tr·ªã m·∫•t m√°t cu·ªëi c√πng s·∫Ω h·ªôi t·ª• v·ªÅ gi√° tr·ªã nh·ªè nh·∫•t. Tuy ƒëi·ªÅu t∆∞∆°ng t·ª± kh√¥ng th·ªÉ √°p d·ª•ng cho c√°c m·∫∑t kh√¥ng l·ªìi, √≠t nh·∫•t thu·∫≠t to√°n s·∫Ω d·∫´n t·ªõi m·ªôt c·ª±c ti·ªÉu (hy v·ªçng l√† t·ªët)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88f397",
   "metadata": {},
   "source": [
    "ƒê∆°n gi·∫£n nh·∫•t ta c√≥ th·ªÉ k·ªÉ ƒë·∫øn l√† vi·ªác t√≠nh ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t, t·ª©c trung b√¨nh c·ªßa c√°c gi√° tr·ªã m·∫•t m√°t ƒë∆∞·ª£c t√≠nh tr√™n m·ªói m·∫´u c·ªßa t·∫≠p d·ªØ li·ªáu. Cu·ªëi c√πng, gradient n√†y ƒë∆∞·ª£c nh√¢n v·ªõi t·ªëc ƒë·ªô h·ªçc $\\eta > 0$, l·∫•y trung b√¨nh tr√™n k√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu v√† k·∫øt qu·∫£ n√†y ƒë∆∞·ª£c tr·ª´ ƒëi t·ª´ c√°c gi√° tr·ªã tham s·ªë hi·ªán t·∫°i. ƒê√¢y l√† ch√≠nh ph∆∞∆°ng ph√°p Gradient Descent (GD). Tr√™n th·ª±c t·∫ø, ƒëi·ªÅu n√†y c√≥ th·ªÉ c·ª±c k·ª≥ ch·∫≠m: ch√∫ng ta ph·∫£i duy·ªát qua to√†n b·ªô t·∫≠p d·ªØ li·ªáu tr∆∞·ªõc khi th·ª±c hi·ªán m·ªôt l·∫ßn c·∫≠p nh·∫≠t duy nh·∫•t, ngay c·∫£ khi c√≥ b∆∞·ªõc c·∫≠p nh·∫≠t c√≥ th·ªÉ r·∫•t m·∫°nh m·∫Ω. T·ªá h∆°n n·ªØa, n·∫øu c√≥ nhi·ªÅu d·ªØ li·ªáu tr√πng l·∫∑p trong t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán, l·ª£i √≠ch c·ªßa vi·ªác c·∫≠p nh·∫≠t to√†n b·ªô s·∫Ω b·ªã h·∫°n ch·∫ø.\n",
    "\n",
    "Vi·ªác c·∫≠p nh·∫≠t c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng c√¥ng th·ª©c d∆∞·ªõi ƒë√¢y:\n",
    "$$\n",
    "(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\frac{\\eta}{|\\boldsymbol{N}|} \\sum_{i \\in \\boldsymbol{N}} \\partial_{(\\mathbf{w}, b)}L_i(\\mathbf{w}, b)\n",
    "$$\n",
    "hay,\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\boldsymbol{N}|} \\sum_{i \\in \\boldsymbol{N}} \\partial_\\mathbf{w} L_i(\\mathbf{w}, b)\n",
    "$$\n",
    "$$\n",
    "b \\leftarrow b - \\frac{\\eta}{|\\boldsymbol{N}|} \\sum_{i \\in \\boldsymbol{N}} \\partial_{b} L_i(\\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a521b87",
   "metadata": {},
   "source": [
    "M·ªôt c√°ch kh√°c ho√†n to√†n l√† ch·ªâ xem x√©t m·ªôt m·∫´u d·ªØ li·ªáu duy nh·∫•t t·∫°i m·ªôt th·ªùi ƒëi·ªÉm v√† th·ª±c hi·ªán c√°c b∆∞·ªõc c·∫≠p nh·∫≠t d·ª±a tr√™n t·ª´ng quan s√°t t·∫°i m·ªôt th·ªùi ƒëi·ªÉm. ƒê√¢y l√† ch√≠nh ph∆∞∆°ng ph√°p Stochastic Gradient Descent (SGD). C√≥ th·ªÉ coi ƒë√¢y l√† m·ªôt chi·∫øn l∆∞·ª£c hi·ªáu qu·∫£, ngay c·∫£ ƒë·ªëi v·ªõi c√°c t·∫≠p d·ªØ li·ªáu l·ªõn. Tuy nhi√™n, SGD c√≥ nh·ªØng nh∆∞·ª£c ƒëi·ªÉm, c·∫£ v·ªÅ m·∫∑t t√≠nh to√°n v√† th·ªëng k√™. M·ªôt v·∫•n ƒë·ªÅ ph√°t sinh t·ª´ th·ª±c t·∫ø l√† b·ªô x·ª≠ l√Ω nh√¢n v√† c·ªông s·ªë nhanh h∆°n nhi·ªÅu so v·ªõi vi·ªác di chuy·ªÉn d·ªØ li·ªáu t·ª´ b·ªô nh·ªõ ch√≠nh ƒë·∫øn b·ªô ƒë·ªám b·ªô x·ª≠ l√Ω. Th·ª±c hi·ªán ph√©p nh√¢n ma tr·∫≠n-vect∆° hi·ªáu qu·∫£ h∆°n t·ªõi m·ªôt c·∫•p ƒë·ªô so v·ªõi s·ªë l∆∞·ª£ng ph√©p to√°n vecto-vect∆° t∆∞∆°ng ·ª©ng. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian h∆°n ƒë·ªÉ x·ª≠ l√Ω m·ªôt m·∫´u t·∫°i m·ªôt th·ªùi ƒëi·ªÉm so v·ªõi to√†n b·ªô m·∫´u. M·ªôt v·∫•n ƒë·ªÅ th·ª© hai l√† m·ªôt s·ªë l·ªõp, ch·∫≥ng h·∫°n nh∆∞ batch normalization y√™u c·∫ßu c√≥ nhi·ªÅu h∆°n m·ªôt m·∫´u d·ªØ li·ªáu t·∫°i m·ªôt th·ªùi ƒëi·ªÉm.\n",
    "\n",
    "Vi·ªác c·∫≠p nh·∫≠t c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng c√¥ng th·ª©c d∆∞·ªõi ƒë√¢y:\n",
    "$$\n",
    "(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta*\\partial_{(\\mathbf{w}, b)}L(\\mathbf{w}, b)\n",
    "$$\n",
    "hay,\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta*\\partial_\\mathbf{w} L(\\mathbf{w}, b)\n",
    "$$\n",
    "$$\n",
    "b \\leftarrow b - \\eta*\\partial_{b} L(\\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930b5c7",
   "metadata": {},
   "source": [
    "Gi·∫£i ph√°p cho c·∫£ hai v·∫•n ƒë·ªÅ l√† ch·ªçn m·ªôt chi·∫øn l∆∞·ª£c trung gian: thay v√¨ l·∫•y to√†n b·ªô m·∫´u d·ªØ li·ªáu ho·∫∑c ch·ªâ m·ªôt t·∫°i m·ªôt th·ªùi ƒëi·ªÉm, ch√∫ng ta l·∫•y m·ªôt s·ªë l∆∞·ª£ng nh·ªè. L·ª±a ch·ªçn c·ª• th·ªÉ v·ªÅ k√≠ch th∆∞·ªõc c·ªßa s·ªë l∆∞·ª£ng n√≥i tr√™n ph·ª• thu·ªôc v√†o nhi·ªÅu y·∫øu t·ªë, ch·∫≥ng h·∫°n nh∆∞ l∆∞·ª£ng b·ªô nh·ªõ, s·ªë l∆∞·ª£ng b·ªô tƒÉng t·ªëc, l·ª±a ch·ªçn l·ªõp v√† t·ªïng k√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu. M·∫∑c d√π v·∫≠y, m·ªôt s·ªë t·ª´ 32 ƒë·∫øn 256, t·ªët nh·∫•t l√† b·ªôi s·ªë c·ªßa m·ªôt l≈©y th·ª´a l·ªõn c·ªßa 2 l√† m·ªôt kh·ªüi ƒë·∫ßu t·ªët. ƒê√¢y ch√≠nh l√† ph∆∞∆°ng ph√°p bi·∫øn th·ªÉ Stochastic Gradient Descent theo minibatch (Minibatch SGD). ·ªû v√≤ng l·∫∑p `t`, ta l·∫•y ng·∫´u nhi√™n m·ªôt s·ªë m·∫´u g·ªçi l√† $B_t$ sao cho k√≠ch th∆∞·ªõc l√† |$B$|. Sau ƒë√≥, ch√∫ng ta t√≠nh ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t tr√™n minibatch ƒë√≥ theo c√°c tham s·ªë c·ªßa m√¥ h√¨nh. Cu·ªëi c√πng, gradient n√†y ƒë∆∞·ª£c nh√¢n v·ªõi t·ªëc ƒë·ªô h·ªçc  $\\eta > 0$, l·∫•y trung b√¨nh tr√™n k√≠ch th∆∞·ªõc minibatch v√† k·∫øt qu·∫£ n√†y ƒë∆∞·ª£c tr·ª´ ƒëi t·ª´ c√°c gi√° tr·ªã tham s·ªë hi·ªán t·∫°i.\n",
    "\n",
    "Vi·ªác c·∫≠p nh·∫≠t c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng c√¥ng th·ª©c d∆∞·ªõi ƒë√¢y:\n",
    "$$\n",
    "(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\frac{\\eta}{|\\boldsymbol{B}|} \\sum_{i \\in \\boldsymbol{B_t}} \\partial_{(\\mathbf{w}, b)}L_i(\\mathbf{w}, b)\n",
    "$$\n",
    "hay,\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\boldsymbol{B}|} \\sum_{i \\in \\boldsymbol{B_t}} \\partial_\\mathbf{w} L_i(\\mathbf{w}, b)\n",
    "$$\n",
    "$$\n",
    "b \\leftarrow b - \\frac{\\eta}{|\\boldsymbol{B}N|} \\sum_{i \\in \\boldsymbol{B_t}} \\partial_{b} L_i(\\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc10fcb",
   "metadata": {},
   "source": [
    "### ·ª®ng d·ª•ng c√°c Thu·∫≠t to√°n t·ªëi ∆∞u cho H·ªìi quy Tuy·∫øn t√≠nh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe8a89",
   "metadata": {},
   "source": [
    "ƒê·ªÉ thu·∫≠n ti·ªán, h√†m l·∫≠p tr√¨nh GD, SGD v√† SGD theo minibatch s·∫Ω c√≥ danh s√°ch tham s·ªë gi·ªëng nhau. C·ª• th·ªÉ, ch√∫ng ta th√™m tr·∫°ng th√°i ƒë·∫ßu v√†o bi·∫øn `states` v√† ƒë·∫∑t si√™u tham s·ªë trong bi·∫øn `hyperparams`. B√™n c·∫°nh ƒë√≥, ch√∫ng ta s·∫Ω t√≠nh gi√° tr·ªã m·∫•t m√°t trung b√¨nh c·ªßa t·ª´ng minibatch trong h√†m hu·∫•n luy·ªán, t·ª´ ƒë√≥ kh√¥ng c·∫ßn ph·∫£i chia gradient cho k√≠ch th∆∞·ªõc batch trong thu·∫≠t to√°n t·ªëi ∆∞u n·ªØa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c54e9-817f-4b0a-aaab-16cbd82c6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, states, hyperparams):\n",
    "    for p in params:\n",
    "        p.data.sub_(hyperparams['lr'] * p.grad)\n",
    "        p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c36b5",
   "metadata": {},
   "source": [
    "**Gi·∫£i th√≠ch**\n",
    "\n",
    "H√†m `sgd` s·∫Ω duy·ªát qua c√°c tham s·ªë `p` trong `params` c·ªßa m√¥ h√¨nh, trong b√†i to√°n H·ªìi quy Tuy·∫øn t√≠nh s·∫Ω l√† vector tr·ªçng s·ªë $\\mathbf{w}$ v√† $b$ v√† c·∫≠p nh·∫≠t theo quy t·∫Øc:\n",
    "$$\n",
    "p := p - \\eta*\\Delta_pL\n",
    "$$\n",
    "Trong ƒë√≥:\n",
    "- `p`: tham s·ªë\n",
    "- `p.grad`: ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t theo p\n",
    "- `hyperparams['lr']` hay $\\eta$: t·ªëc ƒë·ªô h·ªçc (learning rate)\n",
    "Sau ƒë√≥, th·ª±c hi·ªán reset gradient ·ªü b∆∞·ªõc cu·ªëi trong m·ªói v√≤ng l·∫∑p ƒë·ªÉ tr√°nh t√≠ch l≈©y gradient t·ª´ nhi·ªÅu batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3fcbde",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ch√∫ng ta hi·ªán th·ª±c m·ªôt h√†m hu·∫•n luy·ªán t·ªïng qu√°t, s·ª≠ d·ª•ng ƒë∆∞·ª£c cho t·∫•t c·∫£ c√°c thu·∫≠t to√°n t·ªëi ∆∞u. H√†m s·∫Ω kh·ªüi t·∫°o m·ªôt m√¥ h√¨nh H·ªìi quy Tuy·∫øn t√≠nh v√† c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi GD, SGD v√† SGD theo minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa49aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch11(trainer_fn, states, hyperparams, data_iter,\n",
    "               feature_dim, num_epochs=2):\n",
    "    # Initialization\n",
    "    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n",
    "                     requires_grad=True)\n",
    "    b = torch.zeros((1), requires_grad=True)\n",
    "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
    "    # Train\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            l = loss(net(X), y).mean()\n",
    "            l.backward()\n",
    "            trainer_fn([w, b], states, hyperparams)\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (d2l.evaluate_loss(net, data_iter, loss),))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n",
    "    return timer.cumsum(), animator.Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df4c87",
   "metadata": {},
   "source": [
    "**Gi·∫£i th√≠ch**\n",
    "\n",
    "H√†m `train_ch11` s·∫Ω kh·ªüi t·∫°o c√°c gi√° tr·ªã c·∫ßn cho m√¥ h√¨nh H·ªìi quy Tuy·∫øn t√≠nh v√† th·ª±c hi·ªán hu·∫•n luy·ªán.\n",
    "\n",
    "Tham s·ªë ƒë·∫ßu v√†o:\n",
    "- `trainer_fn`: h√†m c·∫≠p nh·∫≠t tham s·ªë m√¥ h√¨nh (GD, SGD, ...)\n",
    "- `states`: c√°c tr·∫°ng th√°i c·∫ßn thi·∫øt cho `trainer_fn`\n",
    "- `hyperparams`: c√°c si√™u tham s·ªë nh∆∞ `lr`, `beta`\n",
    "- `feature_dim`: s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (ho·∫∑c s·ªë chi·ªÅu bi·∫øn ƒë·∫ßu v√†o)\n",
    "- `num_epochs`: s·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán\n",
    "\n",
    "**B∆∞·ªõc 1: Kh·ªüi t·∫°o m√¥ h√¨nh**\n",
    "ƒê·∫ßu ti√™n, kh·ªüi t·∫°o 2 vector tr·ªçng s·ªë $\\mathbf{w}$ v√† bias $b$ ƒë·ªÅu y√™u c·∫ßu gradient, trong ƒë√≥:\n",
    "- $\\mathbf{w}$ tu√¢n theo ph√¢n ph·ªëi chu·∫©n v·ªõi $\\mu$ = 0.0 v√† $\\sigma$ = 0.01, c√≥ k√≠ch th∆∞·ªõc `feature_dim` x 1 \n",
    "- $b$ = 0\n",
    "\n",
    "Ti·∫øp theo, kh·ªüi t·∫°o `net` v·ªõi `loss`, l·∫ßn l∆∞·ª£t l√† m√¥ h√¨nh H·ªìi quy tuy·∫øn t√≠nh c√≥ d·∫°ng $y = \\boldsymbol{X}*\\mathbf{w} + b$ v√† h√†m m·∫•t m√°t theo MSE\n",
    "\n",
    "**B∆∞·ªõc 2: Kh·ªüi t·∫°o ti·∫øn tr√¨nh v·∫Ω h√†m m·∫•t m√°t v√† b·ªô ƒë·∫øm th·ªùi gian**\n",
    "- `animator`: th·ª±c th·ªÉ ƒë·ªÉ bi·ªÉu di·ªÖn h√†m m·∫•t m√°t d∆∞·ªõi d·∫°ng ƒë·ªì th·ªã theo th·ªùi gian\n",
    "- `n`: t·ªïng s·ªë m·∫´u ƒë√£ x·ª≠ l√Ω\n",
    "- `timer`: ƒë·ªÉ ƒëo th·ªùi gian ch·∫°y t·ª´ng epoch\n",
    "\n",
    "**B∆∞·ªõc 3: Hu·∫•n luy·ªán m√¥ h√¨nh**\n",
    "L·∫ßn l∆∞·ª£t duy·ªát qua t·ª´ng batch:\n",
    "1. T√≠nh gi√° tr·ªã h√†m m·∫•t m√°t: `l = loss(net(X), y).mean()` t∆∞∆°ng ·ª©ng vi·ªác l·∫•y trung b√¨nh gi√° tr·ªã MSE cho to√†n b·ªô c√°c ƒëi·ªÉm d·ªØ li·ªáu trong batch\n",
    "2. Th·ª±c hi·ªán lan truy·ªÅn ng∆∞·ª£c ƒë·ªÉ t√≠nh GD: `l.backward()`\n",
    "3. C·∫≠p nh·∫≠t tham s·ªë: `trainer_fn` ƒë∆∞·ª£c g·ªçi v·ªõi `[w, b]`, `states` v√† `hyperparams`\n",
    "4. Reset gradient descent n·∫±m trong `trainer_fn`.\n",
    "\n",
    "V·∫Ω h√†m m·∫•t m√°t v·ªõi m·ªói 200 m·∫´u\n",
    "\n",
    "**B∆∞·ªõc 4: In k·∫øt qu·∫£**\n",
    "In gi√° tr·ªã h√†m m·∫•t m√°t cu·ªëi c√πng v√† th·ªùi gian trung b√¨nh m·ªói epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2defb",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ta t·∫°o m·ªôt h√†m ƒë·∫ßu v√†o ƒë·ªÉ th·ª±c hi·ªán to√†n qu√° tr√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(lr, batch_size, num_epochs=2):\n",
    "    data_iter, feature_dim = get_data_ch11(batch_size)\n",
    "    return train_ch11(sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd1340",
   "metadata": {},
   "source": [
    "**Gi·∫£i th√≠ch**\n",
    "H√†m `train_sgd` th·ª±c hi·ªán vi·ªác ƒë·ªçc d·ªØ li·ªáu, kh·ªüi t·∫°o c√°c tham s·ªë v√† hu·∫•n luy·ªán m√¥ h√¨nh.\n",
    "\n",
    "Tham s·ªë ƒë·∫ßu v√†o:\n",
    "- `lr`: si√™u tham s·ªë t·ªëc ƒë·ªô h·ªçc (learning rate)\n",
    "- `batch_size`: k√≠ch th∆∞·ªõc c·ªßa batch\n",
    "- `num_epochs`: s·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán\n",
    "\n",
    "D·ªØ li·ªáu ƒë∆∞·ª£c tr·∫£ v·ªÅ theo t·ª´ng batch t·ª´ h√†m `get_data_ch11` s·∫Ω ƒë∆∞·ª£c d√πng ƒë·ªÉ hu·∫•n luy·ªán trong h√†m `train_ch11`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77e4cd",
   "metadata": {},
   "source": [
    "#### Th·ª±c nghi·ªám v·ªõi GD\n",
    "H√£y c√πng quan s√°t qu√° tr√¨nh t·ªëi ∆∞u c·ªßa thu·∫≠t to√°n Gradient Descent (GD) theo to√†n b·ªô batch. Ta c√≥ th·ªÉ s·ª≠ d·ª•ng to√†n b·ªô batch b·∫±ng c√°ch thi·∫øt l·∫≠p k√≠ch th∆∞·ªõc minibatch b·∫±ng t·ªïng s·ªë m·∫´u (trong tr∆∞·ªùng h·ª£p n√†y l√† 1500). K·∫øt qu·∫£ l√† c√°c tham s·ªë m√¥ h√¨nh ch·ªâ ƒë∆∞·ª£c c·∫≠p nh·∫≠t m·ªôt l·∫ßn duy nh·∫•t trong m·ªói epoch. C√≥ th·ªÉ th·∫•y kh√¥ng c√≥ ti·∫øn tri·ªÉn n√†o ƒë√°ng k·ªÉ. Trong v√≠ d·ª•, vi·ªác t·ªëi ∆∞u b·ªã ng·ª´ng tr·ªá sau 6 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_res = train_sgd(1, 1500, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d45e29",
   "metadata": {},
   "source": [
    "**Gi·∫£i th√≠ch**\n",
    "\n",
    "·ªû ƒë√¢y ta th·ª±c nghi·ªám hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi GD, s·ª≠ d·ª•ng $\\eta = 1$, s·ªë l∆∞·ª£ng epoch = 10 v√† thi·∫øt l·∫≠p tham s·ªë `batch_size` = 1500 (= k√≠ch th∆∞·ªõc c·ªßa t·∫≠p d·ªØ li·ªáu), t·ª©c l√† v·ªõi m·ªói epoch h√†m m·∫•t m√°t s·∫Ω ƒë∆∞·ª£c t√≠nh v√† th·ª±c hi·ªán c·∫≠p nh·∫≠t $(\\mathbf{w}, b)$ ch·ªâ 1 l·∫ßn duy nh·∫•t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8619e",
   "metadata": {},
   "source": [
    "#### Th·ª±c nghi·ªám v·ªõi SGD\n",
    "\n",
    "Khi k√≠ch th∆∞·ªõc c·ªßa batch b·∫±ng 1, ch√∫ng ta s·ª≠ d·ª•ng thu·∫≠t to√°n SGD ƒë·ªÉ t·ªëi ∆∞u. ƒê·ªÉ ƒë∆°n gi·∫£n h√≥a vi·ªác hi·ªán th·ª±c, ch√∫ng ta c·ªë ƒë·ªãnh t·ªëc ƒë·ªô h·ªçc (learning rate) b·∫±ng m·ªôt h·∫±ng s·ªë (c√≥ gi√° tr·ªã nh·ªè). Trong SGD, c√°c tham s·ªë m√¥ h√¨nh ƒë∆∞·ª£c c·∫≠p nh·∫≠t b·∫•t c·ª© khi n√†o c√≥ m·ªôt m·∫´u hu·∫•n luy·ªán ƒë∆∞·ª£c x·ª≠ l√Ω. Trong tr∆∞·ªùng h·ª£p n√†y, s·∫Ω c√≥ 1500 l·∫ßn c·∫≠p nh·∫≠t trong m·ªói epoch. C√≥ th·ªÉ th·∫•y, s·ª± suy gi·∫£m gi√° tr·ªã c·ªßa h√†m m·ª•c ti√™u ch·∫≠m l·∫°i sau m·ªôt epoch. M·∫∑c d√π c·∫£ hai thu·∫≠t to√°n c√πng x·ª≠ l√Ω 1500 m·∫´u trong m·ªôt epoch, SGD t·ªën th·ªùi gian h∆°n GD trong th√≠ nghi·ªám tr√™n. ƒêi·ªÅu n√†y l√† do SGD c·∫≠p nh·∫≠t c√°c tham s·ªë th∆∞·ªùng xuy√™n h∆°n v√† k√©m hi·ªáu qu·∫£ khi x·ª≠ l√Ω ƒë∆°n l·∫ª t·ª´ng m·∫´u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0322383",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_res = train_sgd(0.005, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e4962",
   "metadata": {},
   "source": [
    "**Gi·∫£i th√≠ch**\n",
    "\n",
    "·ªû ƒë√¢y ta th·ª±c nghi·ªám hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi SGD, s·ª≠ d·ª•ng $\\eta = 0.005$, s·ªë l∆∞·ª£ng epoch = 2 (theo m·∫∑c ƒë·ªãnh) v√† thi·∫øt l·∫≠p tham s·ªë `batch_size` = 1, t·ª©c l√† v·ªõi m·ªói epoch h√†m m·∫•t m√°t s·∫Ω ƒë∆∞·ª£c t√≠nh v√† th·ª±c hi·ªán c·∫≠p nh·∫≠t $(\\mathbf{w}, b)$ v·ªõi m·ªói ƒëi·ªÉm d·ªØ li·ªáu trong t·∫≠p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2255f2",
   "metadata": {},
   "source": [
    "#### Th·ª±c nghi·ªám v·ªõi SGD theo minibatch\n",
    "\n",
    "Cu·ªëi c√πng, khi k√≠ch th∆∞·ªõc c·ªßa batch b·∫±ng 100, ch√∫ng ta s·ª≠ d·ª•ng thu·∫≠t to√°n SGD theo minibatch ƒë·ªÉ t·ªëi ∆∞u. Th·ªùi gian c·∫ßn thi·∫øt cho m·ªói epoch ng·∫Øn h∆°n th·ªùi gian t∆∞∆°ng ·ª©ng c·ªßa SGD v√† GD theo to√†n b·ªô batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini1_res = train_sgd(.4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea9943",
   "metadata": {},
   "source": [
    "**Gi·∫£i th√≠ch**\n",
    "\n",
    "·ªû ƒë√¢y ta th·ª±c nghi·ªám hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi SGD theo minibatch c√≥ k√≠ch th∆∞·ªõc = 100, s·ª≠ d·ª•ng $\\eta = 0.4$, s·ªë l∆∞·ª£ng epoch = 2 (theo m·∫∑c ƒë·ªãnh), t·ª©c l√† v·ªõi m·ªói epoch h√†m m·∫•t m√°t s·∫Ω ƒë∆∞·ª£c t√≠nh v√† th·ª±c hi·ªán c·∫≠p nh·∫≠t $(\\mathbf{w}, b)$ t∆∞∆°ng ·ª©ng l√† $\\frac{1500}{100} = 15$ l·∫ßn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc24d1",
   "metadata": {},
   "source": [
    "Gi·∫£m k√≠ch th∆∞·ªõc c·ªßa batch b·∫±ng 10, th·ªùi gian cho m·ªói epoch tƒÉng v√¨ th·ª±c thi t√≠nh to√°n tr√™n m·ªói batch k√©m hi·ªáu qu·∫£ h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe23a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini2_res = train_sgd(.05, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff3d1a",
   "metadata": {},
   "source": [
    "**Gi·∫£i th√≠ch**\n",
    "\n",
    "·ªû ƒë√¢y ta th·ª±c nghi·ªám hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi SGD theo minibatch c√≥ k√≠ch th∆∞·ªõc = 10, s·ª≠ d·ª•ng $\\eta = 0.05$, s·ªë l∆∞·ª£ng epoch = 2 (theo m·∫∑c ƒë·ªãnh), t·ª©c l√† v·ªõi m·ªói epoch h√†m m·∫•t m√°t s·∫Ω ƒë∆∞·ª£c t√≠nh v√† th·ª±c hi·ªán c·∫≠p nh·∫≠t $(\\mathbf{w}, b)$ t∆∞∆°ng ·ª©ng l√† $\\frac{1500}{10} = 150$ l·∫ßn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04560786",
   "metadata": {},
   "source": [
    "Cu·ªëi c√πng, ch√∫ng ta so s√°nh t∆∞∆°ng quan th·ªùi gian v√† gi√° tr·ªã h√†m m·∫•y m√°t trong b·ªën th√≠ nghi·ªám tr√™n. C√≥ th·ªÉ th·∫•y, d√π h·ªôi t·ª• nhanh h∆°n GD v·ªÅ s·ªë m·∫´u ƒë∆∞·ª£c x·ª≠ l√Ω, SGD t·ªën nhi·ªÅu th·ªùi gian h∆°n ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c c√πng gi√° tr·ªã m·∫•t m√°t nh∆∞ GD v√¨ thu·∫≠t to√°n n√†y t√≠nh gradient descent tr√™n t·ª´ng m·∫´u m·ªôt. Thu·∫≠t to√°n SGD theo minibatch c√≥ th·ªÉ c√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô h·ªôi t·ª• v√† hi·ªáu qu·∫£ t√≠nh to√°n. V·ªõi k√≠ch th∆∞·ªõc minibatch b·∫±ng 10, thu·∫≠t to√°n n√†y hi·ªáu qu·∫£ h∆°n SGD; v√† v·ªõi k√≠ch th∆∞·ªõc minibatch b·∫±ng 100, th·ªùi gian ch·∫°y c·ªßa thu·∫≠t to√°n n√†y th·∫≠m ch√≠ nhanh h∆°n c·∫£ GD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8473cb",
   "metadata": {},
   "source": [
    "### T√≥m t·∫Øt\n",
    "\n",
    "| Ti√™u ch√≠                             | Gradient Descent (GD)            | Stochastic Gradient Descent (SGD) | Minibatch SGD                    |\n",
    "| ------------------------------------ | -------------------------------- | --------------------------------- | --------------------------------- |\n",
    "| **K√≠ch th∆∞·ªõc d·ªØ li·ªáu d√πng m·ªói b∆∞·ªõc** | To√†n b·ªô t·∫≠p d·ªØ li·ªáu              | 1 m·∫´u d·ªØ li·ªáu                     | M·ªôt nh√≥m nh·ªè (batch)              |\n",
    "| **T·∫ßn su·∫•t c·∫≠p nh·∫≠t tham s·ªë**        | 1 l·∫ßn / epoch                    | N l·∫ßn / epoch (v·ªõi N = s·ªë m·∫´u)    | N/B l·∫ßn / epoch (B = batch size)  |\n",
    "| **T·ªëc ƒë·ªô t√≠nh to√°n m·ªói b∆∞·ªõc**        | Ch·∫≠m (ph·∫£i qu√©t to√†n b·ªô d·ªØ li·ªáu) | R·∫•t nhanh                         | Trung b√¨nh                        |\n",
    "| **ƒê·ªô ·ªïn ƒë·ªãnh gradient**              | ·ªîn ƒë·ªãnh, √≠t dao ƒë·ªông             | Dao ƒë·ªông m·∫°nh, nhi·ªÖu nhi·ªÅu        | Dao ƒë·ªông v·ª´a ph·∫£i                 |\n",
    "| **Kh·∫£ nƒÉng h·ªôi t·ª•**                  | Ch·∫≠m nh∆∞ng m∆∞·ª£t                  | Nhanh ban ƒë·∫ßu, c√≥ th·ªÉ dao ƒë·ªông    | C√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô v√† ·ªïn ƒë·ªãnh   |\n",
    "| **Kh·∫£ nƒÉng tho√°t local minima**      | Th·∫•p                             | Cao (nh·ªù nhi·ªÖu)                   | T∆∞∆°ng ƒë·ªëi t·ªët                     |\n",
    "| **Y√™u c·∫ßu b·ªô nh·ªõ (RAM)**             | Cao (v√¨ x·ª≠ l√Ω to√†n b·ªô data)      | R·∫•t th·∫•p                          | V·ª´a ph·∫£i                          |\n",
    "| **·ª®ng d·ª•ng th·ª±c t·∫ø**                 | Hi·∫øm d√πng v·ªõi d·ªØ li·ªáu l·ªõn        | D√πng nhi·ªÅu cho online learning    | Ph·ªï bi·∫øn nh·∫•t trong deep learning |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c5892-76e4-4f5c-b72c-bff4fc4ec739",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b29ea5",
   "metadata": {},
   "source": [
    "Trong Gluon, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng l·ªõp `Trainer` ƒë·ªÉ g·ªçi c√°c thu·∫≠t to√°n t·ªëi ∆∞u. C√°ch n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c√≥ th·ªÉ hi·ªán th·ª±c m·ªôt h√†m hu·∫•n luy·ªán t·ªïng qu√°t. Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng h√†m n√†y xuy√™n su·ªët c√°c ph·∫ßn ti·∫øp theo c·ªßa ch∆∞∆°ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8af60-a69d-40dc-9f2e-5583557c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):\n",
    "    # Initialization\n",
    "    net = nn.Sequential(nn.Linear(5, 1))\n",
    "    def init_weights(module):\n",
    "        if type(module) == nn.Linear:\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    optimizer = trainer_fn(net.parameters(), **hyperparams)\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            out = net(X)\n",
    "            y = y.reshape(out.shape)\n",
    "            l = loss(out, y)\n",
    "            l.mean().backward()\n",
    "            optimizer.step()\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                # `MSELoss` computes squared error without the 1/2 factor\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (d2l.evaluate_loss(net, data_iter, loss) / 2,))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, _ = get_data_ch11(10)\n",
    "trainer = torch.optim.SGD\n",
    "train_concise_ch11(trainer, {'lr': 0.01}, data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565256bc-8212-46d3-b07c-b4b31fa3f62e",
   "metadata": {},
   "source": [
    "## Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadbff6-3a56-4bb7-af84-59a5cf724da1",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "S·ª≠a ƒë·ªïi k√≠ch th∆∞·ªõc batch v√† t·ªëc ƒë·ªô h·ªçc, quan s√°t t·ªëc ƒë·ªô suy gi·∫£m gi√° tr·ªã c·ªßa h√†m m·ª•c ti√™u v√† th·ªùi gian cho m·ªói epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ª≠ nghi·ªám v·ªõi c√°c gi√° tr·ªã kh√°c nhau c·ªßa t·ªëc ƒë·ªô h·ªçc v√† k√≠ch th∆∞·ªõc batch\n",
    "# T·ªëc ƒë·ªô h·ªçc l·ªõn h∆°n v√† k√≠ch th∆∞·ªõc batch nh·ªè h∆°n\n",
    "experiment_1 = train_sgd(lr=0.1, batch_size=5, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3295ee-b93c-4dbb-883d-2a78fc17b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªëc ƒë·ªô h·ªçc nh·ªè h∆°n v√† k√≠ch th∆∞·ªõc batch l·ªõn h∆°n\n",
    "experiment_2 = train_sgd(lr=0.01, batch_size=50, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326251fb-1f59-411a-8935-17e674dc5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªëc ƒë·ªô h·ªçc trung b√¨nh v√† k√≠ch th∆∞·ªõc batch trung b√¨nh\n",
    "experiment_3 = train_sgd(lr=0.05, batch_size=20, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b8ca3",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "1. **T·ªëc ƒë·ªô h·ªçc l·ªõn h∆°n v√† k√≠ch th∆∞·ªõc batch nh·ªè h∆°n**:\n",
    "    - V·ªõi t·ªëc ƒë·ªô h·ªçc l·ªõn v√† k√≠ch th∆∞·ªõc batch nh·ªè, m√¥ h√¨nh c√≥ th·ªÉ h·ªôi t·ª• nhanh h∆°n nh∆∞ng d·ªÖ g·∫∑p ph·∫£i dao ƒë·ªông l·ªõn trong qu√° tr√¨nh t·ªëi ∆∞u h√≥a do ph∆∞∆°ng sai cao c·ªßa gradient.\n",
    "\n",
    "2. **T·ªëc ƒë·ªô h·ªçc nh·ªè h∆°n v√† k√≠ch th∆∞·ªõc batch l·ªõn h∆°n**:\n",
    "    - V·ªõi t·ªëc ƒë·ªô h·ªçc nh·ªè v√† k√≠ch th∆∞·ªõc batch l·ªõn, m√¥ h√¨nh h·ªôi t·ª• ·ªïn ƒë·ªãnh h∆°n nh∆∞ng t·ªëc ƒë·ªô h·ªôi t·ª• c√≥ th·ªÉ ch·∫≠m h∆°n do c√°c b∆∞·ªõc c·∫≠p nh·∫≠t nh·ªè.\n",
    "\n",
    "3. **T·ªëc ƒë·ªô h·ªçc trung b√¨nh v√† k√≠ch th∆∞·ªõc batch trung b√¨nh**:\n",
    "    - V·ªõi t·ªëc ƒë·ªô h·ªçc v√† k√≠ch th∆∞·ªõc batch trung b√¨nh, m√¥ h√¨nh ƒë·∫°t ƒë∆∞·ª£c s·ª± c√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô h·ªôi t·ª• v√† ƒë·ªô ·ªïn ƒë·ªãnh, th∆∞·ªùng mang l·∫°i k·∫øt qu·∫£ t·ªët nh·∫•t.\n",
    "\n",
    "K√≠ch th∆∞·ªõc batch v√† t·ªëc ƒë·ªô h·ªçc l√† c√°c si√™u tham s·ªë quan tr·ªçng, c·∫ßn ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ph√π h·ª£p v·ªõi b√†i to√°n c·ª• th·ªÉ ƒë·ªÉ ƒë·∫°t hi·ªáu qu·∫£ t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b73c7",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "ƒê·ªçc th√™m t√†i li·ªáu MXNet v√† s·ª≠ d·ª•ng h√†m set_learning_rate c·ªßa l·ªõp Trainer ƒë·ªÉ gi·∫£m t·ªëc ƒë·ªô h·ªçc c·ªßa SGD theo minibatch b·∫±ng 1/10 gi√° tr·ªã tr∆∞·ªõc ƒë√≥ sau m·ªói epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851407d-6c0c-4c2a-b844-7f7a35382b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "\n",
    "# Define a simple model\n",
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(10))\n",
    "net.initialize(mx.init.Xavier())\n",
    "\n",
    "# Initialize Trainer with SGD optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "\n",
    "# Simulate training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # Reduce learning rate by a factor of 10 after each epoch\n",
    "    new_lr = trainer.learning_rate * 0.1\n",
    "    trainer.set_learning_rate(new_lr)\n",
    "    print(f'Epoch {epoch+1}: Learning rate = {new_lr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165aee7c",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "H√£y so s√°nh SGD theo minibatch s·ª≠ d·ª•ng m·ªôt bi·∫øn th·ªÉ l·∫•y m·∫´u c√≥ ho√†n l·∫°i t·ª´ t·∫≠p hu·∫•n luy·ªán. ƒêi·ªÅu g√¨ s·∫Ω x·∫£y ra?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a503c84",
   "metadata": {},
   "source": [
    "Khi hu·∫•n luy·ªán m√¥ h√¨nh s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p SGD theo minibatch th√¥ng th∆∞·ªùng, ta c√≥:\n",
    "- C√°ch ho·∫°t ƒë·ªông:\n",
    "    + V·ªõi m·ªói v√≤ng l·∫∑p hu·∫•n luy·ªán, t·∫≠p d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c chia nh·ªè th√†nh c√°c minibatch.\n",
    "    + M·ªói minibatch ch·ª©a c√°c m·∫´u d·ªØ li·ªáu ƒë·ªôc nh·∫•t cho ƒë·∫øn khi v√≤ng l·∫∑p ƒë∆∞·ª£c ho√†n th√†nh.\n",
    "    + M·ªói ƒëi·ªÉm d·ªØ li·ªáu ch·ªâ ƒë∆∞·ª£c d√πng 1 l·∫ßn m·ªói v√≤ng l·∫∑p.\n",
    "- ƒê·∫∑c ƒëi·ªÉm:\n",
    "    + ∆Ø·ªõc l∆∞·ª£ng gradient hi·ªáu qu·∫£: ph∆∞∆°ng sai cho m·ªói minibatch t∆∞∆°ng ƒë·ªëi nh·ªè.\n",
    "    + H·ªôi t·ª• nhanh v√† m∆∞·ª£t m√† h∆°n.\n",
    "    + ƒê·∫£m b·∫£o c√°c m·∫´u ƒë·ªÅu ƒë∆∞·ª£c xem x√©t m·ªói v√≤ng l·∫∑p hu·∫•n luy·ªán.\n",
    "\n",
    "So s√°nh v·ªõi bi·∫øn th·ªÉ minibatch cho ph√©p l·∫•y m·∫´u ho√†n l·∫°i t·ª´ t·∫≠p d·ªØ li·ªáu, ta c√≥:\n",
    "- C√°ch ho·∫°t ƒë·ªông:\n",
    "    + V·ªõi m·ªói v√≤ng l·∫∑p hu·∫•n luy·ªán, c√°c minibatch s·∫Ω ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch l·∫•y ng·∫´u nhi√™n t·ª´ t·∫≠p d·ªØ li·ªáu.\n",
    "    + M·ªôt v√†i m·∫´u s·∫Ω ƒë∆∞·ª£c ch·ªçn nhi·ªÅu l·∫ßn, m·ªôt v√†i m·∫´u th√¨ kh√¥ng bao gi·ªù ƒë∆∞·ª£c ch·ªçn.\n",
    "    + Kh√¥ng ƒë·∫£m b·∫£o r·∫±ng t·∫•t c·∫£ c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c d√πng 1 l·∫ßn m·ªói v√≤ng l·∫∑p.\n",
    "- ƒê·∫∑c ƒëi·ªÉm:\n",
    "    + ∆Ø·ªõc l∆∞·ª£ng gradient k√©m hi·ªáu qu·∫£: c√≥ nhi·ªÅu nhi·ªÖu do b·ªã tr√πng ho·∫∑c thi·∫øu m·ªôt ph·∫ßn m·∫´u.\n",
    "    + H·ªôi t·ª• ch·∫≠m ho·∫∑c k√©m ·ªïn ƒë·ªãnh\n",
    "    + Thi·∫øu t√≠nh kh√°i qu√°t h√≥a do b·ªã overfit v·ªõi c√°c m·∫´u d·ªØ li·ªáu ƒë∆∞·ª£c l·ª±a ch·ªçn nhi·ªÅu v√† underfit v·ªõi c√°c m·∫´u d·ªØ li·ªáu kh√¥ng ƒë∆∞·ª£c l·ª±a ch·ªçn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8bad3",
   "metadata": {},
   "source": [
    "### Exercise 4.\n",
    "M·ªôt √°c th·∫ßn ƒë√£ sao ch√©p t·∫≠p d·ªØ li·ªáu c·ªßa b·∫°n m√† kh√¥ng n√≥i cho b·∫°n bi·∫øt (c·ª• th·ªÉ, m·ªói quan s√°t b·ªã l·∫∑p l·∫°i hai l·∫ßn v√† k√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu tƒÉng g·∫•p ƒë√¥i so v·ªõi ban ƒë·∫ßu). C√°ch ho·∫°t ƒë·ªông c·ªßa c√°c thu·∫≠t to√°n h·∫° gradient, SGD v√† SGD theo minibatch s·∫Ω thay ƒë·ªïi nh∆∞ th·∫ø n√†o?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f1d7f",
   "metadata": {},
   "source": [
    "N·∫øu t·∫≠p d·ªØ li·ªáu b·ªã l·∫∑p l·∫°i, ta ƒëang c√≥ k√≠ch th∆∞·ªõc c·ªßa m·∫´u quan s√°t tƒÉng l√™n nh∆∞ng kh√¥ng c√≥ th√™m th√¥ng tin m·ªõi. ƒêi·ªÅu n√†y ·∫£nh h∆∞·ªüng kh√°c nhau ƒë·∫øn c√°c ph∆∞∆°ng ph√°p t·ªëi ∆∞u theo c√°c c√°ch kh√°c nhau:\n",
    "1. Gradient Descent (GD) - S·ª≠ d·ª•ng to√†n b·ªô m·∫´u d·ªØ li·ªáu\n",
    "- Gradient Descent s·ª≠ d·ª•ng to√†n b·ªô m·∫´u d·ªØ li·ªáu, n√™n s·∫Ω l√†m tƒÉng gi√° tr·ªã h√†m m·∫•t m√°t v√† ƒë·ªô d·ªëc theo t·ªâ l·ªá thu·∫≠n. Tuy nhi√™n, h∆∞·ªõng c·ªßa gradient s·∫Ω kh√¥ng ƒë·ªïi, do ta ch·ªâ c·ªông c√°c th√†nh ph·∫ßn l·∫∑p l·∫°i.\n",
    "- ·∫¢nh h∆∞·ªüng:\n",
    "    + H∆∞·ªõng c·ªßa gradient ƒë∆∞·ª£c gi·ªØ nguy√™n, nh∆∞ng ƒë∆∞·ª£c tƒÉng l√™n.\n",
    "    + T·ªëc ƒë·ªô h·ªçc c·∫ßn ph·∫£i ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªÉ duy tr√¨ ƒë∆∞·ª£c ƒë·ªô ·ªïn ƒë·ªãnh.\n",
    "    + Chi ph√≠ t√≠nh to√°n tƒÉng l√™n (l√¢u h∆°n m·ªói v√≤ng l·∫∑p nh∆∞ng kh√¥ng c√≥ th√™m th√¥ng tin g√¨)\n",
    "2. Stochastic Gradient Descent (SGD) ‚Äì S·ª≠ d·ª•ng 1 m·∫´u\n",
    "- M·ªói m·∫´u s·∫Ω c√≥ kh·∫£ nƒÉng cao h∆°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu l·∫ßn, nh∆∞ng kh√¥ng c√≥ s·ª± thay ƒë·ªïi v·ªÅ ch·∫•t l∆∞·ª£ng gradient v√† t√≠nh ƒëa d·∫°ng c·ªßa t·∫≠p d·ªØ li·ªáu\n",
    "- ·∫¢nh h∆∞·ªüng:\n",
    "    + Kh√¥ng c√≥ s·ª± kh√°c bi·ªát c∆° b·∫£n v·ªÅ qu√° tr√¨nh hu·∫•n luy·ªán.\n",
    "    + Ph∆∞∆°ng sai kh√¥ng ƒë·ªïi.\n",
    "    + H·ªôi t·ª• ch·∫≠m h∆°n do tr√πng l·∫∑p d·ªØ li·ªáu.\n",
    "3. Minibatch Stochastic Gradient Descent (Minibatch SGD) - S·ª≠ d·ª•ng t·∫≠p con\n",
    "- C√°c minibatch s·∫Ω ch·ª©a nhi·ªÅu m·∫´u tr√πng l·∫∑p h∆°n gi·ªØa c√°c v√≤ng l·∫∑p hu·∫•n luy·ªán.\n",
    "- T·ªëc ƒë·ªô h·ªçc c√≥ th·ªÉ ch·∫≠m l·∫°i do li√™n t·ª•c th·∫•y c√°c m·∫´u tr√πng l·∫∑p\n",
    "- ·∫¢nh h∆∞·ªüng:\n",
    "    + H∆∞·ªõng c·ªßa gradient ƒë∆∞·ª£c gi·ªØ nguy√™n, nh∆∞ng t√≠nh kh√°i qu√°t l√¢u ƒë∆∞·ª£c c·∫£i thi·ªán.\n",
    "    + C√≥ th·ªÉ t·ªën c√°c v√≤ng l·∫∑p ƒë·ªÉ hu·∫•n luy·ªán c√°c m·∫´u tr√πng l·∫∑p."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
