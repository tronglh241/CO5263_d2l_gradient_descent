{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dbf639-4c63-4693-8946-139fbc148da6",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Gradient descent l√† m·ªôt thu·∫≠t to√°n t·ªëi ∆∞u h√≥a th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√†o t·∫°o c√°c m√¥ h√¨nh h·ªçc m√°y v√† m·∫°ng n∆°-ron. N√≥ ƒë√†o t·∫°o c√°c m√¥ h√¨nh h·ªçc m√°y b·∫±ng c√°ch gi·∫£m thi·ªÉu l·ªói gi·ªØa k·∫øt qu·∫£ d·ª± ƒëo√°n v√† k·∫øt qu·∫£ th·ª±c t·∫ø.\n",
    "Thu·∫≠t to√°n ho·∫°t ƒë·ªông b·∫±ng c√°ch ƒëi·ªÅu ch·ªânh li√™n t·ª•c c√°c tham s·ªë c·ªßa m√¥ h√¨nh (nh∆∞ tr·ªçng s·ªë v√† ƒë·ªô l·ªách) theo h∆∞·ªõng l√†m gi·∫£m chi ph√≠ nhi·ªÅu nh·∫•t. H∆∞·ªõng n√†y ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng c√°ch t√≠nh to√°n ƒë·ªô d·ªëc (h∆∞·ªõng ƒë·∫øn m·ª©c tƒÉng chi ph√≠ nhi·ªÅu nh·∫•t) c·ªßa h√†m chi ph√≠ li√™n quan ƒë·∫øn c√°c tham s·ªë, sau ƒë√≥ di chuy·ªÉn c√°c tham s·ªë theo h∆∞·ªõng ng∆∞·ª£c l·∫°i.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf1d29-f160-4a3a-854b-3a8748e13307",
   "metadata": {},
   "source": [
    "## One-Dimensional Gradient Descent\n",
    "X√©t m·ªôt h√†m th·ª±c kh·∫£ vi li√™n t·ª•c $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ . S·ª≠ d·ª•ng khai tri·ªÉn Taylor:\n",
    "$$f(x+\\epsilon)=f(x)+\\epsilon f^{\\prime}(x)+O\\left(\\epsilon^2\\right). \\tag{1}$$ \n",
    "T·ª©c l√†, trong x·∫•p x·ªâ b·∫≠c m·ªôt, $f(x+\\epsilon)$ ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi gi√° tr·ªã h√†m $f(x)$ v√† ƒë·∫°o h√†m b·∫≠c m·ªôt $f^{\\prime}(x)$ t·∫°i $x$. C√≥ th·ªÉ gi·∫£ ƒë·ªãnh r·∫±ng v·ªõi $\\epsilon$ nh·ªè, vi·ªác di chuy·ªÉn theo h∆∞·ªõng gradient √¢m s·∫Ω gi·∫£m $f$. Ch·ªçn m·ªôt k√≠ch th∆∞·ªõc b∆∞·ªõc c·ªë ƒë·ªãnh $\\eta>0$ v√† ch·ªçn $\\epsilon=-\\eta f^{\\prime}(x)$. Thay v√†o khai tri·ªÉn Taylor ·ªü tr√™n:\n",
    "$$f\\left(x-\\eta f^{\\prime}(x)\\right)=f(x)-\\eta f^{\\prime 2}(x)+O\\left(\\eta^2 f^{\\prime 2}(x)\\right).\\tag{2}$$\n",
    "N·∫øu ƒë·∫°o h√†m $f^{\\prime}(x) \\neq 0$ kh√¥ng ti√™u bi·∫øn, ta ƒë·∫°t ƒë∆∞·ª£c b∆∞·ªõc ti·∫øn t·ªõi ƒëi·ªÉm t·ªëi ∆∞u v√¨ $\\eta f^{\\prime 2}(x)>0$. H∆°n n·ªØa, ta lu√¥n c√≥ th·ªÉ ch·ªçn $\\eta$ ƒë·ªß nh·ªè ƒë·ªÉ c√°c h·∫°ng b·∫≠c cao tr·ªü n√™n kh√¥ng ƒë√°ng k·ªÉ. Do ƒë√≥:\n",
    "$$f\\left(x-\\eta f^{\\prime}(x)\\right) \\lessapprox f(x). \\tag{3}$$\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†, n·∫øu ta s·ª≠ d·ª•ng:\n",
    "$$x \\leftarrow x-\\eta f^{\\prime}(x). \\tag{4}$$\n",
    "ƒë·ªÉ l·∫∑p $x$, gi√° tr·ªã c·ªßa h√†m $f(x)$ c√≥ th·ªÉ gi·∫£m. Do ƒë√≥, trong gradient descent, ta ƒë·∫ßu ti√™n ch·ªçn m·ªôt gi√° tr·ªã ban ƒë·∫ßu $x$ v√† m·ªôt h·∫±ng s·ªë $\\eta>0$, sau ƒë√≥ s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ l·∫∑p $x$ li√™n t·ª•c cho ƒë·∫øn khi ƒë·∫°t ƒëi·ªÅu ki·ªán d·ª´ng, v√≠ d·ª•, khi ƒë·ªô l·ªõn c·ªßa gradient $\\left|f^{\\prime}(x)\\right|$ ƒë·ªß nh·ªè ho·∫∑c s·ªë l·∫ßn l·∫∑p ƒë·∫°t m·ªôt gi√° tr·ªã nh·∫•t ƒë·ªãnh.\n",
    "### V√≠ d·ª• minh h·ªça\n",
    "Ta ch·ªçn h√†m m·ª•c ti√™u $f(x)=x^2 +5sin(x)$ ƒë·ªÉ minh h·ªça c√°ch th·ª±c hi·ªán gradient descent. ƒê·∫°o h√†m: $f^{\\prime}(x) =2x + 5cos(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5239bff-4d53-470c-98d1-8115d76c38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import d2l\n",
    "def f(x):  # objective function\n",
    "    return x**2 + 5 * np.sin(x)\n",
    "\n",
    "def f_grad(x):  # Gradient (derivative) of the objective function\n",
    "    return 2*x + 5 * np.cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f83e0",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ta s·ª≠ d·ª•ng $x=-5$ l√†m gi√° tr·ªã ban ƒë·∫ßu v√† gi·∫£ s·ª≠ $\\eta=0.1$. S·ª≠ d·ª•ng gradient descent ƒë·ªÉ l·∫∑p $x$ 15 l·∫ßn, ta c√≥ th·ªÉ th·∫•y r·∫±ng x xu·∫•t ph√°t t·ª´ b√™n tr√°i v√† cu·ªëi c√πng, gi√° tr·ªã c·ªßa $x$ ti·∫øn g·∫ßn ƒë·∫øn nghi·ªám t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(eta, f_grad, start_x, step):\n",
    "    x = start_x\n",
    "    results = [x]\n",
    "    for i in range(step):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(x)\n",
    "    print(f'epoch 11, x: {x:.6f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.1, f_grad,-5,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73410907",
   "metadata": {},
   "source": [
    "Ti·∫øn tr√¨nh t·ªëi ∆∞u h√≥a $x$ c√≥ th·ªÉ ƒë∆∞·ª£c v·∫Ω nh∆∞ sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = np.arange(-n, n, 0.01)\n",
    "    d2l.set_figsize()\n",
    "    d2l.plot([f_line, results], [[f(x) for x in f_line], [f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86e155-23d6-4686-ba20-e5492cd75f5e",
   "metadata": {},
   "source": [
    "Ta s·ª≠ d·ª•ng $x=4$ l√†m gi√° tr·ªã ban ƒë·∫ßu v√† gi·∫£ s·ª≠ $\\eta=0.1$. S·ª≠ d·ª•ng gradient descent ƒë·ªÉ l·∫∑p $x$ 30 l·∫ßn, ta c√≥ th·ªÉ th·∫•y r·∫±ng x xu·∫•t ph√°t t·ª´ b√™n ph·∫£i v√† ƒëi d·∫ßn t·ªõi nghi·ªám t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c8e2c-b80d-4786-9d64-b1418b7de522",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gd(0.1, f_grad,4,25)\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fea95",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "T·ªëc ƒë·ªô h·ªçc (learning rate) ùúÇ c√≥ th·ªÉ ƒë∆∞·ª£c thi·∫øt l·∫≠p b·ªüi ng∆∞·ªùi thi·∫øt k·∫ø thu·∫≠t to√°n. N·∫øu ch√∫ng ta s·ª≠ d·ª•ng m·ªôt t·ªëc ƒë·ªô h·ªçc qu√° nh·ªè, n√≥ s·∫Ω khi·∫øn `ùë•` c·∫≠p nh·∫≠t r·∫•t ch·∫≠m, ƒë√≤i h·ªèi nhi·ªÅu v√≤ng l·∫∑p h∆°n ƒë·ªÉ thu ƒë∆∞·ª£c nghi·ªám t·ªët h∆°n. ƒê·ªÉ minh h·ªça ƒëi·ªÅu x·∫£y ra trong tr∆∞·ªùng h·ª£p nh∆∞ v·∫≠y, h√£y xem x√©t ti·∫øn tr√¨nh trong c√πng b√†i to√°n t·ªëi ∆∞u v·ªõi ùúÇ = 0.02. Nh∆∞ ta c√≥ th·ªÉ th·∫•y, ngay c·∫£ sau 10 b∆∞·ªõc l·∫∑p, ch√∫ng ta v·∫´n c√≤n c√°ch xa nghi·ªám t·ªëi ∆∞u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(0.02, f_grad,-5,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d3615",
   "metadata": {},
   "source": [
    "Ng∆∞·ª£c l·∫°i, n·∫øu ch√∫ng ta s·ª≠ d·ª•ng t·ªëc ƒë·ªô h·ªçc qu√° l·ªõn, gi√° tr·ªã `|ùúÇ ùëì'(ùë•)|` c√≥ th·ªÉ tr·ªü n√™n qu√° l·ªõn ƒë·ªëi v·ªõi c√¥ng th·ª©c khai tri·ªÉn Taylor b·∫≠c nh·∫•t. Nghƒ©a l√†, s·ªë h·∫°ng `O (ùúÇ¬≤ ùëì'¬≤(ùë•))` trong c√¥ng th·ª©c (2) c√≥ th·ªÉ tr·ªü n√™n ƒë√°ng k·ªÉ. Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta kh√¥ng th·ªÉ ƒë·∫£m b·∫£o r·∫±ng qu√° tr√¨nh c·∫≠p nh·∫≠t l·∫∑p c·ªßa `ùë•` s·∫Ω l√†m gi·∫£m gi√° tr·ªã c·ªßa h√†m `ùëì(ùë•)`. V√≠ d·ª•, khi ch√∫ng ta ƒë·∫∑t t·ªëc ƒë·ªô h·ªçc `ùúÇ = 1.1`, `ùë•` v∆∞·ª£t qu√° (overshoots) nghi·ªám t·ªëi ∆∞u `ùë• = 0` v√† d·∫ßn d·∫ßn ph√¢n k·ª≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(1.1, f_grad,-5,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c64e8b",
   "metadata": {},
   "source": [
    "### Local Minima\n",
    "ƒê·ªÉ minh h·ªça ƒëi·ªÅu g√¨ x·∫£y ra v·ªõi c√°c h√†m kh√¥ng l·ªìi, h√£y xem x√©t tr∆∞·ªùng h·ª£p $f(x) = 0.25x^4 - \\frac{1}{3}x^3 - 1.5x^2 + 2x$ . H√†m n√†y c√≥  c·ª±c ti·ªÉu c·ª•c b·ªô t·∫°i $x = 2$, v·ªõi $f(2) \\approx -0.67$ v√† c·ª±c ti·ªÉu to√†n c·ª•c n·∫±m t·∫°i $x \\approx -1.618$, v·ªõi $f(-1.618) \\approx -4.04$. T√πy thu·ªôc v√†o l·ª±a ch·ªçn t·ªëc ƒë·ªô h·ªçc v√† m·ª©c ƒë·ªô ƒëi·ªÅu ki·ªán c·ªßa b√†i to√°n, ta c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ªôt trong nhi·ªÅu nghi·ªám. V√≠ d·ª• d∆∞·ªõi ƒë√¢y minh h·ªça c√°ch m·ªôt t·ªëc ƒë·ªô h·ªçc cao s·∫Ω d·∫´n ƒë·∫øn m·ªôt c·ª±c ti·ªÉu c·ª•c b·ªô k√©m. V·ªõi t·ªëc ƒë·ªô h·ªçc 0.08 gi√° tr·ªã c·ªßa x b·∫Øt ƒë·∫ßu t·∫°i -4 v√† ƒëi v·ªÅ ph√≠a b√™n ph·∫£i v√† v∆∞·ª£t qua c·ª±c ti·ªÉu to√†n c·ª•c r·ªìi ƒëi t·ªõi c·ª±c ti·ªÉu c·ª•c b·ªô t·∫°i $x=2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):  # Objective function\n",
    "    return 0.25*x**4 - (1/3)*x**3 - 1.5*x**2 + 2*x\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return x**3 -x**2-3*x+2\n",
    "\n",
    "show_trace(gd(0.08, f_grad,-4,20), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c54212-5549-40c3-a71a-0f979d29431c",
   "metadata": {},
   "source": [
    "## Multivariate Gradient Descent\n",
    "Xem x√©t t√¨nh hu·ªëng m√† $\\mathbf{x}=\\left[x_1, x_2, \\ldots, x_d\\right]^{\\top}$. T·ª©c l√†, h√†m m·ª•c ti√™u $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ √°nh x·∫° c√°c vector th√†nh s·ªë th·ª±c. Gradient c·ªßa n√≥ c≈©ng l√† ƒëa bi·∫øn, l√† m·ªôt vector g·ªìm $d$ ƒë·∫°o h√†m ri√™ng:\n",
    "$$\\nabla f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\\right]^{\\top}. \\tag{5}$$\n",
    "M·ªói ph·∫ßn t·ª≠ ƒë·∫°o h√†m ri√™ng $\\partial f(\\mathbf{x}) / \\partial x_i$ trong gradient bi·ªÉu th·ªã t·ªëc ƒë·ªô thay ƒë·ªïi c·ªßa $f$ t·∫°i $\\mathbf{x}$ theo ƒë·∫ßu v√†o $x_i$. Nh∆∞ trong tr∆∞·ªùng h·ª£p m·ªôt bi·∫øn, ta c√≥ th·ªÉ s·ª≠ d·ª•ng x·∫•p x·ªâ Taylor ƒëa bi·∫øn ƒë·ªÉ c√≥ √Ω t∆∞·ªüng v·ªÅ vi·ªác n√™n l√†m g√¨. C·ª• th·ªÉ, ta c√≥:\n",
    "$$f(\\mathbf{x}+\\boldsymbol{\\epsilon})=f(\\mathbf{x})+\\boldsymbol{\\epsilon}^{\\top} \\nabla f(\\mathbf{x})+O\\left(|\\boldsymbol{\\epsilon}|^2\\right). \\tag{6}$$\n",
    "N√≥i c√°ch kh√°c, ƒë·∫øn c√°c h·∫°ng b·∫≠c hai trong $\\epsilon$, h∆∞·ªõng gi·∫£m nhanh nh·∫•t ƒë∆∞·ª£c cho b·ªüi gradient √¢m $-\\nabla f(\\mathbf{x})$. Ch·ªçn m·ªôt t·ªëc ƒë·ªô h·ªçc ph√π h·ª£p $\\eta>0$ cho ra thu·∫≠t to√°n gradient descent nguy√™n m·∫´u:\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathbf{x}). \\tag{7}$$\n",
    "L·∫•y v√≠ d·ª• h√†m m·ª•c ti√™u $f(\\mathbf{x})=x_1^2+10x_2^2$ v·ªõi vector hai chi·ªÅu $\\mathbf{x}=\\left[x_1, x_2\\right]^{\\top}$ l√†m ƒë·∫ßu v√†o v√† m·ªôt s·ªë th·ª±c l√†m ƒë·∫ßu ra. Gradient ƒë∆∞·ª£c cho b·ªüi $\\nabla f(\\mathbf{x})=\\left[2x_1, 20x_2\\right]^{\\top}$. Ta s·∫Ω quan s√°t qu·ªπ ƒë·∫°o c·ªßa $\\mathbf{x}$ b·∫±ng gradient descent t·ª´ v·ªã tr√≠ ban ƒë·∫ßu $[-5, -2]$.\n",
    "C·∫ßn hai h√†m h·ªó tr·ª£, h√†m ƒë·∫ßu ti√™n s·ª≠ d·ª•ng m·ªôt h√†m c·∫≠p nh·∫≠t v√† √°p d·ª•ng n√≥ 20 l·∫ßn cho gi√° tr·ªã ban ƒë·∫ßu. H√†m h·ªó tr·ª£ th·ª© hai tr·ª±c quan h√≥a qu·ªπ ƒë·∫°o c·ªßa $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f95578-6a5b-4d14-839c-c2b3618aba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2d(trainer, steps=20, f_grad=None):  # save\n",
    "    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n",
    "    # s1 and s2 are internal state variables that will be used in Momentum,\n",
    "    # Adagrad, RMSProp\n",
    "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
    "    results = [(x1, x2)]\n",
    "    for i in range(steps):\n",
    "        if f_grad:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
    "        else:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
    "        results.append((x1, x2))\n",
    "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
    "    return results\n",
    "\n",
    "def show_trace_2d(f, results):  # save\n",
    "    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
    "    x1, x2 = torch.meshgrid(torch.arange(-10, 1.0, 0.1),\n",
    "                            torch.arange(-3.0, 1.0, 0.1), indexing='ij')\n",
    "    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
    "    d2l.plt.xlabel('x1')\n",
    "    d2l.plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ce2ad",
   "metadata": {},
   "source": [
    "Ti·∫øp theo, ta quan s√°t qu·ªπ ƒë·∫°o c·ªßa bi·∫øn t·ªëi ∆∞u $\\mathbf{x}$ v·ªõi t·ªëc ƒë·ªô h·ªçc $\\eta=0.1$. Ta th·∫•y r·∫±ng sau 20 b∆∞·ªõc, gi√° tr·ªã c·ªßa $\\mathbf{x}$ ti·∫øn g·∫ßn ƒë·∫øn c·ª±c ti·ªÉu t·∫°i $[0, 0]$. Ti·∫øn tr√¨nh kh√° ·ªïn ƒë·ªãnh m·∫∑c d√π kh√° ch·∫≠m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2d(x1, x2): # Objective function\n",
    "    return x1 ** 2 + 10 * x2 ** 2\n",
    "def f_2d_grad(x1, x2): # Gradient of the objective function\n",
    "    return (2 * x1, 20 * x2)\n",
    "def gd_2d(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
    "eta = 0.02\n",
    "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b8123-eec4-40cf-921c-a9e71351ced9",
   "metadata": {},
   "source": [
    "## Adaptive Methods\n",
    "Vi·ªác ch·ªçn t·ªëc ƒë·ªô h·ªçc $\\eta$ \"v·ª´a ƒë√∫ng\" l√† m·ªôt vi·ªác kh√≥ khƒÉn. N·∫øu ch·ªçn qu√° nh·ªè, ti·∫øn b·ªô r·∫•t √≠t. N·∫øu ch·ªçn qu√° l·ªõn, nghi·ªám s·∫Ω dao ƒë·ªông v√† trong tr∆∞·ªùng h·ª£p x·∫•u nh·∫•t, c√≥ th·ªÉ ph√¢n k·ª≥. C√°c ph∆∞∆°ng ph√°p b·∫≠c hai, kh√¥ng ch·ªâ xem x√©t gi√° tr·ªã v√† gradient c·ªßa h√†m m·ª•c ti√™u m√† c√≤n xem x√©t ƒë·ªô cong c·ªßa n√≥, c√≥ gi√∫p √≠ch trong tr∆∞·ªùng h·ª£p n√†y. M·∫∑c d√π c√°c ph∆∞∆°ng ph√°p n√†y kh√¥ng th·ªÉ √°p d·ª•ng tr·ª±c ti·∫øp cho deep learning do chi ph√≠ t√≠nh to√°n, ch√∫ng cung c·∫•p tr·ª±c gi√°c h·ªØu √≠ch ƒë·ªÉ thi·∫øt k·∫ø c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a n√¢ng cao, m√¥ ph·ªèng nhi·ªÅu ƒë·∫∑c t√≠nh mong mu·ªën c·ªßa c√°c thu·∫≠t to√°n ƒë∆∞·ª£c tr√¨nh b√†y d∆∞·ªõi ƒë√¢y.\n",
    "### Ph∆∞∆°ng Ph√°p Newton\n",
    "Xem l·∫°i khai tri·ªÉn Taylor c·ªßa h√†m $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$, kh√¥ng c·∫ßn d·ª´ng l·∫°i sau h·∫°ng ƒë·∫ßu ti√™n:\n",
    "$$f(\\mathbf{x}+\\boldsymbol{\\epsilon})=f(\\mathbf{x})+\\boldsymbol{\\epsilon}^{\\top} \\nabla f(\\mathbf{x})+\\frac{1}{2} \\boldsymbol{\\epsilon}^{\\top} \\nabla^2 f(\\mathbf{x}) \\boldsymbol{\\epsilon}+O\\left(|\\boldsymbol{\\epsilon}|^3\\right).\\tag{8}$$\n",
    "ƒê·ªÉ tr√°nh k√Ω hi·ªáu ph·ª©c t·∫°p, ta ƒë·ªãnh nghƒ©a $\\mathbf{H} \\stackrel{\\text{def}}{=} \\nabla^2 f(\\mathbf{x})$ l√† ma tr·∫≠n Hessian c·ªßa $f$, m·ªôt ma tr·∫≠n $d \\times d$.\n",
    "Sau c√πng, c·ª±c ti·ªÉu c·ªßa $f$ th·ªèa m√£n $\\nabla f=0$. B·∫±ng c√°ch l·∫•y ƒë·∫°o h√†m c·ªßa (8) theo $\\epsilon$ v√† b·ªè qua c√°c h·∫°ng b·∫≠c cao:\n",
    "\n",
    "1. **S·ªë h·∫°ng ƒë·∫ßu ti√™n: $ f(x) $**  \n",
    "   H√†m $ f(x) $ l√† h·∫±ng s·ªë ƒë·ªëi v·ªõi $\\epsilon$ b·ªüi v√¨ $ x $ l√† c·ªë ƒë·ªãnh. Cho n√™n:\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\epsilon} f(x) = 0\n",
    "   $$\n",
    "\n",
    "2. **S·ªë h·∫°ng th·ª© 2: $ \\epsilon^T \\nabla f(x) $**\n",
    "\n",
    "    $$ \\boldsymbol{\\epsilon}^\\top \\nabla f(\\mathbf{x})\n",
    "    = \\epsilon_1 \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}\n",
    "    + \\epsilon_2 \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}\n",
    "    + \\cdots\n",
    "    + \\epsilon_d \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\n",
    "    = \\sum_{i=1}^{d} \\epsilon_i \\frac{\\partial f(\\mathbf{x})}{\\partial x_i} $$\n",
    "    \n",
    "    V·ªõi\n",
    "    \n",
    "    $$\n",
    "    \\nabla f(\\mathbf{x}) =\n",
    "    \\left[\n",
    "    \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\n",
    "    \\right]^\\top\n",
    "    $$\n",
    "    \n",
    "    l√† gradient c·ªßa  $f$ t·∫°i $\\mathbf{x}$. Suy ra\n",
    "\n",
    "   $$\n",
    "   \\epsilon^T \\nabla f(x) = \\sum_{i=1}^d \\epsilon_i \\frac{\\partial f(x)}{\\partial x_i}\n",
    "   $$\n",
    "    \n",
    "   T√≠nh to√°n ƒë·∫°o h√†m ri√™ng v·ªõi $\\epsilon_j$:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\epsilon_j} \\left( \\sum_{i=1}^d \\epsilon_i \\frac{\\partial f(x)}{\\partial x_i} \\right) = \\frac{\\partial f(x)}{\\partial x_j}\n",
    "   $$\n",
    "\n",
    "   Gradient l√†:\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\epsilon} (\\epsilon^T \\nabla f(x)) = \\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f(x)}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_d} \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "4. **S·ªë h·∫°ng th·ª© 3: $ \\frac{1}{2} \\epsilon^T \\nabla^2 f(x) \\epsilon $**  \n",
    "   $$\n",
    "   \\frac{1}{2} \\epsilon^T \\nabla^2 f(x) \\epsilon = \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\epsilon_i \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}\n",
    "   $$\n",
    "\n",
    "   ƒê·∫°o h√†m ri√™ng ƒë·ªëi v·ªõi $\\epsilon_k$:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\epsilon_k} \\left( \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\epsilon_i \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j} \\right) = \\frac{1}{2} \\left( \\sum_{j=1}^d \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_j} + \\sum_{i=1}^d \\epsilon_i \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_k} \\right)\n",
    "   $$\n",
    "\n",
    "   B·ªüi v√¨ $\\nabla^2 f(x)$ ƒë·ªëi x·ª©ng n√™n hai t·ªïng b·∫±ng nhau n√™n:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\epsilon_k} = \\sum_{j=1}^d \\epsilon_j \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_j} = \\left[ \\nabla^2 f(x) \\epsilon \\right]_k\n",
    "   $$\n",
    "\n",
    "   Gradient l√†:\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\epsilon} \\left( \\frac{1}{2} \\epsilon^T \\nabla^2 f(x) \\epsilon \\right) = \\nabla^2 f(x) \\epsilon\n",
    "   $$\n",
    "\n",
    "5. **K·∫øt h·ª£p v·ªõi nhau v√† b·ªè qua $ O(\\|\\epsilon\\|^3) $**  \n",
    "   $$\n",
    "   \\nabla_{\\epsilon} f(x + \\epsilon) = 0 + \\nabla f(x) + \\nabla^2 f(x) \\epsilon\n",
    "   $$\n",
    "\n",
    "Nh∆∞ ƒë√£ ƒë·ªãnh nghƒ©a tr∆∞·ªõc ƒë√≥ $\\mathbf{H} \\stackrel{\\text{def}}{=} \\nabla^2 f(\\mathbf{x})$ n√™n $\\nabla_{\\epsilon} f(x + \\epsilon) = \\nabla f(x) + \\mathbf{H} \\epsilon$ v√† ta c·∫ßn t√¨m gi√° tr·ªã $\\nabla_{\\epsilon} f(x + \\epsilon)=0$: $$\\nabla f(\\mathbf{x}) + H \\boldsymbol{\\epsilon} = 0$$v√† do ƒë√≥ $$\\boldsymbol{\\epsilon} = -H^{-1} \\nabla f(\\mathbf{x})$$ \n",
    "V√≠ d·ª• v·ªõi h√†m hyperbolic cosine l·ªìi $f(x)=\\cosh(cx)$ v·ªõi m·ªôt h·∫±ng s·ªë $c$, c·ª±c ti·ªÉu to√†n c·ª•c t·∫°i $x=0$ ƒë∆∞·ª£c ƒë·∫°t sau v√†i l·∫ßn l·∫∑p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25253aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.5)\n",
    "\n",
    "def f(x):  # objective function\n",
    "    return torch.cosh(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return c * torch.sinh(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return c ** 2 * torch.cosh(c * x)\n",
    "\n",
    "def newton(x,step,eta=1):\n",
    "    results = [x]\n",
    "    for i in range(step):\n",
    "        x -= eta * f_grad(x) / f_hess(x)\n",
    "        results.append(float(x))\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "show_trace(newton(10.0,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9babe",
   "metadata": {},
   "source": [
    "X√©t h√†m $f(x)=x \\cos(cx)$ v·ªõi m·ªôt h·∫±ng s·ªë $c$. Trong ph∆∞∆°ng ph√°p Newton, ta chia cho Hessian. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu ƒë·∫°o h√†m b·∫≠c hai √¢m, ta c√≥ th·ªÉ ƒëi theo h∆∞·ªõng l√†m tƒÉng gi√° tr·ªã c·ªßa $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6d17d-6adb-4733-8fba-3da795c64a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return -2 * c * torch.sin(c * x) - c ** 2 * x * torch.cos(c * x)\n",
    "\n",
    "show_trace(newton(10,10), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bf0ca",
   "metadata": {},
   "source": [
    "H√†m s·ªë kh√¥ng di chuy·ªÉn v·ªÅ c·ª±c ti·ªÉu do ƒë·∫°o h√†m b·∫≠c 2 √¢m. M·ªôt ph∆∞∆°ng ph√°p l√† l·∫•y Hessian b·∫±ng c√°ch l·∫•y gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa n√≥. M·ªôt chi·∫øn l∆∞·ª£c kh√°c l√† ƒë∆∞a l·∫°i t·ªëc ƒë·ªô h·ªçc. C√≥ th√¥ng tin b·∫≠c hai cho ph√©p th·∫≠n tr·ªçng b·∫•t c·ª© khi n√†o ƒë·ªô cong l·ªõn v√† th·ª±c hi·ªán c√°c b∆∞·ªõc d√†i h∆°n b·∫•t c·ª© khi n√†o h√†m m·ª•c ti√™u ph·∫≥ng h∆°n. V·ªõi t·ªëc ƒë·ªô h·ªçc nh·ªè h∆°n m·ªôt ch√∫t, ch·∫≥ng h·∫°n $\\eta=0.5$, thu·∫≠t to√°n ho·∫°t ƒë·ªông hi·ªáu qu·∫£ h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd52349",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(newton(10,10,0.5), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b2802-a726-4387-9290-a3539b963903",
   "metadata": {},
   "source": [
    "### V√≠ d·ª• minh h·ªça\n",
    "L·∫•y th√™m m·ªôt v√≠ d·ª• minh h·ªça v·ªõi $f(x) = x \\log(x)$. V·ªõi ph∆∞∆°ng ph√°p gradient descent th√¥ng th∆∞·ªùng v√† ph∆∞∆°ng ph√°p Newton ta c√≥ th·ªÉ th·∫•y ph∆∞∆°ng ph√°p Newton ti·∫øn v·ªÅ ƒëi·ªÉm t·ªëi ∆∞u nhanh h∆°n v·ªõi c√πng t·ªëc ƒë·ªô h·ªçc $\\eta=0.2$ v√† c√πng s·ªë b∆∞·ªõc l√† 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79316d51-ac51-4378-b725-98eef0c89840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.log(x)\n",
    "\n",
    "def f_grad(x):\n",
    "    x=torch.tensor([x])\n",
    "    return (torch.log(x) + 1).item()\n",
    "\n",
    "def f_hess(x):\n",
    "    return 1 / x\n",
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = np.arange(0.01, n, 0.01)\n",
    "    d2l.set_figsize()\n",
    "    d2l.plot([f_line, results], [[f(x) for x in f_line], [f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "show_trace(newton(5,10,0.2), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d85337-d0f6-4f4b-8510-a759b3478b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(eta, f_grad, start_x, step):\n",
    "    x = start_x\n",
    "    results = [x]\n",
    "    for i in range(step):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(x)\n",
    "    print(f'epoch 11, x: {x:.6f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.2, f_grad,5,10)\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31d0ef",
   "metadata": {},
   "source": [
    "### Ph√¢n T√≠ch H·ªôi T·ª•\n",
    "Ta ch·ªâ ph√¢n t√≠ch t·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa ph∆∞∆°ng ph√°p Newton cho m·ªôt h√†m m·ª•c ti√™u l·ªìi v√† kh·∫£ vi ba l·∫ßn, trong ƒë√≥ ƒë·∫°o h√†m b·∫≠c hai kh√°c kh√¥ng, t·ª©c l√† $f^{\\prime\\prime}>0$.\n",
    "G·ªçi $x^{(k)}$ l√† gi√° tr·ªã c·ªßa $x$ t·∫°i l·∫ßn l·∫∑p th·ª© $k$ v√† ƒë·∫∑t $e^{(k)} \\stackrel{\\text{def}}{=} x^{(k)}-x^*$ l√† kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm t·ªëi ∆∞u t·∫°i l·∫ßn l·∫∑p th·ª© $k$. B·∫±ng khai tri·ªÉn Taylor, ta c√≥ ƒëi·ªÅu ki·ªán $f^{\\prime}\\left(x^{(*)}\\right)=0$ c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt l√†:\n",
    "$$0=f^{\\prime}\\left(x^{(k)}-e^{(k)}\\right)=f^{\\prime}\\left(x^{(k)}\\right)-e^{(k)} f^{\\prime\\prime}\\left(x^{(k)}\\right)+\\frac{1}{2}\\left(e^{(k)}\\right)^2 f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right),\\tag{9}$$\n",
    "ƒëi·ªÅu n√†y ƒë√∫ng v·ªõi m·ªôt $\\xi^{(k)} \\in \\left[x^{(k)}-e^{(k)}, x^{(k)}\\right]$. Chia khai tri·ªÉn tr√™n cho $f^{\\prime\\prime}\\left(x^{(k)}\\right)$, ta ƒë∆∞·ª£c:\n",
    "$$e^{(k)}-\\frac{f^{\\prime}\\left(x^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)}=\\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)} .\\tag{10}$$\n",
    "Nh·ªõ r·∫±ng ta c√≥ c·∫≠p nh·∫≠t $x^{(k+1)}=x^{(k)}-f^{\\prime}\\left(x^{(k)}\\right) / f^{\\prime\\prime}\\left(x^{(k)}\\right)$ (ph∆∞∆°ng ph√°p Newton). Thay v√†o ph∆∞∆°ng tr√¨nh c·∫≠p nh·∫≠t n√†y v√† l·∫•y gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa c·∫£ hai v·∫ø, ta c√≥:\n",
    "$$\n",
    "e^{(k+1)}=x^{(k+1)} - x^*\n",
    "         =x^{(k)}-f^{\\prime}\\left(x^{(k)}\\right) / f^{\\prime\\prime}\\left(x^{(k)}\\right)- x^* = e^{(k)}-f^{\\prime}\\left(x^{(k)}\\right) / f^{\\prime\\prime}\\left(x^{(k)}\\right)= \\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)}{f^{\\prime\\prime}\\left(x^{(k)}\\right)}\n",
    "$$\n",
    "L·∫•y tr·ªã tuy·ªát ƒë·ªëi 2 v·∫ø:\n",
    "$$\\left|e^{(k+1)}\\right|=\\frac{1}{2}\\left(e^{(k)}\\right)^2 \\frac{\\left|f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)\\right|}{f^{\\prime\\prime}\\left(x^{(k)}\\right)} .\\tag{11}$$\n",
    "Do ƒë√≥, b·∫•t c·ª© khi n√†o ta ·ªü trong m·ªôt v√πng c√≥ $\\left|f^{\\prime\\prime\\prime}\\left(\\xi^{(k)}\\right)\\right| /\\left(2 f^{\\prime\\prime}\\left(x^{(k)}\\right)\\right) \\leq c$, ta c√≥ sai s·ªë gi·∫£m b·∫≠c hai:\n",
    "$$\\left|e^{(k+1)}\\right| \\leq c\\left(e^{(k)}\\right)^2 .\\tag{12}$$\n",
    "L∆∞u √Ω r·∫±ng c√°c nh√† nghi√™n c·ª©u t·ªëi ∆∞u h√≥a g·ªçi ƒë√¢y l√† h·ªôi t·ª• tuy·∫øn t√≠nh, trong khi m·ªôt ƒëi·ªÅu ki·ªán nh∆∞ $\\left|e^{(k+1)}\\right| \\leq \\alpha\\left|e^{(k)}\\right|$ ƒë∆∞·ª£c g·ªçi l√† t·ªëc ƒë·ªô h·ªôi t·ª• h·∫±ng s·ªë. Ph√¢n t√≠ch n√†y ƒëi k√®m v·ªõi m·ªôt s·ªë l∆∞u √Ω. Th·ª© nh·∫•t, kh√¥ng th·ª±c s·ª± c√≥ ƒë·∫£m b·∫£o khi n√†o s·∫Ω ƒë·∫°t ƒë∆∞·ª£c v√πng h·ªôi t·ª• nhanh. Thay v√†o ƒë√≥ ch·ªâ bi·∫øt r·∫±ng m·ªôt khi ƒë·∫°t ƒë∆∞·ª£c, h·ªôi t·ª• s·∫Ω r·∫•t nhanh. Th·ª© hai, ph√¢n t√≠ch n√†y y√™u c·∫ßu $f$ c√≥ t√≠nh ch·∫•t t·ªët ƒë·∫øn c√°c ƒë·∫°o h√†m b·∫≠c cao. N√≥ ph·ª• thu·ªôc v√†o vi·ªác ƒë·∫£m b·∫£o r·∫±ng $f$ kh√¥ng c√≥ b·∫•t k·ª≥ ƒë·∫∑c t√≠nh \"b·∫•t ng·ªù\" n√†o v·ªÅ c√°ch n√≥ c√≥ th·ªÉ thay ƒë·ªïi gi√° tr·ªã."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e920ef",
   "metadata": {},
   "source": [
    "### Ti·ªÅn ƒêi·ªÅu Ki·ªán H√≥a\n",
    "Kh√¥ng ng·∫°c nhi√™n khi vi·ªác t√≠nh to√°n v√† l∆∞u tr·ªØ to√†n b·ªô Hessian r·∫•t t·ªën k√©m. M·ªôt c√°ch ƒë·ªÉ c·∫£i thi·ªán l√† ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a. N√≥ tr√°nh t√≠nh to√°n to√†n b·ªô Hessian m√† ch·ªâ t√≠nh c√°c ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn c√°c thu·∫≠t to√°n c·∫≠p nh·∫≠t d·∫°ng:\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\operatorname{diag}(\\mathbf{H})^{-1} \\nabla f(\\mathbf{x}) .$$\n",
    "M·∫∑c d√π ƒëi·ªÅu n√†y kh√¥ng t·ªët b·∫±ng ph∆∞∆°ng ph√°p Newton ƒë·∫ßy ƒë·ªß, n√≥ v·∫´n t·ªët h∆°n nhi·ªÅu so v·ªõi vi·ªác kh√¥ng s·ª≠ d·ª•ng. ƒê·ªÉ th·∫•y t·∫°i sao ƒë√¢y l√† √Ω t∆∞·ªüng t·ªët, h√£y xem x√©t m·ªôt t√¨nh hu·ªëng m√† m·ªôt bi·∫øn bi·ªÉu th·ªã chi·ªÅu cao t√≠nh b·∫±ng milimet v√† m·ªôt bi·∫øn kh√°c bi·ªÉu th·ªã chi·ªÅu cao t√≠nh b·∫±ng kil√¥m√©t. Gi·∫£ s·ª≠ r·∫±ng v·ªõi c·∫£ hai, t·ª∑ l·ªá t·ª± nhi√™n l√† m√©t, ta c√≥ s·ª± kh√¥ng kh·ªõp l·ªõn trong tham s·ªë h√≥a. May m·∫Øn thay, vi·ªác s·ª≠ d·ª•ng ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a s·∫Ω lo·∫°i b·ªè ƒëi·ªÅu n√†y. Hi·ªáu qu·∫£, ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a v·ªõi gradient descent t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác ch·ªçn m·ªôt t·ªëc ƒë·ªô h·ªçc kh√°c nhau cho m·ªói bi·∫øn (t·ªça ƒë·ªô c·ªßa vector $\\mathbf{x}$). Nh∆∞ ta s·∫Ω th·∫•y sau, ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a th√∫c ƒë·∫©y m·ªôt s·ªë c·∫£i ti·∫øn trong c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a gradient descent ng·∫´u nhi√™n.\n",
    "### Gradient descent v·ªõi T√¨m Ki·∫øm Tuy·∫øn\n",
    "M·ªôt trong nh·ªØng v·∫•n ƒë·ªÅ ch√≠nh trong h·∫° gradient l√† ta c√≥ th·ªÉ v∆∞·ª£t qu√° m·ª•c ti√™u ho·∫∑c ti·∫øn b·ªô kh√¥ng ƒë·ªß. M·ªôt c√°ch s·ª≠a ƒë∆°n gi·∫£n l√† s·ª≠ d·ª•ng t√¨m ki·∫øm tuy·∫øn k·∫øt h·ª£p v·ªõi gradient descent. T·ª©c l√†, ta s·ª≠ d·ª•ng h∆∞·ªõng ƒë∆∞·ª£c cho b·ªüi $\\nabla f(\\mathbf{x})$ v√† sau ƒë√≥ th·ª±c hi·ªán t√¨m ki·∫øm nh·ªã ph√¢n ƒë·ªÉ x√°c ƒë·ªãnh t·ªëc ƒë·ªô h·ªçc $\\eta$ n√†o t·ªëi ∆∞u h√≥a $f(\\mathbf{x}-\\eta \\nabla f(\\mathbf{x}))$.\n",
    "Thu·∫≠t to√°n n√†y h·ªôi t·ª• nhanh ch√≥ng. Tuy nhi√™n, ƒë·ªëi v·ªõi m·ª•c ƒë√≠ch h·ªçc s√¢u, ƒëi·ªÅu n√†y kh√¥ng th·ª±c s·ª± kh·∫£ thi, v√¨ m·ªói b∆∞·ªõc c·ªßa t√¨m ki·∫øm tuy·∫øn s·∫Ω y√™u c·∫ßu ƒë√°nh gi√° h√†m m·ª•c ti√™u tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu. ƒêi·ªÅu n√†y qu√° t·ªën k√©m ƒë·ªÉ th·ª±c hi·ªán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022be34-d4b4-4a4f-9aa2-7564b660f378",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "### 1. Th·ª≠ nghi·ªám v·ªõi c√°c t·ªëc ƒë·ªô h·ªçc v√† h√†m m·ª•c ti√™u kh√°c nhau ƒë·ªÉ gi·∫£m d·∫ßn ƒë·ªô d·ªëc.\n",
    "ƒê√£ th·ª±c hi·ªán trong qu√° tr√¨nh t√¨m hi·ªÉu\n",
    "### 2. Tri·ªÉn khai t√¨m ki·∫øm tuy·∫øn ƒë·ªÉ gi·∫£m thi·ªÉu m·ªôt h√†m l·ªìi trong kho·∫£ng [ùëé, ùëè].\n",
    "#### 1. B·∫°n c√≥ c·∫ßn ƒë·∫°o h√†m cho t√¨m ki·∫øm nh·ªã ph√¢n kh√¥ng, t·ª©c l√† ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·ªçn $[ùëé, (ùëé +\n",
    "ùëè)/2]$ hay $[(ùëé + ùëè)/2, ùëè]$.\n",
    "Kh√¥ng th·ª±c s·ª± c·∫ßn ƒë·∫°o h√†m cho t√¨m ki·∫øm nh·ªã ph√¢n, nh∆∞ng t√¨m ki·∫øm nh·ªã ph√¢n ƒë∆°n gi·∫£n tr√™n x s·∫Ω kh√¥ng ho·∫°t ƒë·ªông tr·ª±c ti·∫øp n·∫øu kh√¥ng c√≥ th√™m th√¥ng tin.\n",
    "- N·∫øu h√†m ƒë∆°n ƒëi·ªáu tr√™n kho·∫£ng $[a,b]$ th√¨ th·ª±c hi·ªán t√¨m ki·∫øm nh·ªã ph√¢n cho $f(x)=0$ s·∫Ω c√≥ hi·ªáu qu·∫£ v√† kh√¥ng c·∫ßn s·ª≠ d·ª•ng t·ªõi ƒë·∫°o h√†m\n",
    "- N·∫øu h√†m kh√¥ng ƒë∆°n ƒëi·ªáu trong kho·∫£ng $[a,b]$, ƒë·ªÉ t·ªëi thi·ªÉu h√≥a $f(x)$. N·∫øu ch·ªâ ƒë√°nh gi√° $f((a+b)/2)$ th√¨ s·∫Ω kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒëi·ªÉm c·ª±c ti·ªÉu n·∫±m ·ªü trong kho·∫£ng $[(a+b)/2,b]$ hay $[a,(a+b)/2]$. Gi·∫£ s·ª≠ $f((a+b)/2)<f(a)$ ta s·∫Ω kh√¥ng bi·∫øt ƒë∆∞·ª£c ƒëi·ªÉm c·ª±c ti·ªÉu n·∫±m trong kho·∫£ng $[(a+b)/2,b]$ hay $[a,(a+b)/2]$ \n",
    "##### Ph∆∞∆°ng ph√°p kh√¥ng s·ª≠ d·ª•ng ƒë·∫°o h√†m ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·ªçn kho·∫£ng trong tr∆∞·ªùng h·ª£p h√†m l·ªìi.\n",
    "-Ch·ªçn 2 ƒëi·ªÉm $x_1$, $x_2$:\n",
    "  - N·∫øu $f(x_1)<f(x_2)$ ƒëi·ªÉm c·ª±c ti·ªÉu s·∫Ω n·∫±m trong kho·∫£ng $[a,x_2]$\n",
    "  - N·∫øu $f(x_1)>f(x_2)$ ƒëi·ªÉm c·ª±c ti·ªÉu s·∫Ω n·∫±m trong kho·∫£ng $[x_1,b]$\n",
    "##### Ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ƒë·∫°o h√†m ƒë·ªÉ quy·∫øt ƒë·ªãnh ch·ªçn kho·∫£ng trong tr∆∞·ªùng h·ª£p h√†m l·ªìi.\n",
    "- ƒê√°nh gi√° ƒë·∫°o h√†m $f^{\\prime}((a+b)/2)$\n",
    "  - N·∫øu $f^{\\prime}((a+b)/2)<0$ h√†m s·ªë ƒëang gi·∫£m t·∫°i ƒëi·ªÉm $(a+b)/2$ n√™n c·ª±c ti·ªÉu n·∫±m trong kho·∫£ng $[(a+b)/2, b]$\n",
    "  - N·∫øu $f^{\\prime}((a+b)/2)>0$ h√†m s·ªë ƒëang tƒÉng t·∫°i ƒëi·ªÉm $(a+b)/2$ n√™n c·ª±c ti·ªÉu n·∫±m trong kho·∫£ng $[a,(a+b)/2]$\n",
    "#### 2. T·ªëc ƒë·ªô h·ªôi t·ª• c·ªßa thu·∫≠t to√°n nhanh nh∆∞ th·∫ø n√†o?\n",
    "- T·ªëc ƒë·ªô h·ªôi t·ª• l√† **tuy·∫øn t√≠nh**. ƒê·ªô r·ªông c·ªßa kho·∫£ng ƒë∆∞·ª£c gi·∫£m theo m·ªôt h·ªá s·ªë kh√¥ng ƒë·ªïi sau m·ªói b∆∞·ªõc l·∫∑p, h·ªá s·ªë n√†y l√† $ \\frac{1}{\\varphi} \\approx 0{,}618$ (v·ªõi $\\varphi$, x·∫•p x·ªâ $1{,}618 $.ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†, ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c **m·ªôt ch·ªØ s·ªë th·∫≠p ph√¢n ch√≠nh x√°c h∆°n** (t·ª©c l√† gi·∫£m ƒë·ªô r·ªông kho·∫£ng t√¨m ki·∫øm ƒëi 10 l·∫ßn), ta c·∫ßn kho·∫£ng:$\\frac{\\log(10)}{\\log(\\varphi)} \\approx 4{,}78$ b∆∞·ªõc l·∫∑p.\n",
    "- N·∫øu ƒë·∫°o h√†m c·ªßa h√†m $f(x)$ c√≥ s·∫µn, ta c√≥ th·ªÉ th·ª±c hi·ªán **t√¨m ki·∫øm nh·ªã ph√¢n tr√™n $f'(x)$**. Ph∆∞∆°ng ph√°p n√†y c≈©ng c√≥ **t·ªëc ƒë·ªô h·ªôi t·ª• tuy·∫øn t√≠nh**, nh∆∞ng h·ªá s·ªë gi·∫£m ƒë·ªô r·ªông kho·∫£ng t·∫°i m·ªói b∆∞·ªõc l√† $0{,}5$ (t·ª©c l√† chia ƒë√¥i kho·∫£ng). ƒêi·ªÅu n√†y nhanh h∆°n so v·ªõi ph∆∞∆°ng ph√°p **T√¨m ki·∫øm theo T·ª∑ l·ªá V√†ng** (Golden Section Search).ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c **m·ªôt ch·ªØ s·ªë th·∫≠p ph√¢n ch√≠nh x√°c h∆°n** (t·ª©c l√† gi·∫£m ƒë·ªô r·ªông kho·∫£ng ƒëi 10 l·∫ßn), c·∫ßn kho·∫£ng:$\\frac{\\log(10)}{\\log(2)} \\approx 3{,}32$ b∆∞·ªõc l·∫∑p.\n",
    "#### 3. Tri·ªÉn khai thu·∫≠t to√°n v√† √°p d·ª•ng n√≥ ƒë·ªÉ t·ªëi thi·ªÉu log(exp(ùë•) + exp(‚àí2ùë• ‚àí 3))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5d854-c7fc-4bb1-9863-3c53935973c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Function to minimize: f(x) = log(exp(x) + exp(-2x - 3))\"\"\"\n",
    "    return np.log(np.exp(x) + np.exp(-2 * x - 3))\n",
    "\n",
    "def f_prime(x):\n",
    "    \"\"\"Derivative of f(x)\"\"\"\n",
    "    return (np.exp(x) - 2 * np.exp(-2 * x - 3)) / (np.exp(x) + np.exp(-2 * x - 3))\n",
    "\n",
    "def binary_search_derivative(f_prime, a, b, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Find the root of f'(x) = 0 in [a, b] using binary search, assuming f'(a) and f'(b) have opposite signs.\n",
    "    Returns the approximate minimum point x, number of iterations, and list of midpoints.\n",
    "    \"\"\"\n",
    "    if f_prime(a) * f_prime(b) >= 0:\n",
    "        raise ValueError(\"f'(a) and f'(b) must have opposite signs\")\n",
    "    \n",
    "    iterations = 0\n",
    "    midpoints = []\n",
    "    \n",
    "    while (b - a) > tol:\n",
    "        m = (a + b) / 2\n",
    "        fm = f_prime(m)\n",
    "        midpoints.append(m)\n",
    "        iterations += 1\n",
    "        \n",
    "        if abs(fm) < tol:  # If derivative is close to zero, stop\n",
    "            break\n",
    "        elif fm > 0:\n",
    "            b = m  # Root is in [a, m]\n",
    "        else:\n",
    "            a = m  # Root is in [m, b]\n",
    "    \n",
    "    x_min = (a + b) / 2\n",
    "    return x_min, iterations, midpoints\n",
    "\n",
    "# Run Binary Search\n",
    "a, b = -1, 0\n",
    "tol = 1e-5\n",
    "x_min, iterations, midpoints = binary_search_derivative(f_prime, a, b, tol)\n",
    "\n",
    "# Print results\n",
    "print(f\"Approximate minimum at x = {x_min:.6f}\")\n",
    "print(f\"Function value f(x) = {f(x_min):.6f}\")\n",
    "print(f\"Number of iterations: {iterations}\")\n",
    "\n",
    "# Plot the function and search progress\n",
    "x = np.linspace(-2, 1, 1000)\n",
    "y = f(x)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(x, y, '-', label='f(x) = log(exp(x) + exp(-2x - 3))')\n",
    "\n",
    "# Plot the midpoints from the first few iterations\n",
    "plt.plot(-1, f(-1), 'o', color='black', label='a')\n",
    "plt.plot(0, f(0), 'o', color='black', label='b')\n",
    "for i, m in enumerate(midpoints[:5]):  # Show first 5 iterations\n",
    "    plt.plot(m, f(m), 'o', label=f'Iteration {i+1}' if i < 5 else '')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Binary Search on Derivative')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5088a08-ba48-4c94-af3d-5134c2897506",
   "metadata": {},
   "source": [
    "### 3. Thi·∫øt k·∫ø m·ªôt h√†m m·ª•c ti√™u x√°c ƒë·ªãnh tr√™n $\\mathbb{R}^2$ m√† gradient descent r·∫•t ch·∫≠m.\n",
    "#### H√†m $f(x_1, x_2)= x_1^2 + Sx_2^2$\n",
    "T·∫°i sao h√†m $f(x_1, x_2)= x_1^2 + Sx_2^2$ l·∫°i h·ªôi t·ª• ch·∫≠m:\n",
    "- C·ª±c ti·ªÉu to√†n c·ª•c c·ªßa h√†m n√†y r√µ r√†ng n·∫±m t·∫°i $(x_1, x_2) = (0, 0)$, v·ªõi $f(0, 0) = 0$.\n",
    "- $\\nabla f(x_1, x_2) = [2x_1,\\ 2Sx_2]$\n",
    "- $H = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2S\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### S·ªë ƒëi·ªÅu ki·ªán (Condition Number):\n",
    "\n",
    "C√°c gi√° tr·ªã ri√™ng c·ªßa ma tr·∫≠n Hessian l√†:\n",
    "\n",
    "- $\\lambda_1 = 2$\n",
    "- $\\lambda_2 = 2S$\n",
    "\n",
    "V·∫≠y s·ªë ƒëi·ªÅu ki·ªán c·ªßa Hessian l√†:\n",
    "$$\n",
    "\\kappa(H) = \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} = \\frac{2S}{2} = S\n",
    "$$\n",
    "N·∫øu $S$ l·ªõn (v√≠ d·ª• $S = 100$), s·ªë ƒëi·ªÅu ki·ªán l√† 100. Khi $S$ l·ªõn, c√°c ellipse s·∫Ω b·ªã k√©o d√£n m·∫°nh. N·∫øu $S > 1$, ch√∫ng b·ªã k√©o d√£n theo tr·ª•c $x_1$ (nghƒ©a l√† \"thung l≈©ng\" s·∫Ω h·∫πp theo ph∆∞∆°ng $x_2$ v√† d√†i theo ph∆∞∆°ng $x_1$). H√†m tƒÉng r·∫•t nhanh theo h∆∞·ªõng $x_2$ so v·ªõi h∆∞·ªõng $x_1$.\n",
    "\n",
    "### H√†nh vi c·ªßa Gradient Descent:\n",
    "\n",
    "Gradient $\\nabla f = [2x_1,\\ 2Sx_2]$ s·∫Ω c√≥ th√†nh ph·∫ßn l·ªõn h∆°n nhi·ªÅu ·ªü h∆∞·ªõng $x_2$ (do h·ªá s·ªë $2S$) khi $x_2 \\ne 0$.\n",
    "\n",
    "Khi Gradient Descent c·∫≠p nh·∫≠t:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\alpha \\nabla f\n",
    "$$\n",
    "\n",
    "th√¨ thay ƒë·ªïi c·ªßa $x_2$ s·∫Ω l·ªõn h∆°n ƒë√°ng k·ªÉ so v·ªõi $x_1$.\n",
    "\n",
    "ƒêi·ªÅu n√†y khi·∫øn thu·∫≠t to√°n dao ƒë·ªông (zig-zag) qua l·∫°i nhanh trong ph·∫ßn h·∫πp c·ªßa thung l≈©ng (h∆∞·ªõng $x_2$), trong khi ti·∫øn tri·ªÉn d·ªçc theo ph·∫ßn b·∫±ng ph·∫≥ng (h∆∞·ªõng $x_1$) r·∫•t ch·∫≠m.\n",
    "\n",
    "ƒê·ªÉ tr√°nh sai l·ªách do th√†nh ph·∫ßn gradient $x_2$ qu√° l·ªõn, t·ªëc ƒë·ªô h·ªçc $\\alpha$ ph·∫£i ƒë∆∞·ª£c gi·ªØ r·∫•t nh·ªè, ƒëi·ªÅu n√†y c√†ng l√†m ch·∫≠m b∆∞·ªõc ƒëi theo h∆∞·ªõng $x_1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993998-f94f-4aa0-baf3-6cd77062543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S = 100  # Scaling factor\n",
    "\n",
    "def f(x1, x2):\n",
    "  return x1**2 + S * x2**2\n",
    "\n",
    "def grad_f(x1, x2):\n",
    "  return np.array([2 * x1, 2 * S * x2])\n",
    "\n",
    "# --- Gradient Descent Implementation ---\n",
    "def gradient_descent(grad_f, start_point, learning_rate, iterations):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()] # Store the path\n",
    "    for i in range(iterations):\n",
    "        grad = grad_f(x[0], x[1])\n",
    "        x = x - learning_rate * grad\n",
    "        path.append(x.copy())\n",
    "        # Optional: Check for divergence or convergence\n",
    "        if np.linalg.norm(grad) < 1e-6:\n",
    "            print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "        if np.any(np.abs(x) > 1e5): # Crude divergence check\n",
    "            print(f\"Diverged at iteration {i+1}\")\n",
    "            break\n",
    "    return np.array(path)\n",
    "\n",
    "# --- Parameters ---\n",
    "start_x = np.array([10.0, 1.0])   \n",
    "learning_rate_eta = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "# --- Run Gradient Descent ---\n",
    "path = gradient_descent(\n",
    "    grad_f,\n",
    "    start_x,\n",
    "    learning_rate_eta,\n",
    "    num_iterations\n",
    ")\n",
    "def draw_function(path):\n",
    "    \n",
    "    x1_vals = np.linspace(min(path[:, 0].min(), -10) - 1, max(path[:, 0].max(), 10) + 1, 200)\n",
    "    x2_vals = np.linspace(min(path[:, 1].min(), -1.5) - 0.2, max(path[:, 1].max(), 1.5) + 0.2, 200)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "    Z = f(X1, X2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    levels = np.logspace(0, np.log10(Z.max() if Z.max() > 0 else 1), 30) if Z.max() > 0 else 10\n",
    "    contour = plt.contour(X1, X2, Z, levels=levels, cmap='viridis')\n",
    "    plt.colorbar(contour, label='f(x‚ÇÅ, x‚ÇÇ)')\n",
    "    \n",
    "    # Plot the path of gradient descent\n",
    "    plt.plot(path[:, 0], path[:, 1], 'r-o', markersize=3, linewidth=1, label=f'GD Path (Œ∑={learning_rate_eta})')\n",
    "    \n",
    "    plt.scatter(0, 0, color='black', marker='*', s=150, label='Minimum (0,0)', zorder=5)\n",
    "    plt.scatter(start_x[0], start_x[1], color='blue', s=100, label='Start Point', zorder=4)\n",
    "    \n",
    "    plt.title(f'Gradient Descent on f(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬≤ + {S}x‚ÇÇ¬≤')\n",
    "    plt.xlabel('x‚ÇÅ')\n",
    "    plt.ylabel('x‚ÇÇ')\n",
    "    plt.legend()\n",
    "    plt.axis('equal') \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "draw_function(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8604d6-d4b5-4724-aca1-197ec6e127f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---\n",
    "start_x = np.array([10.0, 1.0])   \n",
    "learning_rate_eta = 0.005\n",
    "num_iterations = 100\n",
    "\n",
    "# --- Run Gradient Descent ---\n",
    "path = gradient_descent(\n",
    "    grad_f,\n",
    "    start_x,\n",
    "    learning_rate_eta,\n",
    "    num_iterations\n",
    ")\n",
    "draw_function(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefcc5d-c02f-4f5d-8dd2-821bdc7f926b",
   "metadata": {},
   "source": [
    "### 4. Tri·ªÉn khai phi√™n b·∫£n c·ªßa ph∆∞∆°ng ph√°p Newton b·∫±ng c√°ch s·ª≠ d·ª•ng ti·ªÅn ƒëi·ªÅu ki·ªán h√≥a.\n",
    "Ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o (Diagonal Preconditioner) $M$:\n",
    "\n",
    "Ta l·∫•y c√°c ph·∫ßn t·ª≠ tr√™n ƒë∆∞·ªùng ch√©o ch√≠nh c·ªßa Hessian: $\\text{diag}(H) = [2,\\ 2S]$.\n",
    "\n",
    "D√πng gi√° tr·ªã tuy·ªát ƒë·ªëi (m·∫∑c d√π trong tr∆∞·ªùng h·ª£p n√†y v·ªõi $S > 0$, ch√∫ng ƒë√£ l√† s·ªë d∆∞∆°ng): \n",
    "\n",
    "$$\n",
    "M_{\\text{diag}} = [|2|,\\ |2S|] = [2,\\ 2S]\n",
    "$$\n",
    "\n",
    "Ma tr·∫≠n ti·ªÅn ƒëi·ªÅu ki·ªán $M$ (n·∫øu vi·∫øt ƒë·∫ßy ƒë·ªß, d√π ta ch·ªâ c·∫ßn ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o ƒë·ªÉ t√≠nh $M^{-1} \\nabla f$):\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2S\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Ngh·ªãch ƒë·∫£o c·ªßa ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o $M^{-1}$ (√°p d·ª•ng t·ª´ng ph·∫ßn t·ª≠):\n",
    "\n",
    "N·∫øu $M$ l√† ma tr·∫≠n ch√©o v·ªõi ph·∫ßn t·ª≠ $m_{ii}$, th√¨ $M^{-1}$ c≈©ng l√† ch√©o v·ªõi ph·∫ßn t·ª≠ $1/m_{ii}$.\n",
    "\n",
    "V√¨ v·∫≠y, ti·ªÅn ƒëi·ªÅu ki·ªán c√≥ t√°c d·ª•ng **chia t·ª´ng th√†nh ph·∫ßn c·ªßa gradient cho ph·∫ßn t·ª≠ t∆∞∆°ng ·ª©ng tr√™n ƒë∆∞·ªùng ch√©o c·ªßa Hessian**.\n",
    "\n",
    "---\n",
    "\n",
    "### Quy t·∫Øc c·∫≠p nh·∫≠t:\n",
    "\n",
    "Gradient descent c√≥ ti·ªÅn ƒëi·ªÅu ki·ªán chu·∫©n l√†:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\eta \\cdot M^{-1} \\nabla f\n",
    "$$\n",
    "\n",
    "V·ªõi $M$ l√† ma tr·∫≠n ch√©o, ta c√≥:\n",
    "\n",
    "- $x_1^{\\text{new}} = x_1^{\\text{old}} - \\eta \\cdot \\left( \\frac{\\partial f / \\partial x_1}{|H_{11}|} \\right)$  \n",
    "- $x_2^{\\text{new}} = x_2^{\\text{old}} - \\eta \\cdot \\left( \\frac{\\partial f / \\partial x_2}{|H_{22}|} \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### Thay c√°c gi√° tr·ªã c·ª• th·ªÉ v√†o:\n",
    "\n",
    "- $x_1^{\\text{new}} = x_1^{\\text{old}} - \\eta \\cdot \\left( \\frac{2x_1^{\\text{old}}}{|2|} \\right) = x_1^{\\text{old}} - \\eta \\cdot x_1^{\\text{old}} = x_1^{\\text{old}} (1 - \\eta)$\n",
    "\n",
    "- $x_2^{\\text{new}} = x_2^{\\text{old}} - \\eta \\cdot \\left( \\frac{2Sx_2^{\\text{old}}}{|2S|} \\right) = x_2^{\\text{old}} - \\eta \\cdot x_2^{\\text{old}} = x_2^{\\text{old}} (1 - \\eta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01d3c8-59dd-45f5-b550-a0d550707fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def diag_hessian_abs_f(x1, x2):\n",
    "\n",
    "  h11 = 2.0\n",
    "  h22 = 2.0 * S\n",
    "  epsilon = 1e-8\n",
    "  return np.array([abs(h11) + epsilon, abs(h22) + epsilon])\n",
    "\n",
    "def preconditioned_gradient_descent(\n",
    "    grad_f, diag_hess_abs_f, start_point, learning_rate_eta, iterations\n",
    "):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()] # Store the path\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = grad_f(x[0], x[1])\n",
    "        diag_H_abs = diag_hess_abs_f(x[0], x[1])\n",
    "\n",
    "        # Element-wise division for preconditioning\n",
    "        preconditioned_grad = grad / diag_H_abs\n",
    "\n",
    "        x = x - learning_rate_eta * preconditioned_grad\n",
    "        path.append(x.copy())\n",
    "\n",
    "        # Check for convergence or divergence\n",
    "        if np.linalg.norm(grad) < 1e-7: # Check original gradient for convergence\n",
    "            print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "        if np.any(np.abs(x) > 1e6): # Crude divergence check\n",
    "            print(f\"Diverged at iteration {i+1}\")\n",
    "            break\n",
    "    return np.array(path)\n",
    "\n",
    "# --- Parameters ---\n",
    "start_x = np.array([10.0, 1.0])   # Start pointt\n",
    "learning_rate_eta = 0.5 \n",
    "num_iterations = 50\n",
    "\n",
    "\n",
    "path_preconditioned = preconditioned_gradient_descent(\n",
    "    grad_f,\n",
    "    diag_hessian_abs_f,\n",
    "    start_x,\n",
    "    learning_rate_eta,\n",
    "    num_iterations\n",
    ")\n",
    "draw_function(path_preconditioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26e41e-5847-4251-a027-7b726067b493",
   "metadata": {},
   "source": [
    "V·ªõi t·ªëc ƒë·ªô h·ªçc $\\eta=0.5$ ta th·∫•y sau l·∫ßn l·∫∑p ƒë·∫ßu ti√™n gi√° tr·ªã c·ªßa $x_1, x_2$ ƒë√£ gi·∫£m 1 n·ª≠a do \n",
    "- $x_1^{\\text{new}} = x_1^{\\text{old}} (1 - \\eta)=x_1^{\\text{old}} (1 - 0.5)$ \n",
    "- $x_2^{\\text{new}} = x_2^{\\text{old}} (1 - \\eta)=x_2^{\\text{old}} (1 - 0.5)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826da86-828d-4a54-bfa7-ab2de1d89b54",
   "metadata": {},
   "source": [
    "### √Åp d·ª•ng thu·∫≠t to√°n tr√™n cho m·ªôt s·ªë h√†m m·ª•c ti√™u (l·ªìi ho·∫∑c kh√¥ng). ƒêi·ªÅu g√¨ x·∫£y ra n·∫øu b·∫°n xoay t·ªça ƒë·ªô 45 ƒë·ªô?\n",
    "#### 1. Elliptic Paraboloid:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 100x_2^2\n",
    "$$\n",
    "\n",
    "- $\\text{diag}_H^{\\text{abs}} = [2,\\ 200]$ (h·∫±ng s·ªë)\n",
    "- **K·ª≥ v·ªçng**: H·ªôi t·ª• nhanh v√† tr·ª±c ti·∫øp (1 b∆∞·ªõc n·∫øu $\\eta = 1$)\n",
    "#### 2. Circular Paraboloid:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + x_2^2\n",
    "$$\n",
    "\n",
    "- $\\text{diag}_H^{\\text{abs}} = [2,\\ 2]$ (h·∫±ng s·ªë)\n",
    "- **K·ª≥ v·ªçng**: H·ªôi t·ª• nhanh v√† tr·ª±c ti·∫øp (1 b∆∞·ªõc n·∫øu $\\eta = 1$).  \n",
    "#### 3. H√†m Rosenbrock (Kh√¥ng l·ªìi):\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2\n",
    "$$\n",
    "\n",
    "- $\\frac{\\partial^2 f}{\\partial x_1^2} = 2 - 400(x_2 - x_1^2) + 1200x_1^2$\n",
    "- $\\frac{\\partial^2 f}{\\partial x_2^2} = 200$\n",
    "\n",
    "- $\\text{diag}_H^{\\text{abs}} = [|2 - 400(x_2 - x_1^2) + 1200x_1^2|,\\ 200]$  *($H_{11}$ c√≥ th·ªÉ √¢m!)*\n",
    "\n",
    "- **K·ª≥ v·ªçng**: Th√†nh ph·∫ßn $H_{11}$ r·∫•t ph·ª©c t·∫°p v√† ph·ª• thu·ªôc v√†o $x_1, x_2$.  \n",
    "\n",
    "#### 4. H√†m kh√¥ng l·ªìi ƒë∆°n gi·∫£n (Hai ƒëi·ªÉm c·ª±c ti·ªÉu):\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^4 - 2x_1^2 + x_2^2\n",
    "$$\n",
    "\n",
    "- C·ª±c ti·ªÉu t·∫°i $(\\pm1,\\ 0)$, ƒëi·ªÉm y√™n t·∫°i $(0,\\ 0)$\n",
    "- $\\frac{\\partial^2 f}{\\partial x_1^2} = 12x_1^2 - 4$\n",
    "- $\\frac{\\partial^2 f}{\\partial x_2^2} = 2$\n",
    "- $\\text{diag}_H^{\\text{abs}} = [|12x_1^2 - 4|,\\ 2]$  *( $H_{11}$ c√≥ th·ªÉ √¢m ho·∫∑c b·∫±ng 0)*\n",
    "- **K·ª≥ v·ªçng**: Thu·∫≠t to√°n s·∫Ω h·ªôi t·ª• ƒë·∫øn m·ªôt c·ª±c ti·ªÉu t√πy theo ƒëi·ªÉm kh·ªüi ƒë·∫ßu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167b94e-1736-47e2-860b-d03890feca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Helper: Generic Optimization and Plotting ---\n",
    "def run_optimizer(\n",
    "    optimizer_func,\n",
    "    f_obj,\n",
    "    grad_f,\n",
    "    diag_hess_abs_f, # Specific to preconditioned GD\n",
    "    start_point,\n",
    "    learning_rate_eta,\n",
    "    iterations,\n",
    "    title_prefix=\"\",\n",
    "    S_param=None # For functions that use S\n",
    "):\n",
    "    if S_param is not None: # If the function needs S, curry it\n",
    "        obj_func_to_plot = lambda x1, x2: f_obj(x1, x2, S_param)\n",
    "    else:\n",
    "        obj_func_to_plot = f_obj\n",
    "\n",
    "    path = optimizer_func(\n",
    "        grad_f,\n",
    "        diag_hess_abs_f, # Pass this\n",
    "        start_point,\n",
    "        learning_rate_eta,\n",
    "        iterations,\n",
    "        S_param # Pass S if needed for grad/hessian\n",
    "    )\n",
    "\n",
    "    print(f\"{title_prefix} - Starting at: {path[0]}\")\n",
    "    print(f\"{title_prefix} - Ending at after {len(path)-1} iterations: {path[-1]}\")\n",
    "\n",
    "    # Visualization\n",
    "    # Adjust plot ranges based on path and known features of the function\n",
    "    x_min_plot = min(path[:, 0].min() - 1, -2.5 if \"Rosenbrock\" in title_prefix else -3)\n",
    "    x_max_plot = max(path[:, 0].max() + 1, 2.5 if \"Rosenbrock\" in title_prefix else 3)\n",
    "    y_min_plot = min(path[:, 1].min() - 1, -1.5 if \"Rosenbrock\" in title_prefix else -3)\n",
    "    y_max_plot = max(path[:, 1].max() + 1, 3.5 if \"Rosenbrock\" in title_prefix else 3)\n",
    "\n",
    "    x1_vals = np.linspace(x_min_plot, x_max_plot, 200)\n",
    "    x2_vals = np.linspace(y_min_plot, y_max_plot, 200)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "    Z = obj_func_to_plot(X1, X2)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    levels = np.logspace(np.log10(max(Z.min(), 0.01)), np.log10(Z.max() if Z.max() > 0 else 1), 30) if Z.min() < Z.max() else 15\n",
    "    try:\n",
    "        contour = plt.contour(X1, X2, Z, levels=levels, cmap='viridis')\n",
    "        plt.colorbar(contour, label='f(x‚ÇÅ, x‚ÇÇ)')\n",
    "    except Exception as e:\n",
    "        print(f\"Contour plot error for {title_prefix}: {e}\")\n",
    "        plt.contour(X1, X2, Z, cmap='viridis') # Fallback\n",
    "\n",
    "\n",
    "    plt.plot(path[:, 0], path[:, 1], 'g-o', markersize=3, linewidth=1, label=f'Preconditioned GD (Œ∑={learning_rate_eta})')\n",
    "    plt.scatter(path[0, 0], path[0, 1], color='blue', s=100, label='Start', zorder=4)\n",
    "    # Add known minima if applicable\n",
    "    if \"Ill-Conditioned\" in title_prefix or \"Well-Conditioned\" in title_prefix:\n",
    "        plt.scatter(0, 0, color='black', marker='*', s=150, label='Minimum (0,0)', zorder=5)\n",
    "    elif \"Rosenbrock\" in title_prefix:\n",
    "        plt.scatter(1, 1, color='black', marker='*', s=150, label='Minimum (1,1)', zorder=5)\n",
    "    elif \"Two Minima\" in title_prefix:\n",
    "        plt.scatter([1, -1], [0, 0], color='black', marker='*', s=150, label='Minima (¬±1,0)', zorder=5)\n",
    "\n",
    "\n",
    "    plt.title(title_prefix)\n",
    "    plt.xlabel('x‚ÇÅ')\n",
    "    plt.ylabel('x‚ÇÇ')\n",
    "    plt.legend()\n",
    "    plt.axis('equal' if \"Ill-Conditioned\" in title_prefix or \"Well-Conditioned\" in title_prefix else 'tight')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return path \n",
    "    \n",
    "\n",
    "# --- Diagonal Preconditioned Gradient Descent (from previous example) ---\n",
    "def preconditioned_gradient_descent(\n",
    "    grad_f, diag_hess_abs_f, start_point, learning_rate_eta, iterations, S_param=None\n",
    "):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()]\n",
    "    epsilon_hess = 1e-8 # For numerical stability if diag_H is zero\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Pass S if the gradient/Hessian functions need it\n",
    "        current_grad = grad_f(x[0], x[1], S_param) if S_param is not None else grad_f(x[0], x[1])\n",
    "        current_diag_H_abs = diag_hess_abs_f(x[0], x[1], S_param) if S_param is not None else diag_hess_abs_f(x[0], x[1])\n",
    "\n",
    "        # Ensure diagonal elements are not zero (add epsilon)\n",
    "        preconditioner = np.maximum(current_diag_H_abs, epsilon_hess)\n",
    "        preconditioned_grad = current_grad / preconditioner\n",
    "\n",
    "        x = x - learning_rate_eta * preconditioned_grad\n",
    "        path.append(x.copy())\n",
    "\n",
    "        if np.linalg.norm(current_grad) < 1e-7:\n",
    "            # print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "        if np.any(np.abs(x) > 1e7): # Divergence check\n",
    "            print(f\"Diverged at iteration {i+1}\")\n",
    "            break\n",
    "    return np.array(path)\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "# 1. Ill-Conditioned Convex (Elliptical Paraboloid)\n",
    "S_ill = 100\n",
    "def f_ill(x1, x2, S=S_ill): return x1**2 + S * x2**2\n",
    "def grad_f_ill(x1, x2, S=S_ill): return np.array([2 * x1, 2 * S * x2])\n",
    "def diag_hess_abs_f_ill(x1, x2, S=S_ill): return np.array([abs(2.0), abs(2.0 * S)])\n",
    "\n",
    "# 2. Well-Conditioned Convex (Circular Paraboloid)\n",
    "def f_well(x1, x2): return x1**2 + x2**2\n",
    "def grad_f_well(x1, x2): return np.array([2 * x1, 2 * x2])\n",
    "def diag_hess_abs_f_well(x1, x2): return np.array([abs(2.0), abs(2.0)])\n",
    "\n",
    "# 3. Rosenbrock Function (Non-Convex)\n",
    "def f_rosen(x1, x2): return (1 - x1)**2 + 100 * (x2 - x1**2)**2\n",
    "def grad_f_rosen(x1, x2):\n",
    "    g1 = -2 * (1 - x1) - 400 * x1 * (x2 - x1**2)\n",
    "    g2 = 200 * (x2 - x1**2)\n",
    "    return np.array([g1, g2])\n",
    "def diag_hess_abs_f_rosen(x1, x2):\n",
    "    h11 = 2 - 400 * (x2 - x1**2) + 1200 * x1**2 # Corrected from 800 to 1200\n",
    "    h22 = 200.0\n",
    "    return np.array([abs(h11), abs(h22)])\n",
    "\n",
    "# 4. Simple Non-Convex (Two Minima)\n",
    "def f_two_min(x1, x2): return x1**4 - 2*x1**2 + x2**2\n",
    "def grad_f_two_min(x1, x2): return np.array([4*x1**3 - 4*x1, 2*x2])\n",
    "def diag_hess_abs_f_two_min(x1, x2):\n",
    "    h11 = 12*x1**2 - 4\n",
    "    h22 = 2.0\n",
    "    return np.array([abs(h11), abs(h22)])\n",
    "\n",
    "\n",
    "# --- Run Experiments ---\n",
    "eta = 0.5 # A reasonably robust learning rate for this preconditioned method\n",
    "# For quadratics, eta=1 is often optimal, but 0.5 is safer for non-quadratics\n",
    "iters = 200\n",
    "\n",
    "print(\"--- 1. Ill-Conditioned Elliptical Paraboloid ---\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_ill, grad_f_ill, diag_hess_abs_f_ill,\n",
    "              [10.0, 1.0], 1.0, 50, \"Ill-Conditioned Convex\", S_param=S_ill) # eta=1 is good here\n",
    "\n",
    "print(\"\\n--- 2. Well-Conditioned Circular Paraboloid ---\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_well, grad_f_well, diag_hess_abs_f_well,\n",
    "              [10.0, 1.0], 1.0, 50, \"Well-Conditioned Convex\") # eta=1 is good here\n",
    "\n",
    "print(\"\\n--- 3. Rosenbrock Function ---\")\n",
    "# Rosenbrock needs smaller eta and more iterations\n",
    "run_optimizer(preconditioned_gradient_descent, f_rosen, grad_f_rosen, diag_hess_abs_f_rosen,\n",
    "              [-1.5, 1.5], 0.1, 500, \"Rosenbrock (Non-Convex)\") # Try smaller eta\n",
    "\n",
    "print(\"\\n--- 4. Two Minima Function ---\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_two_min, grad_f_two_min, diag_hess_abs_f_two_min,\n",
    "              [0.5, 0.5], eta, iters, \"Two Minima (Start near saddle)\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_two_min, grad_f_two_min, diag_hess_abs_f_two_min,\n",
    "              [2.0, 0.5], eta, iters, \"Two Minima (Start near x1=1 min)\")\n",
    "run_optimizer(preconditioned_gradient_descent, f_two_min, grad_f_two_min, diag_hess_abs_f_two_min,\n",
    "              [-2.0, -0.5], eta, iters, \"Two Minima (Start near x1=-1 min)\")\n",
    "\n",
    "S_rot = 100 # Use the same S as in the ill-conditioned example\n",
    "\n",
    "# Rotated function f_rot(u1, u2)\n",
    "def f_rotated(u1, u2, S=S_rot):\n",
    "    return 0.5 * ((1+S)*u1**2 + (1+S)*u2**2 + 2*(S-1)*u1*u2)\n",
    "\n",
    "def grad_f_rotated(u1, u2, S=S_rot):\n",
    "    g_u1 = (1+S)*u1 + (S-1)*u2\n",
    "    g_u2 = (S-1)*u1 + (1+S)*u2\n",
    "    return np.array([g_u1, g_u2])\n",
    "\n",
    "def diag_hess_abs_f_rotated(u1, u2, S=S_rot):\n",
    "    # Diagonal elements of H_rot are both (1+S)\n",
    "    h_diag = 1.0 + S\n",
    "    return np.array([abs(h_diag), abs(h_diag)])\n",
    "\n",
    "print(\"\\n--- 5. Rotated Ill-Conditioned Function (45 degrees) ---\")\n",
    "# Start point in the rotated coordinate system\n",
    "# If original start was (10,1), rotated start could be approx (10/‚àö2 + 1/‚àö2, -10/‚àö2 + 1/‚àö2)\n",
    "# Or just pick a challenging start like [10.0, 1.0] in u1, u2 space\n",
    "rotated_start = [7.0, -7.0] # Example start in u1, u2 space\n",
    "\n",
    "# Let's compare with standard GD as well for this one\n",
    "def standard_gradient_descent(grad_f, start_point, learning_rate_eta, iterations, S_param=None):\n",
    "    x = np.array(start_point, dtype=float)\n",
    "    path = [x.copy()]\n",
    "    for i in range(iterations):\n",
    "        current_grad = grad_f(x[0], x[1], S_param) if S_param is not None else grad_f(x[0], x[1])\n",
    "        x = x - learning_rate_eta * current_grad\n",
    "        path.append(x.copy())\n",
    "        if np.linalg.norm(current_grad) < 1e-7: break\n",
    "        if np.any(np.abs(x) > 1e7): print(\"Std GD Diverged\"); break\n",
    "    return np.array(path)\n",
    "\n",
    "eta_rotated = 0.005 # Need a smaller eta for the rotated version (both methods)\n",
    "iters_rotated = 300\n",
    "\n",
    "path_prec_rot = run_optimizer(preconditioned_gradient_descent, f_rotated, grad_f_rotated, diag_hess_abs_f_rotated,\n",
    "                  rotated_start, eta_rotated, iters_rotated, \"Rotated Ill-Conditioned (Preconditioned GD)\", S_param=S_rot)\n",
    "\n",
    "print(\"\\nComparing with Standard GD on Rotated function:\")\n",
    "path_std_gd_rot = standard_gradient_descent(grad_f_rotated, rotated_start, eta_rotated, iters_rotated, S_param=S_rot)\n",
    "\n",
    "# Visualization for comparison\n",
    "plt.figure(figsize=(10, 7))\n",
    "x1_vals_rot = np.linspace(min(path_prec_rot[:,0].min(), path_std_gd_rot[:,0].min()) -1, max(path_prec_rot[:,0].max(), path_std_gd_rot[:,0].max()) +1, 200)\n",
    "x2_vals_rot = np.linspace(min(path_prec_rot[:,1].min(), path_std_gd_rot[:,1].min()) -1, max(path_prec_rot[:,1].max(), path_std_gd_rot[:,1].max()) +1, 200)\n",
    "X1_rot, X2_rot = np.meshgrid(x1_vals_rot, x2_vals_rot)\n",
    "Z_rot = f_rotated(X1_rot, X2_rot, S_rot)\n",
    "\n",
    "levels_rot = np.logspace(np.log10(max(Z_rot.min(),0.01)), np.log10(Z_rot.max() if Z_rot.max() > 0 else 1), 30) if Z_rot.min() < Z_rot.max() else 15\n",
    "contour_rot = plt.contour(X1_rot, X2_rot, Z_rot, levels=levels_rot, cmap='viridis')\n",
    "plt.colorbar(contour_rot, label='f_rot(u‚ÇÅ, u‚ÇÇ)')\n",
    "\n",
    "plt.plot(path_prec_rot[:, 0], path_prec_rot[:, 1], 'g-o', markersize=3, linewidth=1, label=f'Preconditioned GD (Œ∑={eta_rotated})')\n",
    "plt.plot(path_std_gd_rot[:, 0], path_std_gd_rot[:, 1], 'm--x', markersize=3, linewidth=1, label=f'Standard GD (Œ∑={eta_rotated})')\n",
    "\n",
    "plt.scatter(0, 0, color='black', marker='*', s=150, label='Minimum (0,0)', zorder=5) # Minimum is still at origin\n",
    "plt.scatter(rotated_start[0], rotated_start[1], color='blue', s=100, label='Start', zorder=4)\n",
    "plt.title(f'Comparison on Rotated f(x‚ÇÅ,x‚ÇÇ) = x‚ÇÅ¬≤ + {S_rot}x‚ÇÇ¬≤')\n",
    "plt.xlabel('u‚ÇÅ (rotated coord)')\n",
    "plt.ylabel('u‚ÇÇ (rotated coord)')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc75a3-99ed-42e7-b104-3fa855c178d1",
   "metadata": {},
   "source": [
    "#### V·∫•n ƒë·ªÅ: ·∫¢nh h∆∞·ªüng c·ªßa ph√©p quay ƒë·∫øn c·∫•u tr√∫c Hessian\n",
    "\n",
    "Khi quay m·ªôt h√†m nh∆∞:$f(x_1, x_2) = x_1^2 + 100x_2^2$ m·ªôt g√≥c 45 ƒë·ªô, h√†m m·ªõi (v·ªõi t·ªça ƒë·ªô g·ªçi l√† $u_1, u_2$) s·∫Ω c√≥ c√°c **th√†nh ph·∫ßn ngo√†i ƒë∆∞·ªùng ch√©o** m√† gi√° tr·ªã c·ªßa ch√∫ng l·ªõn h∆°n gi√° tr·ªã c·ªßa th√†nh ph·∫ßn tr√™n ƒë∆∞·ªùng ch√©o trong ma tr·∫≠n Hessian.\n",
    "\n",
    "C√°c **tr·ª•c ch√≠nh** c·ªßa c√°c ƒë∆∞·ªùng ƒë·ªìng m·ª©c h√¨nh elip **s·∫Ω kh√¥ng c√≤n th·∫≥ng h√†ng** v·ªõi c√°c tr·ª•c $u_1$ v√† $u_2$ n·ªØa.\n",
    "\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† h√¨nh d·∫°ng c·ªßa b√†i to√°n b·ªã \"nghi√™ng\", l√†m cho c√°c ph∆∞∆°ng ph√°p nh∆∞ Gradient Descent hay ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o d·ª±a tr√™n c√°c ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o **m·∫•t hi·ªáu qu·∫£** r√µ r·ªát.\n",
    "\n",
    "#### H√†m g·ªëc (t·ªça ƒë·ªô $x_1, x_2$):\n",
    "\n",
    "Gi·∫£ s·ª≠ ta c√≥ h√†m $f(x_1, x_2) = x_1^2 + Sx_2^2$. Hessian $H = \\begin{bmatrix} H_{11} & 0 \\\\ 0 & H_{22} \\end{bmatrix}$\n",
    "\n",
    "L√† ma tr·∫≠n ch√©o. B·ªô ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o (diagonal preconditioner) ho·∫°t ƒë·ªông t·ªët v√¨:$M_{\\text{diag}} = [|H_{11}|, |H_{22}|]$ x√°c ƒë·ªãnh ch√≠nh x√°c c√°c t·ªâ l·ªá thay ƒë·ªïi theo t·ª´ng h∆∞·ªõng kh√°c nhau n√†y v√† chu·∫©n h√≥a ch√∫ng.\n",
    "\n",
    "#### H√†m sau khi quay (t·ªça ƒë·ªô $u_1, u_2$):\n",
    "\n",
    "Ph√©p bi·∫øn ƒë·ªïi quay 45 ƒë·ªô:$x_1 = \\frac{u_1 - u_2}{\\sqrt{2}}, \\quad x_2 = \\frac{u_1 + u_2}{\\sqrt{2}}$\n",
    "Thay v√†o h√†m: $f(x_1, x_2) = x_1^2 + Sx_2^2$\n",
    "\n",
    "Ta ƒë∆∞·ª£c h√†m sau khi quay:$f_{\\text{rot}}(u_1, u_2) = \\left( \\frac{u_1 - u_2}{\\sqrt{2}} \\right)^2 + S \\left( \\frac{u_1 + u_2}{\\sqrt{2}} \\right)^2$. \n",
    "\n",
    "R√∫t g·ªçn: \n",
    "$f_{\\text{rot}}(u_1, u_2) = \\frac{1}{2}(u_1^2 - 2u_1u_2 + u_2^2) + \\frac{S}{2}(u_1^2 + 2u_1u_2 + u_2^2)\n",
    "= \\frac{1}{2}(1 + S)u_1^2 + \\frac{1}{2}(1 + S)u_2^2 + (S - 1)u_1u_2$\n",
    "\n",
    "---\n",
    "\n",
    "#### Ma tr·∫≠n Hessian sau khi quay:\n",
    "\n",
    "$$\n",
    "H_{\\text{rot}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial u_1^2} & \\frac{\\partial^2 f}{\\partial u_1 \\partial u_2} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial u_2 \\partial u_1} & \\frac{\\partial^2 f}{\\partial u_2^2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 + S & S - 1 \\\\\n",
    "S - 1 & 1 + S\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ·∫¢nh h∆∞·ªüng ƒë·∫øn ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o:\n",
    "\n",
    "B·ªô ti·ªÅn ƒëi·ªÅu ki·ªán ch·ªâ d√πng ph·∫ßn t·ª≠ ch√©o c·ªßa Hessian:\n",
    "\n",
    "$$\n",
    "\\text{diag}(H_{\\text{rot}}) = [|1 + S|,\\ |1 + S|]\n",
    "$$\n",
    "\n",
    "N√≥ **b·ªè qua ho√†n to√†n** ph·∫ßn t·ª≠ ngo√†i ƒë∆∞·ªùng ch√©o $(S - 1)$, ƒë√¢y l√† th√†nh ph·∫ßn quan tr·ªçng khi $S$ l·ªõn.\n",
    "\n",
    "V√≠ d·ª•, n·∫øu $S = 100$ th√¨ $S - 1 = 99$ ‚Äî cho th·∫•y m·ªëi t∆∞∆°ng quan m·∫°nh gi·ªØa $u_1$ v√† $u_2$ m√† ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c. ƒêi·ªÅu n√†y khi·∫øn d·∫´n t·ªõi:\n",
    "- Hi·ªáu su·∫•t c·ªßa ti·ªÅn ƒëi·ªÅu ki·ªán ch√©o s·∫Ω **gi·∫£m s√∫t ƒë√°ng k·ªÉ**.\n",
    "- Thu·∫≠t to√°n s·∫Ω **dao ƒë·ªông** tr·ªü l·∫°i nh∆∞ Gradient Descent th√¥ng th∆∞·ªùng.\n",
    "- T·ªëc ƒë·ªô h·ªôi t·ª• s·∫Ω ch·∫≠m l·∫°i do v·∫•n ƒë·ªÅ t·ª∑ l·ªá co gi√£n kh√¥ng c√≤n n·∫±m theo tr·ª•c t·ªça ƒë·ªô."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
