{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0500338e-f41f-4614-a493-e90e0ef4f602",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ed53f-736b-4850-af0a-a118197e6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f423-afc9-4927-9865-400be8d7f102",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9796f1-a271-441e-afb5-005b72be94b6",
   "metadata": {},
   "source": [
    "Trong học sâu (deep learning), hàm mục tiêu thường là trung bình của các hàm mất mát (loss function) cho từng mẫu trong tập dữ liệu huấn luyện.\n",
    "\n",
    "Giả sử có một tập huấn luyện gồm $n$ mẫu, gọi $f_i(\\mathbf{x})$ là hàm mất mát tương ứng với mẫu huấn luyện thứ $i$, trong đó $\\mathbf{x}$ là vector tham số.\n",
    "Khi đó, ta có hàm mục tiêu:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Gradient của hàm mục tiêu tại $\\mathbf{x}$ được tính bằng:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Nếu sử dụng phương pháp **Gradient Descent**, chi phí tính toán cho mỗi vòng lặp cập nhật tham số sẽ là $\\mathcal{O}(n)$, tức là **tăng tuyến tính** theo $n$. Do đó, khi tập dữ liệu huấn luyện **càng lớn** thì **chi phí** cho mỗi bước lặp của gradient descent **càng cao**.\n",
    "\n",
    "Phương pháp **Stochastic Gradient Descent (SGD)** giúp giảm chi phí tính toán ở mỗi bước lặp. Ở mỗi bước của SGD, ta chọn **ngẫu nhiên một mẫu** $i \\in \\{1, \\ldots, n\\}$ từ tập dữ liệu, và tính gradient $\\nabla f_i(\\mathbf{x})$ để cập nhật tham số $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "trong đó $\\eta$ là tốc độ học (learning rate). Ta thấy rằng chi phí tính toán cho mỗi bước lặp giảm từ $\\mathcal{O}(n)$ (của gradient descent) xuống còn hằng số $\\mathcal{O}(1)$.\n",
    "\n",
    "Ngoài ra, cần nhấn mạnh rằng gradient ngẫu nhiên $\\nabla f_i(\\mathbf{x})$ là một ước lượng không chệch (unbiased estimate) của gradient đầy đủ $\\nabla f(\\mathbf{x})$ vì:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_i \\nabla f_i(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Điều này có nghĩa là, nhìn chung, gradient ngẫu nhiên là một ước lượng tốt cho gradient thực sự.\n",
    "\n",
    "Bây giờ, chúng ta sẽ so sánh nó với gradient descent bằng cách thêm nhiễu ngẫu nhiên có kỳ vọng bằng 0 và phương sai bằng 1 vào gradient để mô phỏng stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef6152-b278-4f7c-8eae-df2f70e032a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):  # Objective function\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_grad(x1, x2):  # Gradient of the objective function\n",
    "    return 2 * x1, 4 * x2\n",
    "\n",
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    # Simulate noisy gradient\n",
    "    g1 += torch.normal(0.0, 1, (1,)).item()\n",
    "    g2 += torch.normal(0.0, 1, (1,)).item()\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "def constant_lr():\n",
    "    return 1\n",
    "\n",
    "eta = 0.1\n",
    "lr = constant_lr  # Constant learning rate\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0443f-6eb0-4665-b99e-2dc924a66a22",
   "metadata": {},
   "source": [
    "Như chúng ta có thể thấy, quỹ đạo của các biến trong phương pháp stochastic gradient descent nhiễu hơn nhiều so với quỹ đạo quan sát được trong gradient descent. Điều này là do bản chất ngẫu nhiên của gradient. Cụ thể, ngay cả khi chúng ta đã tiến gần đến điểm cực tiểu, ta vẫn bị ảnh hưởng bởi sự bất định được đưa vào qua gradient tức thời dưới dạng $\\eta \\nabla f_i(\\mathbf{x})$. Ngay cả sau 50 bước, chất lượng nghiệm vẫn chưa tốt. Thậm chí tệ hơn, nó sẽ không được cải thiện thêm dù có thực hiện thêm nhiều bước nữa.\n",
    "\n",
    "Điều này dẫn đến lựa chọn duy nhất: thay đổi tốc độ học $\\eta$. Tuy nhiên, nếu chọn giá trị quá nhỏ, ta sẽ không đạt được tiến triển đáng kể ban đầu. Ngược lại, nếu chọn quá lớn, ta sẽ không thu được nghiệm tốt, như đã thấy ở trên. Cách duy nhất để giải quyết mâu thuẫn này là giảm tốc độ học một cách *động* khi quá trình tối ưu hóa tiến triển.\n",
    "\n",
    "Đây cũng là lý do vì sao cần thêm một hàm tốc độ học `lr` vào hàm cập nhật `sgd`. Trong ví dụ trên, ta đặt đã hàm `lr` tương ứng là hằng số."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021715d3-2623-4468-84fd-d2cff032b85e",
   "metadata": {},
   "source": [
    "## Dynamic Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0f72e-cf0c-4e90-8002-58a4e034c858",
   "metadata": {},
   "source": [
    "Thay thế $\\eta$ bằng một tốc độ học phụ thuộc vào thời gian $\\eta(t)$ làm tăng độ phức tạp trong việc kiểm soát sự hội tụ của một thuật toán tối ưu. Cụ thể, chúng ta cần xác định tốc độ giảm của $\\eta$. Nếu giảm quá nhanh, quá trình tối ưu sẽ dừng lại quá sớm. Nếu giảm quá chậm, ta sẽ lãng phí quá nhiều thời gian cho việc tối ưu.\n",
    "\n",
    "Sau đây là một vài chiến lược cơ bản để điều chỉnh $\\eta$ theo thời gian:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta(t) & = \\eta_i \\textrm{ nếu } t_i \\leq t \\leq t_{i+1}  && \\textrm{hằng theo từng khoảng (piecewise constant)} \\\\\n",
    "    \\eta(t) & = \\eta_0 \\cdot e^{-\\lambda t} && \\textrm{giảm theo hàm mũ (exponential decay)} \\\\\n",
    "    \\eta(t) & = \\eta_0 \\cdot (\\beta t + 1)^{-\\alpha} && \\textrm{giảm theo đa thức (polynomial decay)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Trong kịch bản *hằng theo từng khoảng*, chúng ta giảm tốc độ học, ví dụ, mỗi khi quá trình tối ưu hóa không còn tiến triển. Đây là một chiến lược phổ biến khi huấn luyện các mạng nơ-ron sâu. Ngoài ra, chúng ta có thể giảm mạnh hơn bằng cách *giảm theo hàm mũ*. Tuy nhiên, điều này thường dẫn đến việc dừng tối ưu quá sớm trước khi thuật toán hội tụ.\n",
    "\n",
    "Một lựa chọn phổ biến là *giảm theo đa thức* với $\\alpha = 0.5$. Trong trường hợp tối ưu hóa lồi (convex optimization), có nhiều chứng minh cho thấy tốc độ giảm này hoạt động hiệu quả và ổn định.\n",
    "\n",
    "Hãy cùng xem việc giảm theo hàm mũ trông như thế nào trong thực tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed384d7-2972-4be0-846f-c580c1ae5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return math.exp(-0.1 * t)\n",
    "\n",
    "t = 1\n",
    "lr = exponential_lr\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6af05-4b24-426e-9e56-bb86a728058a",
   "metadata": {},
   "source": [
    "Như dự đoán, phương sai trong các tham số đã giảm đáng kể. Tuy nhiên, điều này phải đánh đổi bằng việc không hội tụ đến nghiệm tối ưu $\\mathbf{x} = (0, 0)$. Ngay cả sau 1000 bước lặp, chúng ta vẫn còn rất xa so với nghiệm tối ưu. Thực tế, thuật toán không hội tụ chút nào.\n",
    "\n",
    "Ngược lại, nếu chúng ta sử dụng phương pháp giảm theo đa thức, trong đó tốc độ học giảm theo nghịch đảo căn bậc hai của số bước, thì khả năng hội tụ được cải thiện rõ rệt chỉ sau 50 bước lặp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05c250-d44b-40ff-8896-0dbc4c198aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return (1 + 0.1 * t) ** (-0.5)\n",
    "\n",
    "t = 1\n",
    "lr = polynomial_lr\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c143ea-bc8c-452b-b616-1a04c7b6cc93",
   "metadata": {},
   "source": [
    "Có rất nhiều cách khác nhau để thiết lập tốc độ học. Ví dụ, ta có thể bắt đầu với một tốc độ học nhỏ, sau đó tăng nhanh rồi lại giảm dần, tuy nhiên giảm chậm hơn. Thậm chí, ta có thể xen kẽ giữa tốc độ học nhỏ và lớn. Có rất nhiều dạng lịch trình tốc độ học như vậy.\n",
    "\n",
    "Hiện tại, hãy tập trung vào các lịch trình tốc độ học mà có thể phân tích lý thuyết một cách toàn diện, tức là các tốc độ học trong bối cảnh lồi. Đối với các bài toán không lồi tổng quát, rất khó để đưa ra được đảm bảo hội tụ có ý nghĩa, vì nói chung việc tối thiểu hóa các hàm phi tuyến không lồi là một bài toán NP-khó. Để tìm hiểu thêm, có thể tham khảo [bài giảng](https://www.stat.cmu.edu/%7Eryantibs/convexopt-F15/lectures/26-nonconvex.pdf) xuất sắc của Tibshirani năm 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52573c3-24bf-4bd1-b96c-4bcb8d3a85d9",
   "metadata": {},
   "source": [
    "## Convergence Analysis for Convex Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a44124-9f84-4832-bc85-6402bcfd64ac",
   "metadata": {},
   "source": [
    "Phân tích hội tụ sau đây của phương pháp gradient ngẫu nhiên (stochastic gradient descent – SGD) đối với các hàm mục tiêu lồi là tùy chọn và chủ yếu nhằm truyền đạt trực giác sâu hơn về bài toán. Chúng ta chỉ giới hạn trong một trong những chứng minh đơn giản nhất \\:cite:`Nesterov.Vial.2000`. Tuy nhiên, có nhiều kỹ thuật chứng minh tiên tiến hơn, đặc biệt là khi hàm mục tiêu có tính chất tốt.\n",
    "\n",
    "Giả sử hàm mục tiêu $f(\\boldsymbol{\\xi}, \\mathbf{x})$ là lồi theo $\\mathbf{x}$ với mọi $\\boldsymbol{\\xi}$. Cụ thể, ta xét công thức cập nhật của SGD:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - \\eta_t \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x}),\n",
    "$$\n",
    "\n",
    "trong đó $f(\\boldsymbol{\\xi}_t, \\mathbf{x})$ là hàm mục tiêu tương ứng với mẫu huấn luyện $\\boldsymbol{\\xi}_t$ được lấy ngẫu nhiên tại bước $t$, và $\\mathbf{x}$ là tham số mô hình. Gọi:\n",
    "\n",
    "$$\n",
    "R(\\mathbf{x}) = E_{\\boldsymbol{\\xi}}[f(\\boldsymbol{\\xi}, \\mathbf{x})]\n",
    "$$\n",
    "\n",
    "là rủi ro kỳ vọng, và $R^*$ là giá trị tối thiểu của nó theo $\\mathbf{x}$. Gọi $\\mathbf{x}^*$ là điểm tối ưu (giả định tồn tại trong miền xác định của $\\mathbf{x}$). Khi đó ta có thể theo dõi khoảng cách giữa tham số hiện tại $\\mathbf{x}_t$ tại thời điểm $t$ và điểm tối ưu $\\mathbf{x}^*$ để xem liệu có tiến bộ theo thời gian không:\n",
    "\n",
    "$$\\begin{aligned}    &\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 \\\\ =& \\|\\mathbf{x}_{t} - \\eta_t \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x}) - \\mathbf{x}^*\\|^2 \\\\    =& \\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 + \\eta_t^2 \\|\\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\|^2 - 2 \\eta_t    \\left\\langle \\mathbf{x}_t - \\mathbf{x}^*, \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\right\\rangle.   \\end{aligned}$$\n",
    "\n",
    "Giả sử chuẩn $\\ell_2$ của gradient ngẫu nhiên bị chặn bởi một hằng số $L$, ta có:\n",
    "\n",
    "$$\n",
    "\\eta_t^2 \\|\\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\|^2 \\leq \\eta_t^2 L^2.\n",
    "$$\n",
    "\n",
    "Chúng ta quan tâm chủ yếu đến việc khoảng cách giữa $\\mathbf{x}_t$ và $\\mathbf{x}^*$ thay đổi như thế nào *trung bình*. Trên thực tế, với mỗi chuỗi bước cụ thể, khoảng cách này có thể tăng hoặc giảm, tùy thuộc vào mẫu $\\boldsymbol{\\xi}_t$ mà ta gặp phải. Do đó, ta cần chặn tích vô hướng.\n",
    "\n",
    "Với mọi hàm lồi $f$, ta có:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\langle f'(\\mathbf{x}), \\mathbf{y} - \\mathbf{x} \\rangle.\n",
    "$$\n",
    "\n",
    "Áp dụng với $\\mathbf{x}_t$ và $\\mathbf{x}^*$, ta được:\n",
    "$$\\begin{aligned}\n",
    " f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) &\\geq f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) + \\left\\langle \\mathbf{x}^* - \\mathbf{x}_t, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle\\\\ \\left\\langle \\mathbf{x}^* - \\mathbf{x}_t, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle &\\leq f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t)\\\\ - \\left\\langle \\mathbf{x}_t - \\mathbf{x}^*, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle &\\leq f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thay hai bất đẳng thức trên vào biểu thức ban đầu, ta thu được:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 &\\leq \\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 + \\eta_t^2 L^2 - 2 \\eta_t (f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t))\\\\\n",
    "\\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 &\\geq 2 \\eta_t (f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*)) - \\eta_t^2 L^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Điều này có nghĩa là ta đang tiến gần đến điểm tối ưu miễn là sự khác biệt giữa mất mát hiện tại và mất mát tối ưu lớn hơn $\\eta_t L^2 / 2$. Do sự khác biệt này sẽ tiến dần về 0 nên tốc độ học $\\eta_t$ cũng cần *giảm dần*.\n",
    "\n",
    "Tiếp theo, lấy kỳ vọng của bất đẳng thức trên:\n",
    "\n",
    "$$\n",
    "E\\left[\\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2\\right] - E\\left[\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2\\right] \\geq 2 \\eta_t [E[R(\\mathbf{x}_t)] - R^*] -  \\eta_t^2 L^2.\n",
    "$$\n",
    "\n",
    "Tổng các bất đẳng thức này với $t = 1, \\ldots, T$, và bỏ đi phần âm, ta được:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}_1 - \\mathbf{x}^*\\|^2 \\geq 2 \\left (\\sum_{t=1}^T   \\eta_t \\right) [E[R(\\mathbf{x}_t)] - R^*] - L^2 \\sum_{t=1}^T \\eta_t^2.\n",
    "$$\n",
    "\n",
    "Do $\\mathbf{x}_1$ là giá trị đã biết nên ta bỏ kỳ vọng. Định nghĩa:\n",
    "\n",
    "$$\n",
    "\\bar{\\mathbf{x}} := \\frac{\\sum_{t=1}^T \\eta_t \\mathbf{x}_t}{\\sum_{t=1}^T \\eta_t}.\n",
    "$$\n",
    "\n",
    "Ta có:\n",
    "\n",
    "$$\n",
    "E\\left(\\frac{\\sum_{t=1}^T \\eta_t R(\\mathbf{x}_t)}{\\sum_{t=1}^T \\eta_t}\\right) = \\frac{\\sum_{t=1}^T \\eta_t E[R(\\mathbf{x}_t)]}{\\sum_{t=1}^T \\eta_t} = E[R(\\mathbf{x}_t)].\n",
    "$$\n",
    "\n",
    "Do bất đẳng thức Jensen và tính lồi của $R$, ta có:\n",
    "\n",
    "$$\n",
    "E[R(\\mathbf{x}_t)] \\geq E[R(\\bar{\\mathbf{x}})],\n",
    "\\Rightarrow \\sum_{t=1}^T \\eta_t E[R(\\mathbf{x}_t)] \\geq \\sum_{t=1}^T \\eta_t  E\\left[R(\\bar{\\mathbf{x}})\\right].\n",
    "$$\n",
    "\n",
    "Thay vào bất đẳng thức trên, ta được:\n",
    "\n",
    "$$\n",
    "\\left[E[R(\\bar{\\mathbf{x}})]\\right] - R^* \\leq \\frac{r^2 + L^2 \\sum_{t=1}^T \\eta_t^2}{2 \\sum_{t=1}^T \\eta_t},\n",
    "$$\n",
    "\n",
    "với $r^2 := \\|\\mathbf{x}_1 - \\mathbf{x}^*\\|^2$ là độ chênh lệch giữa điểm khởi đầu và điểm tối ưu. Tóm lại, tốc độ hội tụ phụ thuộc vào việc gradient ngẫu nhiên được chặn như thế nào ($L$) và điểm bắt đầu cách xa tối ưu bao nhiêu ($r$). Lưu ý rằng kết quả là đối với $\\bar{\\mathbf{x}}$, tức phiên bản trung bình mượt mà của quá trình tối ưu.\n",
    "\n",
    "Khi $r, L$, và $T$ đã biết, ta có thể chọn tốc độ học $\\eta = r/(L \\sqrt{T})$. Khi đó, ta có chặn trên là $rL/\\sqrt{T}$, tức ta hội tụ với tốc độ $\\mathcal{O}(1/\\sqrt{T})$ đến nghiệm tối ưu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64582163-40de-4db8-8aad-65148ef8e17e",
   "metadata": {},
   "source": [
    "## Stochastic Gradients and Finite Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321623a-710c-43c7-999e-01b02362994b",
   "metadata": {},
   "source": [
    "Cho đến giờ, chúng ta đã nói về **stochastic gradient descent (SGD)** một cách khá đơn giản và sơ lược. Chúng ta giả định rằng ta lấy các mẫu \\$x\\_i\\$, thường đi kèm với nhãn \\$y\\_i\\$, từ một phân phối nào đó \\$p(x, y)\\$ và sử dụng chúng để cập nhật các tham số của mô hình theo một cách nào đó. Cụ thể hơn, với một tập dữ liệu hữu hạn, ta đã lập luận rằng phân phối rời rạc \\$p(x, y) = \\frac{1}{n} \\sum\\_{i=1}^n \\delta\\_{x\\_i}(x) \\delta\\_{y\\_i}(y)\\$\n",
    "với một số hàm \\$\\delta\\_{x\\_i}\\$ và \\$\\delta\\_{y\\_i}\\$\n",
    "cho phép ta thực hiện SGD trên phân phối đó.\n",
    "\n",
    "Tuy nhiên, thực tế không hoàn toàn như vậy. Trong các ví dụ minh họa đơn giản ở phần này, ta đơn giản chỉ thêm nhiễu vào gradient không ngẫu nhiên, tức là ta giả vờ như đang có các cặp \\$(x\\_i, y\\_i)\\$. Điều này là hợp lý trong ngữ cảnh ở đây (xem bài tập để hiểu chi tiết hơn). Điều đáng lo ngại hơn là trong các phần trước, rõ ràng ta không làm như vậy. Thay vào đó, ta **duyệt qua tất cả các mẫu huấn luyện đúng một lần**. Để thấy tại sao cách làm này tốt hơn, hãy xét tình huống ngược lại: giả sử ta chọn ngẫu nhiên \\$n\\$ quan sát từ phân phối rời rạc **có hoàn lại**. Xác suất để chọn một phần tử \\$i\\$ bất kỳ là \\$1/n\\$. Do đó, xác suất để **chọn được ít nhất một lần** là:\n",
    "\n",
    "$P(\\textrm{chọn~} i) = 1 - P(\\textrm{không chọn~} i) = 1 - (1-1/n)^n \\approx 1-e^{-1} \\approx 0.63.$\n",
    "\n",
    "Lập luận tương tự cho thấy xác suất chọn một mẫu **chính xác một lần duy nhất** là:\n",
    "\n",
    "${n \\choose 1} \\frac{1}{n} \\left(1-\\frac{1}{n}\\right)^{n-1} = \\frac{n}{n-1} \\left(1-\\frac{1}{n}\\right)^{n} \\approx e^{-1} \\approx 0.37.$\n",
    "\n",
    "Việc lấy mẫu **có hoàn lại** làm tăng phương sai và giảm hiệu quả sử dụng dữ liệu so với lấy mẫu **không hoàn lại**. Do đó, trong thực tế, ta thường dùng cách lấy mẫu **không hoàn lại** (và đây cũng là cách được mặc định sử dụng xuyên suốt trong cuốn sách này). Cuối cùng, lưu ý rằng nếu duyệt lại tập huấn luyện nhiều lần thì mỗi lần như vậy sẽ duyệt qua tập dữ liệu theo **một thứ tự ngẫu nhiên khác nhau**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f04a1e-302e-4752-8383-44cbb2d74af9",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a4dae-2fac-4779-bf48-a861dd649eee",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2640f78-8dea-40aa-a8c9-344c16727f34",
   "metadata": {},
   "source": [
    "Experiment with different learning rate schedules for stochastic gradient descent and with different numbers of iterations. In particular, plot the distance from the optimal solution $(0, 0)$ as a function of the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd588328-307f-43da-b70b-edc7c6b98b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    dist_to_optimum.append(d2l.np.linalg.norm([x1, x2]).item())\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    # Simulate noisy gradient\n",
    "    g1 += torch.normal(0.0, 1, (1,)).item()\n",
    "    g2 += torch.normal(0.0, 1, (1,)).item()\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "# CONSTANT LEARNING RATE\n",
    "dist_to_optimum = []\n",
    "eta = 0.1\n",
    "lr = constant_lr  # Constant learning rate\n",
    "d2l.train_2d(sgd, steps=50, f_grad=f_grad)\n",
    "\n",
    "# Plotting\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('Constant learning rate: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()\n",
    "\n",
    "# EXPONENTIAL LEARNING RATE\n",
    "dist_to_optimum = []\n",
    "t = 1\n",
    "lr = exponential_lr\n",
    "d2l.train_2d(sgd, steps=50, f_grad=f_grad)\n",
    "\n",
    "# Plotting\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('Exponential learning rate: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()\n",
    "\n",
    "# POLYNOMIAL LEARNING RATE\n",
    "dist_to_optimum = []\n",
    "t = 1\n",
    "lr = polynomial_lr\n",
    "d2l.train_2d(sgd, steps=50, f_grad=f_grad)\n",
    "\n",
    "# Plotting\n",
    "d2l.plt.figure(figsize=(8, 5))\n",
    "d2l.plt.plot(dist_to_optimum, marker='o', linestyle='-', color='blue', label='Distance to Optimum')\n",
    "d2l.plt.xlabel('Iteration')\n",
    "d2l.plt.ylabel('Distance')\n",
    "d2l.plt.title('Polynomial learning rate: Convergence to Optimum')\n",
    "d2l.plt.ylim(0, 6)\n",
    "d2l.plt.grid(True)\n",
    "d2l.plt.legend()\n",
    "d2l.plt.tight_layout()\n",
    "d2l.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094e35d-8ebe-4bf6-8ef3-022eac747065",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313de73f-acf8-4922-aeeb-573c181baece",
   "metadata": {},
   "source": [
    "Prove that for the function $f(x_1, x_2) = x_1^2 + 2 x_2^2$ adding normal noise to the gradient is equivalent to minimizing a loss function $f(\\mathbf{x}, \\mathbf{w}) = (x_1 - w_1)^2 + 2 (x_2 - w_2)^2$ where $\\mathbf{x}$ is drawn from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec09cc-7b25-45b2-9fd9-5ffbc2b62e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf1d6095-b7eb-493b-bc3f-da9466804e06",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fbfab-42b4-40d7-9f32-02c4b51cf49c",
   "metadata": {},
   "source": [
    "Compare convergence of stochastic gradient descent when you sample from $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$ with replacement and when you sample without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51711f-ebf6-443d-9e46-f21da08a9e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53da6724-9c69-4023-b80d-b0f0df780b04",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50574902-dfa1-4584-a969-0ed1b94f6c4f",
   "metadata": {},
   "source": [
    "How would you change the stochastic gradient descent solver if some gradient (or rather some coordinate associated with it) was consistently larger than all the other gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fde04f-ba1e-4aca-8f4a-d9646af5bbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a83f472e-1196-48cc-9c97-40f816648189",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54a65c-0bd3-4557-922b-24d5dc869c37",
   "metadata": {},
   "source": [
    "Assume that $f(x) = x^2 (1 + \\sin x)$. How many local minima does $f$ have? Can you change $f$ in such a way that to minimize it one needs to evaluate all the local minima?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ab783-e5aa-49f2-ae84-290dd99b880e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
